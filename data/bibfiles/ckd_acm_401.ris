TY  - CONF
TI  - Performance Measurements of Machine Learning Approaches for Prediction and Diagnosis of Chronic Kidney Disease (CKD)
AU  - Saha, Anik
AU  - Saha, Abir
AU  - Mittra, Tanni
T3  - ICCCM '19
AB  - Chronic Kidney Disease (CKD) is indicated by gradual degradation of kidney function. Long time complications include heart disease, high blood pressure, bone disease and death of hundreds and thousands of people each year. However, automated early detection of this disease can diminish the mortality rate. More efficient and accurate analysis and models are mandatory to be implemented to negotiate with this situation. To do so, this paper analyzes the key parameters of this disease to be strengthened by incorporating machine learning and data mining approaches together. For early classification of this disease Random Forest, Naïve Bayes, Multilayer Perceptron, Logistic Regression and Neural Network optimized by Adam optimizer have been concentrated on. For further statistical analysis and induction of relationships between attributes, J48 and WEKA tool have been used. Moreover, association rules have been extracted to assist expertise to diagnose this disease. Comparative analysis and experiment depict that Adam-Deep learning outperforms all the approaches by predicting accuracy of 97.34% and J48 demonstrates accurate association rules.
C1  - New York, NY, USA
C3  - Proceedings of the 7th International Conference on Computer and Communications Management
DA  - 2019///
PY  - 2019
DO  - 10.1145/3348445.3348462
SP  - 200
EP  - 204
PB  - Association for Computing Machinery
SN  - 978-1-4503-7195-7
UR  - https://doi.org/10.1145/3348445.3348462
KW  - J48
KW  - Chronic Kidney Disease (CKD)
KW  - multilayer perceptron
KW  - association rules
KW  - blood glucose level
KW  - Deep Neural Network (DNN)
ER  - 

TY  - CONF
TI  - A Mobile Application for Chronic Kidney Disease (CKD) Diagnosis
AU  - Periyasamy, Kasilingam
AU  - Kaivelikkal, Athira Dinesh
AU  - Iyer, Venkateshwaran
T3  - ICMHI '22
AB  - Chronic Kidney Disease (CKD) is a condition where kidneys partially work in filtering waste products from blood. In most cases, CKD patients do not experience any symptoms in the early stages. This makes early detection of CKD much harder. The result is starting treatment at a later stage which not only complicates the health condition of the patient, but it also increases the healthcare cost for the patient. This paper describes a mobile application the authors have developed which can be used by any healthcare practitioner. In addition to detecting the presence and the stage of CKD, the application also lets the user find out two risk factors associated with CKD patients, namely Anemia and Mineral Bone Disease (MBD). After detecting either or both of these risk factors, the tool recommends initial treatment plans for the same based on guidelines provided the National Kidney Foundation (NKF) in the United States.
C1  - New York, NY, USA
C3  - Proceedings of the 6th International Conference on Medical and Health Informatics
DA  - 2022///
PY  - 2022
DO  - 10.1145/3545729.3545740
SP  - 32
EP  - 36
PB  - Association for Computing Machinery
SN  - 978-1-4503-9630-1
UR  - https://doi.org/10.1145/3545729.3545740
KW  - Diagnosis
KW  - CKD
KW  - Chronic Kidney Disease
KW  - Health Informatics
KW  - Mobile Application
ER  - 

TY  - CONF
TI  - Using Context Ontology and Linear SVM for Chronic Kidney Disease Prediction
AU  - Guermah, Hatim
AU  - Fissaa, Tarik
AU  - Guermah, Bassma
AU  - Hafiddi, Hatim
AU  - Nassar, Mahmoud
T3  - LOPAL '18
AB  - In the e-Health learning area, the use of chronic patient context has become very important given the increase in the number of individuals who suffer from these diseases and the unavailability of medications. Specifically, chronic kidney failure is one of the diseases that goes undetected and undiagnosed until it is well advanced. The need for preventive prediction remains an essential task for the well-being of patients at risk. In this paper, we aim to explore the added value of using ontology-based prediction, focusing on Linear SVM, to deal with Chronic Kidney Disease Problems.
C1  - New York, NY, USA
C3  - Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications
DA  - 2018///
PY  - 2018
DO  - 10.1145/3230905.3230941
PB  - Association for Computing Machinery
SN  - 978-1-4503-5304-5
UR  - https://doi.org/10.1145/3230905.3230941
KW  - Chronic Kidney Disease
KW  - Ontologies
KW  - Context-Awareness
KW  - Linear SVM
ER  - 

TY  - CONF
TI  - MHealth in End Stage Renal Disease Self-Management: A Usability Study
AU  - Pedro Barros, Joao
AU  - Vieira Marques, Pedro
AU  - Ferreira, Ana
T3  - ICMHI '21
AB  - The prevalence of chronic diseases is increasing worldwide alongside with comorbidities and morbidity in modern societies. End Stage Renal Disease (ESRD), as a chronic disease, needs patients to adapt to a whole new daily life situation due to its complex therapeutic regimen. Developments in mHealth have been an answer for many challenges that patients face, providing tools for helping and supporting patient's disease self-management. The aim of this study is to identify relevant mobile apps provided for ESRD Patients, verify which one is closer to provide patient needs in terms of disease self-management, supporting and monitoring their therapeutics, and assess it in terms of functionality and usability. After a literature review, a mobile app which supports ESRD self-management was selected and assessed regarding its functionality and usability by applying the Think Aloud (TA) method and the System Usability Score (SUS). The target population were healthcare professionals with expertise in ESRD. The assessment was performed in two different moments: with the app original version and after an interface redesign with a mockup development. The obtained results show an increase in the SUS score, from 64,25 to 73 and better outcomes in the TA evaluation, with users referring a “better interactivity with the app functionalities”, and that “it is easy to use”.
C1  - New York, NY, USA
C3  - Proceedings of the 5th International Conference on Medical and Health Informatics
DA  - 2021///
PY  - 2021
DO  - 10.1145/3472813.3473190
SP  - 199
EP  - 204
PB  - Association for Computing Machinery
SN  - 978-1-4503-8984-6
UR  - https://doi.org/10.1145/3472813.3473190
KW  - chronic kidney disease
KW  - mobile app
KW  - patient generated health data
KW  - smartphone
ER  - 

TY  - CONF
TI  - Stage Classification in Chronic Kidney Disease by Ultrasound Image
AU  - Hsieh, Jun-Wei
AU  - Lee, C.-Hung
AU  - Chen, Y.-Chih
AU  - Lee, W.-Shan
AU  - Chiang, H.-Fen
T3  - IVCNZ '14
AB  - Ultrasound imaging can provide radiation-free, non-invasive, low cost, and convenient for disease detection. However, speckle effect makes it noisy and thus reduces its overall diagnostic abilities in disease analysis. This paper develops a real time system to analyze chronic kidney disease (CKD) using only Ultrasound images. As we know, this is the first work to analyze CKD stages of patients directly from ultrasound images without using any blood examination such as Creatinine index. To build the scoring index, this paper uses Nakagami distribution and Local Binary Pattern (LBP) to model the scattering properties of CKD patients' ultrasound images. In addition, we find the age distribution is also important for CKD stage analysis. After integration, a codebook concept is adopted to extract important visual codes to describe various texture and scattering characteristics of each CKD stage. Then, an ensemble scheme is proposed for CKD stage prediction and classification by separating input ultrasound images to several grids and then integrating different classifiers trained on these grids to build a strong CKD stage classifier via SVM. Experimental results demonstrate the sensitivity and specificity of this system up to 93.82% and 83.34%, respectively.
C1  - New York, NY, USA
C3  - Proceedings of the 29th International Conference on Image and Vision Computing New Zealand
DA  - 2014///
PY  - 2014
DO  - 10.1145/2683405.2683457
SP  - 271
EP  - 276
PB  - Association for Computing Machinery
SN  - 978-1-4503-3184-5
UR  - https://doi.org/10.1145/2683405.2683457
KW  - Chronic Kidney Disease
KW  - Support Vector Machine
KW  - Local Binary Pattern
KW  - Nakagami Distribution
ER  - 

TY  - JOUR
TI  - XGBoost Model for Chronic Kidney Disease Diagnosis
AU  - Ogunleye, Adeola
AU  - Wang, Qing-Guo
T2  - IEEE/ACM Trans. Comput. Biol. Bioinformatics
AB  - Chronic Kidney Disease (CKD) is a menace that is affecting 10 percent of the world population and 15 percent of the South African population. The early and cheap diagnosis of this disease with accuracy and reliability will save 20,000 lives in South Africa per year. Scientists are developing smart solutions with Artificial Intelligence (AI). In this paper, several typical and recent AI algorithms are studied in the context of CKD and the extreme gradient boosting (XGBoost) is chosen as our base model for its high performance. Then, the model is optimized and the optimal full model trained on all the features achieves a testing accuracy, sensitivity, and specificity of 1.000, 1.000, and 1.000, respectively. Note that, to cover the widest range of people, the time and monetary costs of CKD diagnosis have to be minimized with fewest patient tests. Thus, the reduced model using fewer features is desirable while it should still maintain high performance. To this end, the set-theory based rule is presented which combines a few feature selection methods with their collective strengths. The reduced model using about a half of the original full features performs better than the models based on individual feature selection methods and achieves accuracy, sensitivity and specificity, of 1.000, 1.000, and 1.000, respectively.
DA  - 2020/12//
PY  - 2020
DO  - 10.1109/TCBB.2019.2911071
VL  - 17
IS  - 6
SP  - 2131
EP  - 2140
SN  - 1545-5963
UR  - https://doi.org/10.1109/TCBB.2019.2911071
ER  - 

TY  - CONF
TI  - Prediction of Chronic Kidney Disease Risk Using Multimodal Data
AU  - Ma, Dongfang
AU  - Li, Ximin
AU  - Mou, Shenghong
AU  - Cheng, Zhiyuan
AU  - Yan, Xiaoqian
AU  - Lu, Ying
AU  - Yan, Ruijian
AU  - Cao, Shiyue
T3  - ICCDA 2021
AB  - Chronic kidney disease (CKD) is a widespread public health problem and often leads to kidney failure which needs hemodialysis or even kidney transplantation. Undoubtedly, prediction of the risk of CKD among healthy people is highly desirable and very meaningful. However, most studies in this field used logistic regression (LR) and produced results with limited accuracy. Also, these studies ignored unstructured data which contained useful information. To improve CKD prediction, in this study, we built a novel multimodal data model that integrated Bidirectional Encoder Representations from Transformers with Light Gradient Boosting Machine (termed MD-BERT-LGBM model hereafter), and applied it to a group of 3295 participants for CKD prediction study. We collected medical data for over three months from each participant. We compared this novel integrated framework with three conventional models: the LR, LGBM, and Multimodal Disease Risk Prediction algorithm based on Convolutional Neural Networks (CNN-MDRP). The experimental results show that the new MD-BERT-LGBM model outperformed all the three conventional models in terms of accuracy, recall, and Area Under the ROC curve (AUC), which are 78.12%, 75.65%, and 85.15%, respectively. This result demonstrates the potential of this proposed method in the clinical application of CKD prediction and prevention.
C1  - New York, NY, USA
C3  - 2021 The 5th International Conference on Compute and Data Analysis
DA  - 2021///
PY  - 2021
DO  - 10.1145/3456529.3456533
SP  - 20
EP  - 25
PB  - Association for Computing Machinery
SN  - 978-1-4503-8911-2
UR  - https://doi.org/10.1145/3456529.3456533
KW  - Chronic Kidney Disease
KW  - Bidirectional Encoder Representations from Transformers
KW  - Light Gradient Boosting Machine
KW  - Multimodal Data
ER  - 

TY  - CONF
TI  - Automatic Extraction of Deep Phenotypes for Precision Medicine in Chronic Kidney Disease
AU  - Singh, Prerna
AU  - Chandola, Varun
AU  - Fox, Chester
T3  - DH '17
AB  - Chronic Kidney Disease (CKD) is one of the deadliest diseases in the world, with 10% of the global population affected by the disease. Identifying subpopulations with characteristic disease progressions is important to find more efficient treatments for patients with this disease. The abundance of electronic health records (EHR) data can be used to find meaningful subtypes for CKD but comes with challenges during analysis, including irregular data sampling, and skewness in the data collected over time. In this paper, multiple regression techniques were used to fill in the missing estimated glomerular filtration rate (or eGFR – a key measure for kidney function) trajectory data, so it can be clustered effectively. Clustering is applied to the enhanced data to obtain six subtypes, which capture crucial trends in the disease progression of patients. Moreover, the characteristics of patients in each of the subtypes had minor differences from others. These characteristics demonstrate risk factors and positive lifestyles choices of patients with CKD, which can help develop new treatments for CKD.
C1  - New York, NY, USA
C3  - Proceedings of the 2017 International Conference on Digital Health
DA  - 2017///
PY  - 2017
DO  - 10.1145/3079452.3079489
SP  - 195
EP  - 199
PB  - Association for Computing Machinery
SN  - 978-1-4503-5249-9
UR  - https://doi.org/10.1145/3079452.3079489
KW  - partitioning around medoids
KW  - regression
KW  - spline
KW  - time-series clustering
ER  - 

TY  - CONF
TI  - Portable, Non-Invasive Fall Risk Assessment in End Stage Renal Disease Patients on Hemodialysis
AU  - Lockhart, Thurmon E.
AU  - Barth, Adam T.
AU  - Zhang, Xiaoyue
AU  - Songra, Rahul
AU  - Abdel-Rahman, Emaad
AU  - Lach, John
T3  - WH '10
AB  - Patients with end stage renal diseases (ESRD) on hemodialysis (HD) have high morbidity and mortality due to multiple causes, one of which is dramatically higher fall rates than the general population. The mobility mechanisms that contribute to falls in this population must be understood if adequate interventions for fall prevention are to be achieved. This study utilizes emerging non-invasive, portable gait, posture, strength, and stability assessment technologies to extract various mobility parameters that research has shown to be predictive of fall risk in the general population. As part of an ongoing human subjects study, mobility measures such as postural and locomotion profiles were obtained from five (5) ESRD patients undergoing HD treatments. To assess the effects of post-HD-fatigue on fall risk, both the pre- and post-HD measurements were obtained. Additionally, the effects of inter-HD periods (two days vs. three days) were investigated using the non-invasive, wireless, body-worn motion capture technology and novel signal processing algorithms. The results indicated that HD treatment influenced strength and mobility (i.e., weaker and slower after the dialysis, increasing the susceptibility to falls while returning home) and inter-dialysis period influenced pre-HD profiles (increasing the susceptibility to falls before they come in for a HD treatment). Methodology for early detection of increased fall risk – before a fall event occurs – using the portable mobility assessment technology for out-patient monitoring is further explored, including targeting interventions to identified individuals for fall prevention.
C1  - New York, NY, USA
C3  - Wireless Health 2010
DA  - 2010///
PY  - 2010
DO  - 10.1145/1921081.1921092
SP  - 84
EP  - 93
PB  - Association for Computing Machinery
SN  - 978-1-60558-989-3
UR  - https://doi.org/10.1145/1921081.1921092
ER  - 

TY  - CONF
TI  - Scaling up Dynamic Topic Models
AU  - Bhadury, Arnab
AU  - Chen, Jianfei
AU  - Zhu, Jun
AU  - Liu, Shixia
T3  - WWW '16
AB  - Dynamic topic models (DTMs) are very effective in discovering topics and capturing their evolution trends in time series data. To do posterior inference of DTMs, existing methods are all batch algorithms that scan the full dataset before each update of the model and make inexact variational approximations with mean-field assumptions. Due to a lack of a more scalable inference algorithm, despite the usefulness, DTMs have not captured large topic dynamics. This paper fills this research void, and presents a fast and parallelizable inference algorithm using Gibbs Sampling with Stochastic Gradient Langevin Dynamics that does not make any unwarranted assumptions. We also present a Metropolis-Hastings based $O(1)$ sampler for topic assignments for each word token. In a distributed environment, our algorithm requires very little communication between workers during sampling (almost embarrassingly parallel) and scales up to large-scale applications. We are able to learn the largest Dynamic Topic Model to our knowledge, and learned the dynamics of 1,000 topics from 2.6 million documents in less than half an hour, and our empirical results show that our algorithm is not only orders of magnitude faster than the baselines but also achieves lower perplexity.
C1  - Republic and Canton of Geneva, CHE
C3  - Proceedings of the 25th International Conference on World Wide Web
DA  - 2016///
PY  - 2016
DO  - 10.1145/2872427.2883046
SP  - 381
EP  - 390
PB  - International World Wide Web Conferences Steering Committee
SN  - 978-1-4503-4143-1
UR  - https://doi.org/10.1145/2872427.2883046
KW  - dynamic topic model
KW  - large scale machine learning
KW  - MCMC
KW  - MPI
KW  - parallel computing
KW  - topic model
ER  - 

TY  - JOUR
TI  - Computational Health Informatics in the Big Data Age: A Survey
AU  - Fang, Ruogu
AU  - Pouyanfar, Samira
AU  - Yang, Yimin
AU  - Chen, Shu-Ching
AU  - Iyengar, S. S.
T2  - ACM Comput. Surv.
AB  - The explosive growth and widespread accessibility of digital health data have led to a surge of research activity in the healthcare and data sciences fields. The conventional approaches for health data management have achieved limited success as they are incapable of handling the huge amount of complex data with high volume, high velocity, and high variety. This article presents a comprehensive overview of the existing challenges, techniques, and future directions for computational health informatics in the big data age, with a structured analysis of the historical and state-of-the-art methods. We have summarized the challenges into four Vs (i.e., volume, velocity, variety, and veracity) and proposed a systematic data-processing pipeline for generic big data in health informatics, covering data capturing, storing, sharing, analyzing, searching, and decision support. Specifically, numerous techniques and algorithms in machine learning are categorized and compared. On the basis of this material, we identify and discuss the essential prospects lying ahead for computational health informatics in this big data age.
DA  - 2016/06//
PY  - 2016
DO  - 10.1145/2932707
VL  - 49
IS  - 1
SN  - 0360-0300
UR  - https://doi.org/10.1145/2932707
KW  - machine learning
KW  - data mining
KW  - 4V challenges
KW  - Big data analytics
KW  - clinical decision support
KW  - computational health informatics
KW  - survey
ER  - 

TY  - CONF
TI  - Scalable Inference in Max-Margin Topic Models
AU  - Zhu, Jun
AU  - Zheng, Xun
AU  - Zhou, Li
AU  - Zhang, Bo
T3  - KDD '13
AB  - Topic models have played a pivotal role in analyzing large collections of complex data. Besides discovering latent semantics, supervised topic models (STMs) can make predictions on unseen test data. By marrying with advanced learning techniques, the predictive strengths of STMs have been dramatically enhanced, such as max-margin supervised topic models, state-of-the-art methods that integrate max-margin learning with topic models. Though powerful, max-margin STMs have a hard non-smooth learning problem. Existing algorithms rely on solving multiple latent SVM subproblems in an EM-type procedure, which can be too slow to be applicable to large-scale categorization tasks.In this paper, we present a highly scalable approach to building max-margin supervised topic models. Our approach builds on three key innovations: 1) a new formulation of Gibbs max-margin supervised topic models for both multi-class and multi-label classification; 2) a simple “augment-and-collapse" Gibbs sampling algorithm without making restricting assumptions on the posterior distributions; 3) an efficient parallel implementation that can easily tackle data sets with hundreds of categories and millions of documents. Furthermore, our algorithm does not need to solve SVM subproblems. Though performing the two tasks of topic discovery and learning predictive models jointly, which significantly improves the classification performance, our methods have comparable scalability as the state-of-the-art parallel algorithms for the standard LDA topic models which perform the single task of topic discovery only. Finally, an open-source implementation is also provided at: http://www.ml-thu.net/ jun/medlda.
C1  - New York, NY, USA
C3  - Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
DA  - 2013///
PY  - 2013
DO  - 10.1145/2487575.2487658
SP  - 964
EP  - 972
PB  - Association for Computing Machinery
SN  - 978-1-4503-2174-7
UR  - https://doi.org/10.1145/2487575.2487658
KW  - inference
KW  - large-scale systems
KW  - max-margin learning
KW  - topic models
ER  - 

TY  - JOUR
TI  - Gibbs Max-Margin Topic Models with Data Augmentation
AU  - Zhu, Jun
AU  - Chen, Ning
AU  - Perkins, Hugh
AU  - Zhang, Bo
T2  - J. Mach. Learn. Res.
AB  - Max-margin learning is a powerful approach to building classifiers and structured output predictors. Recent work on max-margin supervised topic models has successfully integrated it with Bayesian topic models to discover discriminative latent semantic structures and make accurate predictions for unseen testing data. However, the resulting learning problems are usually hard to solve because of the non-smoothness of the margin loss. Existing approaches to building max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents an alternative approach by defining a new max-margin loss. Namely, we present Gibbs max-margin supervised topic models, a latent variable Gibbs classifier to discover hidden topic representations for various tasks, including classification, regression and multi-task learning. Gibbs max-margin supervised topic models minimize an expected margin loss, which is an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables and integrating out the Dirichlet variables analytically by conjugacy, we develop simple Gibbs sampling algorithms with no restrictive assumptions and no need to solve SVM subproblems. Furthermore, each step of the "augment-and-collapse" Gibbs sampling algorithms has an analytical conditional distribution, from which samples can be easily drawn. Experimental results on several medium-sized and large-scale data sets demonstrate significant improvements on time effciency. The classification performance is also improved over competitors on binary, multi-class and multi-label classification tasks.
DA  - 2014/01//
PY  - 2014
VL  - 15
IS  - 1
SP  - 1073
EP  - 1110
SN  - 1532-4435
KW  - support vector machines
KW  - max-margin learning
KW  - Gibbs classifiers
KW  - regularized Bayesian inference
KW  - supervised topic models
ER  - 

TY  - CONF
TI  - Predictive Modeling of Diabetic Kidney Disease Using Random Forest Algorithm along with Features Selection
AU  - Xu, Hongxia
AU  - Kong, Yonghui
AU  - Tan, Shaofeng
T3  - ISAIMS '20
AB  - At present, the number of diabetes mellitus patients in China ranks first in the world, and diabetic kidney disease is the most common disease in complications. Therefore, it is necessary to establish a predictive model for early diagnosis of diabetic kidney disease. The model predicts the risk of diabetic kidney disease in the general Asian population, and recognizes high-risk groups, then warns the onset of diabetes. The data were obtained from the electronic medical record of patients in Beijing Pinggu Hospital. Twenty-nine initial candidate indicators including age, ALB, and A/C were selected. The random forest algorithm was used to predict diabetic kidney disease, and the classification accuracy was 89.831%. The importance weight ratio of each factor index was also given, Microalbuminuria (ALB), albumin-to-creatinine ratio (A/C), serum creatinine (SCr), Serum albumin (umALB), and blood urea nitrogen (BUN) accounted for a relatively high proportion of the weight of the characteristic variables. So these five indicators can be the primary indicators of our classification prediction, and the accuracy can reach 87.453%. Some other typical classification algorithms, liking KNN, logistic regression, and decision tree, were compared to classify and predict diabetic kidney disease, and precision recall f1-score and area AUC under ROC curve were used to evaluate these models. By experiments, random forest model was better than other algorithm models on both the classification accuracy and the evaluation indicators. The results can be applied to the screening of patients with high risk of diabetic kidney disease and the guidance of risk intervention measures. Consequently, the detection rate of undiagnosed diabetic kidney disease in the population can be improved, and the prevention effect of diabetic kidney disease can be enhanced as well.
C1  - New York, NY, USA
C3  - Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences
DA  - 2020///
PY  - 2020
DO  - 10.1145/3429889.3429894
SP  - 23
EP  - 27
PB  - Association for Computing Machinery
SN  - 978-1-4503-8860-3
UR  - https://doi.org/10.1145/3429889.3429894
KW  - risk prediction
KW  - Random forest
KW  - diabetic kidney disease
ER  - 

TY  - CONF
TI  - Predictive Modelling for Chronic Disease: Machine Learning Approach
AU  - Hoque, Md. Rakibul
AU  - Rahman, Mohammed Sajedur
T3  - ICCDA 2020
AB  - Chronic diseases are responsible for half of annual mortality (51%) and almost half of the burden of all diseases (41%) in Bangladesh. Developing countries like Bangladesh are in a probable state of approximate loss of $7.3 trillion due to chronic diseases by 2025. Healthcare industries in Bangladesh now generate, collect, and store large amount of data. With the emergence of big data analytics, the approach to determine the factors causing specific effects on health is increasingly based on machine learning techniques. Therefore, it is important to conduct a predictive big data analysis using machine learning techniques to understand the likelihood of chronic diseases, specifically diabetes, hypertension, and heart diseases that are caused by age, income, and years of diseases. The aim of this research is to develop a predictive analytics tool for chronic diseases using machine learning techniques. The application of machine learning in the healthcare sector can minimize the costs of treatment and can help in taking proactive actions.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis
DA  - 2020///
PY  - 2020
DO  - 10.1145/3388142.3388174
SP  - 97
EP  - 101
PB  - Association for Computing Machinery
SN  - 978-1-4503-7644-0
UR  - https://doi.org/10.1145/3388142.3388174
KW  - Machine Learning
KW  - Big Data
KW  - Chronic Disease
KW  - Predictive Analytics
ER  - 

TY  - JOUR
TI  - Multi-Disease Predictive Analytics: A Clinical Knowledge-Aware Approach
AU  - Qiu, Lin
AU  - Gorantla, Sruthi
AU  - Rajan, Vaibhav
AU  - Tan, Bernard C. Y.
T2  - ACM Trans. Manage. Inf. Syst.
AB  - Multi-Disease Predictive Analytics (MDPA) models simultaneously predict the risks of multiple diseases in patients and are valuable in early diagnoses. Patients tend to have multiple diseases simultaneously or develop multiple complications over time, and MDPA models can learn and effectively utilize such correlations between diseases. Data from large-scale Electronic Health Records (EHR) can be used through Multi-Label Learning (MLL) methods to develop MDPA models. However, data-driven approaches for MDPA face the challenge of data imbalance, because rare diseases tend to have much less data than common diseases. Insufficient data for rare diseases makes it difficult to leverage correlations with other diseases. These correlations are studied and recorded in biomedical literature but are rarely utilized in predictive analytics. This article presents a novel method called Knowledge-Aware Approach (KAA) that learns clinical correlations from the rapidly growing body of clinical knowledge. KAA can be combined with any data-driven MLL model for MDPA to refine the predictions of the model. Our extensive experiments, on real EHR data, show that the use of KAA improves the predictive performance of commonly used MDPA models, particularly for rare diseases. KAA is also found to be superior to existing general approaches of combining clinical knowledge with data-driven models. Further, a counterfactual analysis shows the efficacy of KAA in improving physicians’ ability to prescribe preventive treatments.
DA  - 2021/06//
PY  - 2021
DO  - 10.1145/3447942
VL  - 12
IS  - 3
SN  - 2158-656X
UR  - https://doi.org/10.1145/3447942
KW  - knowledge graph
KW  - biomedical literature
KW  - diagnosis prediction
KW  - Electronic health records
KW  - multi-label learning
KW  - rare diseases
ER  - 

TY  - CONF
TI  - Domain Ontology Driven Data Mining: A Medical Case Study
AU  - Kuo, Yen-Ting
AU  - Lonie, Andrew
AU  - Sonenberg, Liz
AU  - Paizis, Kathy
T3  - DDDM '07
AB  - This paper reports a domain ontology-driven approach to data mining on a medical database containing clinical data on patients undergoing treatment for chronic kidney disease. Each record within the dataset is comprised of a large number (up to 96) of quantitative and qualitative metrics which represent the physiological state of a particular patient on a particular day of treatment. One of the challenges of mining such a dataset is that the meaning of many of the metrics/attributes is not easily understood by someone who is not familiar with the domain of kidney disease and treatment, and it is not clear which of the attributes are useful in data mining.This paper explores the possibility of utilizing a medical domain ontology as a source of domain knowledge to aid in both extracting knowledge and expressing the extracted knowledge in a useful format. We describe an approach in which the domain ontology is used to categorize attributes in preparation for mining 'association rules' in the data; the mined rules were then reviewed by comparison to domain knowledge derived from a domain expert in order to gauge their 'usefulness'. We conclude that domain ontology driven data mining can obtain more meaningful results than naïve mining.
C1  - New York, NY, USA
C3  - Proceedings of the 2007 International Workshop on Domain Driven Data Mining
DA  - 2007///
PY  - 2007
DO  - 10.1145/1288552.1288554
SP  - 11
EP  - 17
PB  - Association for Computing Machinery
SN  - 978-1-59593-846-6
UR  - https://doi.org/10.1145/1288552.1288554
ER  - 

TY  - CONF
TI  - Classification of Glomeruli with Membranous Nephropathy on Renal Digital Pathological Images with Deep Learning
AU  - Hao, Fang
AU  - Li, Ming
AU  - Liu, Xueyu
AU  - Li, Xinyu
AU  - Yue, Junhong
AU  - Han, Weixia
T3  - CAIH2020
AB  - Membranous nephropathy (MN) is the one of the most common pathological types that cause adult nephrotic syndrome (NS). Recently, the incidence of MN has shown a clear upward trend. Nevertheless, there is no more accurate and fast artificial intelligence algorithm for diagnose of MN which work is laborintensive and time-consuming if it is done manually. In this article, MN-Net, a CNN-based method, is applied to glomeruli detection and classification on whole slide images (WSIs). This work is mainly divided into two parts, a glomerulus detection network and a classification network. The detection network is utilized to locate glomeruli on WSIs. Multiple instance learning (MIL), a weakly supervised classification network following detection network classifies the glomeruli detected earlier. Our network is training on PASM-stained WSIs of 1281 cases collected from multi-centers. Experimental results prove that our method is effective with a high precision of 99.66% for glomeruli detection and 99.53% for MN glomeruli classification on this dataset. In summary, this method has been proved to be an effective method with advantages of speed, high accuracy, strong robustness, and low cost of data annotation that can be applied to the diagnosis of renal pathology. In the future, this method can also be extended to the classification of other glomerular diseases under light microscope (LM). The introduction of glomerular basement membrane (GBM) segmentation and measurement models can further improve the reliability of this model.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare
DA  - 2020///
PY  - 2020
DO  - 10.1145/3433996.3434486
SP  - 239
EP  - 243
PB  - Association for Computing Machinery
SN  - 978-1-4503-8864-1
UR  - https://doi.org/10.1145/3433996.3434486
KW  - Deep learning
KW  - Classification
KW  - Artificial intelligence
KW  - Membranous Nephropathy
ER  - 

TY  - JOUR
TI  - Cross-Corpora Unsupervised Learning of Trajectories in Autism Spectrum Disorders
AU  - Elibol, Huseyin Melih
AU  - Nguyen, Vincent
AU  - Linderman, Scott
AU  - Johnson, Matthew
AU  - Hashmi, Amna
AU  - Doshi-Velez, Finale
T2  - J. Mach. Learn. Res.
AB  - Patients with developmental disorders, such as autism spectrum disorder (ASD), present with symptoms that change with time even if the named diagnosis remains fixed. For example, language impairments may present as delayed speech in a toddler and difficulty reading in a school-age child. Characterizing these trajectories is important for early treatment. However, deriving these trajectories from observational sources is challenging: electronic health records only reflect observations of patients at irregular intervals and only record what factors are clinically relevant at the time of observation. Meanwhile, caretakers discuss daily developments and concerns on social media.In this work, we present a fully unsupervised approach for learning disease trajectories from incomplete medical records and social media posts, including cases in which we have only a single observation of each patient. In particular, we use a dynamic topic model approach which embeds each disease trajectory as a path in RD. A Pólya-gamma augmentation scheme is used to efficiently perform inference as well as incorporate multiple data sources. We learn disease trajectories from the electronic health records of 13,435 patients with ASD and the forum posts of 13,743 caretakers of children with ASD, deriving interesting clinical insights as well as good predictions.
DA  - 2016/01//
PY  - 2016
VL  - 17
IS  - 1
SP  - 4597
EP  - 4634
SN  - 1532-4435
KW  - dynamic topic model
KW  - disease progression model
ER  - 

TY  - CONF
TI  - Classification Performance Analysis in Medical Science: Using Kidney Disease Data
AU  - Jeewantha, R. A.
AU  - Halgamuge, Malka N.
AU  - Mohammad, Azeem
AU  - Ekici, Gullu
T3  - ICBDR '17
AB  - Health-care practices face data storage problems in the growing world. Huge data storage demands have caused undeniable data storage problems leaving health practitioners exclaimed. Without delay, accumulated data becomes too difficult to analyzed and handled by traditional approaches. A solution to this problem is urgently needed. One possible answer to this problem is Data mining that delivers the technology and procedure to convert these embankments of ordinary data into meaningful evidences for futuristic planning and decision-making. Data mining is a tool that not only solves the problem of piled up data; nonetheless it similarly turns it into meaningful data themes based on reoccurrences of trends in the data. The healthcare trade is mostly an "information and document rich industry," and manual handling is not feasible in practical life. These huge volumes of data have been key to the arena of data-mining to generate associations among the attributes and extract expedient information. Recent research shows that combating Kidney diseases is a complex assignment that involves considerable knowledge and experience for annual testing and screening. In developed nations, Kidney diseases have become a silent killer, that makes key factors of disease burden in third world nations. Various data mining procedures are available for forecasting diseases such as clustering, classification, association rules, regression, and summarizations. The key objective of this study is to analyze datasets collected from 400 patients grounded on 25 different attributes attended for treatment for Chronic Kidney Disease (CKD), after using classification methods to forecast class precisely. Our analysis illustrates that Multilayer Perceptron is the most suitable classification method that outperforms the highest classification accuracy by 99.75% (0.0085 error) with only 5% of fluctuation among algorithm measures. Introspectively, the computational time, Multilayer Perception can be time-consuming comparatively, when it comes to deal with billions of data. Nonetheless, for the field of bioinformatics and medical science accuracy, the key objective is to deal with sensitive data because, a single error can lead to a disastrous confidentiality breech. Hence, our results show that Multilayer Perception classification method is the most accurate and suitable classification algorithm that could be used in the field of bioinformatics and medical science, for further data analysis and predictions. This paper will be useful for many medical institutions and work-related bioinformatics in pursuance to understand the prediction accuracies of data patterns in related work.
C1  - New York, NY, USA
C3  - Proceedings of the 1st International Conference on Big Data Research
DA  - 2017///
PY  - 2017
DO  - 10.1145/3152723.3152724
SP  - 1
EP  - 6
PB  - Association for Computing Machinery
SN  - 978-1-4503-5356-4
UR  - https://doi.org/10.1145/3152723.3152724
KW  - Classification
KW  - Machine Learning
KW  - Multilayer Perceptron
KW  - Big Data
KW  - Chronic Kidney Disease (CKD)
ER  - 

TY  - CONF
TI  - Knowledge Distillation In Medical Data Mining: A Survey
AU  - Meng, Hefeng
AU  - Lin, Zhiqiang
AU  - Yang, Fan
AU  - Xu, Yonghui
AU  - Cui, Lizhen
T3  - ICCSE '21
AB  - In recent years, there have always been many problems in the medical field, such as a shortage of professionals and a shortage of medical resources. With the application of machine learning in the medical field, these problems have been alleviated to a certain extent, but these machine learning methods also have shortcomings, such as models are often too large to be deployed on lightweight equipment, and medical data sets are difficult to share, Many researchers have put forward many methods, and knowledge distillation is one of them. As a model compression and acceleration technology, knowledge distillation has been widely used in the medical field. The research of many researchers also shows that the use of knowledge distillation can effectively compress huge and complex models and improve the performance of models. Many studies show that the use of knowledge distillation can effectively solve many problems existing in models in the medical field, Aiming at the various applications of knowledge distillation in the medical field, this paper makes a comprehensive review from the perspectives of knowledge distillation, the problems that knowledge distillation can solve in the medical field and the practical application of knowledge distillation.
C1  - New York, NY, USA
C3  - 5th International Conference on Crowd Science and Engineering
DA  - 2022///
PY  - 2022
DO  - 10.1145/3503181.3503211
SP  - 175
EP  - 182
PB  - Association for Computing Machinery
SN  - 978-1-4503-9554-0
UR  - https://doi.org/10.1145/3503181.3503211
KW  - Medical diagnosis
KW  - Knowledge distillation
KW  - Medical image classification
KW  - Model compression
ER  - 

TY  - CONF
TI  - Unsupervised Learning of Disease Progression Models
AU  - Wang, Xiang
AU  - Sontag, David
AU  - Wang, Fei
T3  - KDD '14
AB  - Chronic diseases, such as Alzheimer's Disease, Diabetes, and Chronic Obstructive Pulmonary Disease, usually progress slowly over a long period of time, causing increasing burden to the patients, their families, and the healthcare system. A better understanding of their progression is instrumental in early diagnosis and personalized care. Modeling disease progression based on real-world evidence is a very challenging task due to the incompleteness and irregularity of the observations, as well as the heterogeneity of the patient conditions. In this paper, we propose a probabilistic disease progression model that address these challenges. As compared to existing disease progression models, the advantage of our model is three-fold: 1) it learns a continuous-time progression model from discrete-time observations with non-equal intervals; 2) it learns the full progression trajectory from a set of incomplete records that only cover short segments of the progression; 3) it learns a compact set of medical concepts as the bridge between the hidden progression process and the observed medical evidence, which are usually extremely sparse and noisy. We demonstrate the capabilities of our model by applying it to a real-world COPD patient cohort and deriving some interesting clinical insights.
C1  - New York, NY, USA
C3  - Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
DA  - 2014///
PY  - 2014
DO  - 10.1145/2623330.2623754
SP  - 85
EP  - 94
PB  - Association for Computing Machinery
SN  - 978-1-4503-2956-9
UR  - https://doi.org/10.1145/2623330.2623754
KW  - bayesian network
KW  - disease progression modeling
KW  - markov jump process
KW  - medical informatics
ER  - 

TY  - JOUR
TI  - Readmission Prediction for Patients with Heterogeneous Medical History: A Trajectory-Based Deep Learning Approach
AU  - Xie, Jiaheng
AU  - Zhang, Bin
AU  - Ma, Jian
AU  - Zeng, Daniel
AU  - Lo-Ciganic, Jenny
T2  - ACM Trans. Manage. Inf. Syst.
AB  - Hospital readmission refers to the situation where a patient is re-hospitalized with the same primary diagnosis within a specific time interval after discharge. Hospital readmission causes $26 billion preventable expenses to the U.S. health systems annually and often indicates suboptimal patient care. To alleviate those severe financial and health consequences, it is crucial to proactively predict patients’ readmission risk. Such prediction is challenging because the evolution of patients’ medical history is dynamic and complex. The state-of-the-art studies apply statistical models which use static predictors in a period, failing to consider patients’ heterogeneous medical history. Our approach – Trajectory-BAsed DEep Learning (TADEL) – is motivated to tackle the deficiencies of the existing approaches by capturing dynamic medical history. We evaluate TADEL on a five-year national Medicare claims dataset including 3.6 million patients per year over all hospitals in the United States, reaching an F1 score of 87.3% and an AUC of 88.4%. Our approach significantly outperforms all the state-of-the-art methods. Our findings suggest that health status factors and insurance coverage are important predictors for readmission. This study contributes to IS literature and analytical methodology by formulating the trajectory-based readmission prediction problem and developing a novel deep-learning-based readmission risk prediction framework. From a health IT perspective, this research delivers implementable methods to assess patients’ readmission risk and take early interventions to avoid potential negative consequences.
DA  - 2021/10//
PY  - 2021
DO  - 10.1145/3468780
VL  - 13
IS  - 2
SN  - 2158-656X
UR  - https://doi.org/10.1145/3468780
KW  - deep learning
KW  - Hospital readmission
KW  - computational design science
KW  - predictive analytics
ER  - 

TY  - CONF
TI  - Fine-Grained Concept Linking Using Neural Networks in Healthcare
AU  - Dai, Jian
AU  - Zhang, Meihui
AU  - Chen, Gang
AU  - Fan, Ju
AU  - Ngiam, Kee Yuan
AU  - Ooi, Beng Chin
T3  - SIGMOD '18
AB  - To unlock the wealth of the healthcare data, we often need to link the real-world text snippets to the referred medical concepts described by the canonical descriptions. However, existing healthcare concept linking methods, such as dictionary-based and simple machine learning methods, are not effective due to the word discrepancy between the text snippet and the canonical concept description, and the overlapping concept meaning among the fine-grained concepts. To address these challenges, we propose a Neural Concept Linking (NCL) approach for accurate concept linking using systematically integrated neural networks. We call the novel neural network architecture as the COMposite AttentIonal encode-Decode neural network (COM-AID). COM-AID performs an encode-decode process that encodes a concept into a vector and decodes the vector into a text snippet with the help of two devised contexts. On the one hand, it injects the textual context into the neural network through the attention mechanism, so that the word discrepancy can be overcome from the semantic perspective. On the other hand, it incorporates the structural context into the neural network through the attention mechanism, so that minor concept meaning differences can be enlarged and effectively differentiated. Empirical studies on two real-world datasets confirm that the NCL produces accurate concept linking results and significantly outperforms state-of-the-art techniques.
C1  - New York, NY, USA
C3  - Proceedings of the 2018 International Conference on Management of Data
DA  - 2018///
PY  - 2018
DO  - 10.1145/3183713.3196907
SP  - 51
EP  - 66
PB  - Association for Computing Machinery
SN  - 978-1-4503-4703-7
UR  - https://doi.org/10.1145/3183713.3196907
KW  - attention mechanism
KW  - neural networks
KW  - healthcare
KW  - fine-grained concept linking
ER  - 

TY  - CONF
TI  - DeepSI: Interactive Deep Learning for Semantic Interaction
AU  - Bian, Yali
AU  - North, Chris
T3  - IUI '21
AB  - In this paper, we design novel interactive deep learning methods to improve semantic interactions in visual analytics applications. The ability of semantic interaction to infer analysts’ precise intents during sensemaking is dependent on the quality of the underlying data representation. We propose the DeepSIfinetune framework that integrates deep learning into the human-in-the-loop interactive sensemaking pipeline, with two important properties. First, deep learning extracts meaningful representations from raw data, which improves semantic interaction inference. Second, semantic interactions are exploited to fine-tune the deep learning representations, which then further improves semantic interaction inference. This feedback loop between human interaction and deep learning enables efficient learning of user- and task-specific representations. To evaluate the advantage of embedding the deep learning within the semantic interaction loop, we compare DeepSIfinetune against a state-of-the-art but more basic use of deep learning as only a feature extractor pre-processed outside of the interactive loop. Results of two complementary studies, a human-centered qualitative case study and an algorithm-centered simulation-based quantitative experiment, show that DeepSIfinetune more accurately captures users’ complex mental models with fewer interactions.
C1  - New York, NY, USA
C3  - 26th International Conference on Intelligent User Interfaces
DA  - 2021///
PY  - 2021
DO  - 10.1145/3397481.3450670
SP  - 197
EP  - 207
PB  - Association for Computing Machinery
SN  - 978-1-4503-8017-1
UR  - https://doi.org/10.1145/3397481.3450670
KW  - BERT
KW  - Interactive Deep Learning
KW  - Semantic Interaction
KW  - Visual Analytics
ER  - 

TY  - CONF
TI  - Medical Entity Disambiguation Using Graph Neural Networks
AU  - Vretinaris, Alina
AU  - Lei, Chuan
AU  - Efthymiou, Vasilis
AU  - Qin, Xiao
AU  - Özcan, Fatma
T3  - SIGMOD '21
AB  - Medical knowledge bases (KBs), distilled from biomedical literature and regulatory actions, are expected to provide high-quality information to facilitate clinical decision making. Entity disambiguation (also referred to as entity linking) is considered as an essential task in unlocking the wealth of such medical KBs. However, existing medical entity disambiguation methods are not adequate due to word discrepancies between the entities in the KB and the text snippets in the source documents. Recently, graph neural networks (GNNs) have proven to be very effective and provide state-of-the-art results for many real-world applications with graph-structured data. In this paper, we introduce ED-GNN based on three representative GNNs (GraphSAGE, R-GCN, and MAGNN) for medical entity disambiguation. We develop two optimization techniques to fine-tune and improve ED-GNN. First, we introduce a novel strategy to represent entities that are mentioned in text snippets as a query graph. Second, we design an effective negative sampling strategy that identifies hard negative samples to improve the model's disambiguation capability. Compared to the best performing state-of-the-art solutions, our ED-GNN offers an average improvement of 7.3% in terms of F1 score on five real-world datasets.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 International Conference on Management of Data
DA  - 2021///
PY  - 2021
DO  - 10.1145/3448016.3457328
SP  - 2310
EP  - 2318
PB  - Association for Computing Machinery
SN  - 978-1-4503-8343-1
UR  - https://doi.org/10.1145/3448016.3457328
KW  - entity disambiguation
KW  - graph neural network
KW  - medical ontology
ER  - 

TY  - CONF
TI  - Infer Cause of Death for Population Health Using Convolutional Neural Network
AU  - Wu, Hang
AU  - Wang, May D.
T3  - ACM-BCB '17
AB  - In biomedical data analysis, inferring the cause of death is a challenging and important task, which is useful for both public health reporting purposes, as well as improving patients' quality of care by identifying severer conditions. Causal inference, however, is notoriously difficult. Traditional causal inference mainly relies on analyzing data collected from experiment of specific design, which is expensive, and limited to a certain disease cohort, making the approach less generalizable. In our paper, we adopt a novel data-driven perspective to analyze and improve the death reporting process, to assist physicians identify the single underlying cause of death. To achieve this, we build state-of-the-art deep learning models, convolution neural network (CNN), and achieve around 75% accuracy in predicting the single underlying cause of death from a list of relevant medical conditions. We also provide interpretations for the black-box neural network models, so that death reporting physicians can apply the model with better understanding of the model.
C1  - New York, NY, USA
C3  - Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics
DA  - 2017///
PY  - 2017
DO  - 10.1145/3107411.3107447
SP  - 526
EP  - 535
PB  - Association for Computing Machinery
SN  - 978-1-4503-4722-8
UR  - https://doi.org/10.1145/3107411.3107447
KW  - deep learning
KW  - interpretability
KW  - causal inference
ER  - 

TY  - CONF
TI  - Prediction Model of Blood Pressure during Hemodialysis Base on Deep Learning
AU  - Wang, Guan
AU  - Fan, Haojie
T3  - ICCAI 2021
AB  - Hemodialysis is the main treatment for patients with renal failure. Significant complications associated with treatment include hypotension, cramps, insufficient blood flow, and arrhythmia. Most complications are related to unstable blood pressure during hemodialysis. Although the science and technology and computer industry have made great progress in recent years, the problem of blood pressure prediction during hemodialysis is still a big challenge. Aiming at the problem that the shallow model used in the current research does not consider the high-dimensional nonlinear combination characteristics of hemodialysis data, this paper proposes a blood pressure prediction model during hemodialysis based on deep belief network (DBN) and support vector regression (SVR). In this model, DBN extracts the non-linear combination features of hemodialysis data layer by layer, and then transfers the extracted high-dimensional features to the top-level SVR for regression prediction. The experimental results show that the mean absolute error (MAE) of the model is 3.79, the root mean square error (RMSE) is 9.01, and is 0.88. Compared with the shallow model used in the current research, the prediction effect has been significantly improved.
C1  - New York, NY, USA
C3  - 2021 7th International Conference on Computing and Artificial Intelligence
DA  - 2021///
PY  - 2021
DO  - 10.1145/3467707.3467772
SP  - 431
EP  - 436
PB  - Association for Computing Machinery
SN  - 978-1-4503-8950-1
UR  - https://doi.org/10.1145/3467707.3467772
KW  - deep belief network
KW  - blood pressure during hemodialysis
KW  - intradialysis hypotension
KW  - support vector regression
ER  - 

TY  - CONF
TI  - IMR Based Anonymization for Privacy Preservation in Data Mining
AU  - Arumugam, G.
AU  - Sulekha, V. Jane Varamani
T3  - KMO '16
AB  - Privacy Preserving Data Mining (PPDM) is a data mining research area that aims to protect individual's personal information from unsolicited or unauthorized disclosure. Privacy relates to personal information that a person would not wish others to know without authorization, and to a person's right to be free from the attention of others (UN Declaration of Human Rights, 1948). Advances in computer technology, social networking sites, communications and improvement in hardware technologies have made it possible to collect and store huge amounts of data in digital form. This results in the increasing ability to collect large amounts of personal information. However the collection and sharing of data have also raised a number of ethical issues. Such issues include privacy, data security, and intellectual property rights. These Privacy Issues violates a person's right and brings dignitary harm to the research participant. In addition, it would also bring social mortification, shame, disgrace, and damage to social and economic status. In recent years, numerous data mining algorithms combining privacy preserving techniques have been developed that hide sensitive item sets or patterns. An important issue here is to decide which privacy preserving technique gives better protection for sensitive information. It is also important to access the quality of the result as well as the performance of the algorithm, after applying the privacy preserving techniques. In this paper, we propose IMR (I - Buckets based Map Reducing) based Anonymization and present a brief introduction of different kinds of PPDM techniques with their merits and demerits. Our work highlights some considerations about future work and promising directions in the perspective of privacy preservation in data mining.
C1  - New York, NY, USA
C3  - Proceedings of the The 11th International Knowledge Management in Organizations Conference on The Changing Face of Knowledge Management Impacting Society
DA  - 2016///
PY  - 2016
DO  - 10.1145/2925995.2926005
PB  - Association for Computing Machinery
SN  - 978-1-4503-4064-9
UR  - https://doi.org/10.1145/2925995.2926005
KW  - Security
KW  - Data Mining
KW  - Anonymization
KW  - PPDM
KW  - Privacy
KW  - Privacy Preservation
ER  - 

TY  - CONF
TI  - Machine Learning Support for Kidney Transplantation Decision Making
AU  - Reinaldo, Francisco
AU  - Rahman, Md. Anishur
AU  - Alves, Carlos Fernandes
AU  - Malucelli, Andreia
AU  - Camacho, Rui
T3  - ISB '10
AB  - Organ transplantation is a highly complex decision process that requires expert decisions. The major problem in a transplantation procedure is the possibility of the receiver's immune system attack and destroy the transplanted tissue. It is therefore of capital importance to find a donor with the highest possible compatibility with the receiver, and thus reduce rejection. Finding a good donor is not a straight-forward task because a complex network of relations exists between the immunological and the clinical variables that influence the receiver's acceptance of the transplanted organ. Currently the process of analysis of these variables involves a careful study by the clinical transplant team. The number and complexity of causal dependencies among variables make the manual process very slow. In this paper we assess the usefulness of Machine Learning algorithms as a tool to improve and speed up the decisions of a transplant team. We achieve that objective by analysing past real cases and constructing models as set of rules. Such models are accurate and understandable by experts.
C1  - New York, NY, USA
C3  - Proceedings of the International Symposium on Biocomputing
DA  - 2010///
PY  - 2010
DO  - 10.1145/1722024.1722079
PB  - Association for Computing Machinery
SN  - 978-1-60558-722-6
UR  - https://doi.org/10.1145/1722024.1722079
KW  - machine learning
KW  - kidney transplantation
ER  - 

TY  - CONF
TI  - StageNet: Stage-Aware Neural Networks for Health Risk Prediction
AU  - Gao, Junyi
AU  - Xiao, Cao
AU  - Wang, Yasha
AU  - Tang, Wen
AU  - Glass, Lucas M.
AU  - Sun, Jimeng
T3  - WWW '20
AB  - Deep learning has demonstrated success in health risk prediction especially for patients with chronic and progressing conditions. Most existing works focus on learning disease patterns from longitudinal patient data, but pay little attention to the disease progression stage itself. To fill the gap, we propose a Stage-aware neural Network (StageNet) model to extract disease stage information from patient data and integrate it into risk prediction. StageNet is enabled by (1) a stage-aware long short-term memory (LSTM) module that extracts health stage variations unsupervisedly; (2) a stage-adaptive convolutional module that incorporates stage-related progression patterns into risk prediction. We evaluate StageNet on two real-world datasets and show that StageNet outperforms state-of-the-art models in risk prediction task and patient subtyping task. Compared to the best baseline model, StageNet achieves up to 12% higher AUPRC for risk prediction task on two real-world patient datasets. StageNet also achieves over 58% higher Calinski-Harabasz score (a cluster quality metric) for a patient subtyping task.
C1  - New York, NY, USA
C3  - Proceedings of The Web Conference 2020
DA  - 2020///
PY  - 2020
DO  - 10.1145/3366423.3380136
SP  - 530
EP  - 540
PB  - Association for Computing Machinery
SN  - 978-1-4503-7023-3
UR  - https://doi.org/10.1145/3366423.3380136
KW  - risk prediction
KW  - electronic health record
KW  - healthcare informatics
ER  - 

TY  - CONF
TI  - A Data Mining Approach to MPGN Type II Renal Survival Analysis
AU  - Yang, Chen
AU  - Street, Nick W.
AU  - Lu, Der-Fa
AU  - Lanning, Lynne
T3  - IHI '10
AB  - There are three recognized types of Membranoproliferative glomerulonephritis (MPGN). Type II or Dense Deposit Disease (DDD)has a renal survival of 50% at 10 years. The goal of this study was to better identify patients at high risk of early renal failure,and to understand the factors that lead to fast progression of the disease. We identified six diagnostic features on the 98 DDD patients who responed to a Web-based survey, and examined the prognostic performance of these features in isolation and simple combinations. We then combined the features to build predictive models using both Cox proportional hazards regression (CHR), a standard statistical approach, and support vector machines (SVMs), a classification technique from the data mining literature. While the age and gender features showed some prognostic ability, the combined models – particularly the SVM – were superior in identifying cases with fast disease progression.This approach can be applied to disease survival analysis and prognosis, and might be useful to healthcare providers and patients in making healthcare decisions.
C1  - New York, NY, USA
C3  - Proceedings of the 1st ACM International Health Informatics Symposium
DA  - 2010///
PY  - 2010
DO  - 10.1145/1882992.1883062
SP  - 454
EP  - 458
PB  - Association for Computing Machinery
SN  - 978-1-4503-0030-8
UR  - https://doi.org/10.1145/1882992.1883062
KW  - data mining
KW  - chr
KW  - ddd
KW  - survival analysis
KW  - svm
ER  - 

TY  - JOUR
TI  - Membership Inference Attacks on Machine Learning: A Survey
AU  - Hu, Hongsheng
AU  - Salcic, Zoran
AU  - Sun, Lichao
AU  - Dobbie, Gillian
AU  - Yu, Philip S.
AU  - Zhang, Xuyun
T2  - ACM Comput. Surv.
AB  - Machine learning (ML) models have been widely applied to various applications, including image classification, text generation, audio recognition, and graph data analysis. However, recent studies have shown that ML models are vulnerable to membership inference attacks (MIAs), which aim to infer whether a data record was used to train a target model or not. MIAs on ML models can directly lead to a privacy breach. For example, via identifying the fact that a clinical record that has been used to train a model associated with a certain disease, an attacker can infer that the owner of the clinical record has the disease with a high chance. In recent years, MIAs have been shown to be effective on various ML models, e.g., classification models and generative models. Meanwhile, many defense methods have been proposed to mitigate MIAs. Although MIAs on ML models form a newly emerging and rapidly growing research area, there has been no systematic survey on this topic yet. In this article, we conduct the first comprehensive survey on membership inference attacks and defenses. We provide the taxonomies for both attacks and defenses, based on their characterizations, and discuss their pros and cons. Based on the limitations and gaps identified in this survey, we point out several promising future research directions to inspire the researchers who wish to follow this area. This survey not only serves as a reference for the research community but also provides a clear description for researchers outside this research domain. To further help the researchers, we have created an online resource repository, which we will keep updated with future relevant work. Interested readers can find the repository at https://github.com/HongshengHu/membership-inference-machine-learning-literature.
DA  - 2022/09//
PY  - 2022
DO  - 10.1145/3523273
VL  - 54
IS  - 11s
SN  - 0360-0300
UR  - https://doi.org/10.1145/3523273
KW  - deep leaning
KW  - differential privacy
KW  - Membership inference attacks
KW  - privacy risk
ER  - 

TY  - CONF
TI  - Supervised Reinforcement Learning with Recurrent Neural Network for Dynamic Treatment Recommendation
AU  - Wang, Lu
AU  - Zhang, Wei
AU  - He, Xiaofeng
AU  - Zha, Hongyuan
T3  - KDD '18
AB  - Dynamic treatment recommendation systems based on large-scale electronic health records (EHRs) become a key to successfully improve practical clinical outcomes. Prior relevant studies recommend treatments either use supervised learning (e.g. matching the indicator signal which denotes doctor prescriptions), or reinforcement learning (e.g. maximizing evaluation signal which indicates cumulative reward from survival rates). However, none of these studies have considered to combine the benefits of supervised learning and reinforcement learning. In this paper, we propose Supervised Reinforcement Learning with Recurrent Neural Network (SRL-RNN), which fuses them into a synergistic learning framework. Specifically, SRL-RNN applies an off-policy actor-critic framework to handle complex relations among multiple medications, diseases and individual characteristics. The "actor” in the framework is adjusted by both the indicator signal and evaluation signal to ensure effective prescription and low mortality. RNN is further utilized to solve the Partially-Observed Markov Decision Process (POMDP) problem due to lack of fully observed states in real world applications. Experiments on the publicly real-world dataset, i.e., MIMIC-3, illustrate that our model can reduce the estimated mortality, while providing promising accuracy in matching doctors' prescriptions.
C1  - New York, NY, USA
C3  - Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining
DA  - 2018///
PY  - 2018
DO  - 10.1145/3219819.3219961
SP  - 2447
EP  - 2456
PB  - Association for Computing Machinery
SN  - 978-1-4503-5552-0
UR  - https://doi.org/10.1145/3219819.3219961
KW  - deep sequential recommendation
KW  - dynamic treatment regime
KW  - supervised reinforcement learning
ER  - 

TY  - CONF
TI  - Deep Convolutional Neural Network Diabetic Entity Relationship Extraction Model Based on Enhanced Semantic Representation
AU  - Yu, Tao
AU  - Yong, Cuo
T3  - ISAIMS '21
AB  - Diabetes, as the number one chronic disease in China, plagues the lives of people of different age groups and causes great distress to people. This paper extracts the semantic relationships among diabetes entities through the entity relationship extraction technique, which helps to build the knowledge graph of diabetes domain. The experimental results show that the deep convolutional neural network (ESPDCNN) based on enhanced semantic representation proposed in this paper effectively improves the accuracy of the diabetic entity relationship extraction model.
C1  - New York, NY, USA
C3  - Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences
DA  - 2021///
PY  - 2021
DO  - 10.1145/3500931.3501009
SP  - 460
EP  - 465
PB  - Association for Computing Machinery
SN  - 978-1-4503-9558-8
UR  - https://doi.org/10.1145/3500931.3501009
KW  - Diabetes
KW  - DiaKG
KW  - Entity relationship extraction
KW  - ESPDCNN
ER  - 

TY  - CONF
TI  - HCNN: Heterogeneous Convolutional Neural Networks for Comorbid Risk Prediction with Electronic Health Records
AU  - Zhang, Jinghe
AU  - Gong, Jiaqi
AU  - Barnes, Laura
T3  - CHASE '17
AB  - The increasing adoption of electronic health record (EHR) systems has brought tremendous opportunities in medicine enabling more personalized prognostic models. However, most work to date has investigated the binary classification problem for predicting the onset of one chronic disease, but little attention has been given to assessing risk of developing comorbidities that are major causes of morbidity and mortality. For example, type 2 diabetes and chronic kidney disease frequently accompany congestive heart failure. This paper is motivated by the problem of predicting comorbid diseases and aims to answer the following question: can we predict the comorbid risk using a patient's medical history? We propose a new predictive learning framework, Heterogeneous Convolutional Neural Network (HCNN), that represents EHRs as graphs with heterogeneous attributes (e.g. diagnoses, procedures, and medication), and then develop a novel deep learning methodology for risk prediction of multiple comorbid diseases. The main innovation of the framework is that it defines the distance between the heterogeneous attributes of the graph representation extracted from the EHR and develops an appropriate learning infrastructure that is a composition of sparse convolutional layers and local pooling steps that match with the local structure of the space of the heterogeneous attributes. As a result, the new method is capable of capturing features about the relationships between heterogeneous attributes of the graphs. Through a comparative study on patient EHR data, HCNN achieves better performance than traditional convolutional neural networks on the risk prediction of comorbid diseases.
C3  - Proceedings of the Second IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies
DA  - 2017///
PY  - 2017
DO  - 10.1109/CHASE.2017.80
SP  - 214
EP  - 221
PB  - IEEE Press
SN  - 978-1-5090-4721-5
UR  - https://doi.org/10.1109/CHASE.2017.80
KW  - electronic health records
KW  - risk prediction
KW  - neural networks
KW  - heterogeneous convolution
ER  - 

TY  - CONF
TI  - Design of RBF Neural Network Based on Improved Canopy-K-Means Algorithm
AU  - Lijie, Jia
AU  - Wenjing, Li
AU  - Junfei, Qiao
T3  - HPCCT &amp; BDAI '20
AB  - To solve the problem that RBF neural network parameters are difficult to determine, an improved Canopy-K-means algorithm is proposed to optimize the RBF neural network. By using the density-based Canopy algorithm to roughly cluster the data, the cluster centers of the K-means algorithm are initialized, meanwhile the number of cluster centers is obtained, and the Canopy-K-means algorithm based on the density of samples (CKD) is used to optimize the RBF neural network. Three experiments, including nonlinear function approximation, classification of UCI website datasets, the effluent ammonia nitrogen (NH4-N) prediction in wastewater treatment process were used to verify the effectiveness of the algorithm. The results showed that CKDRBF network had high classification accuracy, strong approximation ability.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 4th High Performance Computing and Cluster Technologies Conference &amp; 2020 3rd International Conference on Big Data and Artificial Intelligence
DA  - 2020///
PY  - 2020
DO  - 10.1145/3409501.3409526
SP  - 58
EP  - 63
PB  - Association for Computing Machinery
SN  - 978-1-4503-7560-3
UR  - https://doi.org/10.1145/3409501.3409526
KW  - density-based Canopy algorithm
KW  - K-means algorithm
KW  - RBF Neural Network
ER  - 

TY  - CONF
TI  - Residual Speech Signal Compression: An Experiment in the Practical Application of Neural Network Technology
AU  - Pratt, Lorien
AU  - Cebulka, Kathleen D.
AU  - Clitherow, Peter
T3  - IEA/AIE '90
AB  - Neural networks are a popular area of research today. However, neural network algorithms have only recently proven valuable to application problems. This paper seeks to aid in the process of transferring neural network technology from research to a development environment by describing our experience in applying this technology.The application studied here is Speaker Identity Verification (SIV), which is the task of verifying a speaker's identity by comparing the speaker's voice pattern to a stored template.In this paper, we describe the application of the back-propagation neural network algorithm to one aspect of the SIV problem, called Residual Compression (RC). The RC problem is to extract useful features from a part of the speech signal that was not utilized by previous SIV systems. Here, we describe a neural network architecture, pre-processing algorithm, training methodology, and empirical results for this problem. We also present a few guidelines for the use of neural networks in applied settings.
C1  - New York, NY, USA
C3  - Proceedings of the 3rd International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems - Volume 2
DA  - 1990///
PY  - 1990
DO  - 10.1145/98894.99124
SP  - 1063
EP  - 1072
PB  - Association for Computing Machinery
SN  - 0-89791-372-8
UR  - https://doi.org/10.1145/98894.99124
ER  - 

TY  - CONF
TI  - Using Electronic Health Records to Accurately Predict COVID-19 Health Outcomes through a Novel Machine Learning Pipeline
AU  - Feng, Alice
T3  - BCB '21
AB  - Current COVID-19 predictive models primarily focus on predicting the risk of mortality, and rely on COVID-19 specific medical data such as chest imaging after COVID-19 diagnosis. In this project, we developed an innovative supervised machine learning pipeline using longitudinal Electronic Health Records (EHR) to accurately predict COVID-19 related health outcomes including mortality, ventilation, days in hospital or ICU. In particular, we developed unique and effective data processing algorithms, including data cleaning, initial feature screening, vector representation, and feature normalization. Then we trained models using state-of-the-art machine learning strategies combined with different parameter settings and feature selection. Based on routinely collected EHR, our machine learning pipeline not only consistently outperformed those developed by other research groups using the same data set, but also achieved similar mortality prediction accuracy as those trained on medical data available only after COVID-19 diagnosis. In addition, we identified top COVID-19 risk factors, which are consistent with epidemiologic findings.
C1  - New York, NY, USA
C3  - Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics
DA  - 2021///
PY  - 2021
DO  - 10.1145/3459930.3469490
PB  - Association for Computing Machinery
SN  - 978-1-4503-8450-6
UR  - https://doi.org/10.1145/3459930.3469490
KW  - machine learning
KW  - electronic health record
KW  - predictive model
ER  - 

TY  - CONF
TI  - Toward Personalized Treatment of Chronic Diseases: The CKDCase Study
AU  - Chen, Chih-Yang
AU  - Chou, Chun-Nan
AU  - Wu, I.-Wen
T3  - MMHealth '17
AB  - Chronic diseases greatly influence the patients' life and incur the bulk of healthcare costs. Medical treatments should be personalized to consider individual variance. In this study, we take a first step toward personalized treatment of chronic kidney disease by formulating two prediction problems. We utilize random forest to learn the prediction models, and the preliminary results look promising.
C1  - New York, NY, USA
C3  - Proceedings of the 2nd International Workshop on Multimedia for Personal Health and Health Care
DA  - 2017///
PY  - 2017
DO  - 10.1145/3132635.3132646
SP  - 73
EP  - 76
PB  - Association for Computing Machinery
SN  - 978-1-4503-5504-9
UR  - https://doi.org/10.1145/3132635.3132646
KW  - chronic kidney disease
KW  - personalized medicine
ER  - 

TY  - CONF
TI  - Health Consumer Usage Patterns in Management of CVD Using Data Mining Techniques
AU  - Nagappan, Devipriyaa
AU  - Warren, Jim
AU  - Riddle, Patricia
T3  - ACSW 2019
AB  - The Healthcare system is exposed to the increasing impact of chronic diseases including cardiovascular diseases; it is of much importance to analyze and understand the health trajectories for efficient planning and fair allotment of resources. This work proposes an approach based on mining clinical data to support the exploration of health trajectories related to cardiovascular diseases. As the health data are highly confidential, we aimed to conduct our experiments using a large, synthetic, longitudinal dataset, constituted to represent the CVD risk factors distribution and temporal sequence of events related to heart failure hospitalization and readmission.This research work analyses and represents the temporal events or states of the patient's trajectory with the aim of understanding the patient's journey in the management of the chronic condition and its complications by using data mining techniques. This study focuses on developing an efficient algorithm to find cohesive clusters for handling the temporal events. Clustering health trajectories have been carried out by proposing an improved version of the Ant-based clustering algorithm. Insights from this study can potentially result in evidence that these approaches are useful in understanding and analyzing patient's health trajectories for better management of the chronic condition and its progression.
C1  - New York, NY, USA
C3  - Proceedings of the Australasian Computer Science Week Multiconference
DA  - 2019///
PY  - 2019
DO  - 10.1145/3290688.3290732
PB  - Association for Computing Machinery
SN  - 978-1-4503-6603-8
UR  - https://doi.org/10.1145/3290688.3290732
KW  - Clustering
KW  - Cardio-vascular diseases(CVD)
KW  - Chronic diseases
KW  - Health trajectories
ER  - 

TY  - CONF
TI  - A Big Data Architecture to a Multiple Purpose in Healthcare Surveillance: The Brazilian Syphilis Case
AU  - Silva, Rodrigo Dantas da
AU  - de Araújo, Jean Jar Pereira
AU  - de Paiva, Álvaro Ferreira Pires
AU  - de Medeiros Valentim, Ricardo Alexsandro
AU  - Coutinho, Karilany Dantas
AU  - de Paiva, Jailton Carlos
AU  - Roussanaly, Azim
AU  - Boyer, Anne
T3  - EATIS '20
AB  - For many decades society did need to monitor and assess the standard of living of the population. In the 1950s, the United Nations (UN) saw this need and proposed 12 areas that should be evaluated, the first of which is listed under "Health and Demography", which focuses on what is expressed as the level of a population's health. Decades have passed and great results have been gained from similar initiatives such as reducing mortality from infectious diseases and even eradicating some others. In the age of the digital society, needs have grown. Monitoring demands that once perished from data to become concrete now suffer from the opposite effect, the excess of data from everywhere. Healthcare systems around the world use many different information systems, collecting and generating hundreds of data at unimaginable speed. We are billions of people on the planet and most of us are connected to the virtual world, sharing information, experiences and events with some kind of cloud. In this information age, the ability to aggregate and process this data is a major factor in raising public health to a new level. The development of tools capable of analyzing a large volume of data in seconds and producing knowledge for targeted decision making can help in the fight against specific diseases, in the process of continuing education of professionals, in the formation of new professionals, in the elaboration of new policies. with the specific locoregional look, in the analysis of hidden trends in front of so much information faced in everyday life and other possibilities. The present work proposes an architecture capable of storing and manipulating seeking to standardize the variables in order to allow to correlate this large amount of data in a systematic way, providing to several services and researchers the possibility of consuming health, social, economic and educational data for the promotion of public health.
C1  - New York, NY, USA
C3  - Proceedings of the 10th Euro-American Conference on Telematics and Information Systems
DA  - 2021///
PY  - 2021
DO  - 10.1145/3401895.3402092
PB  - Association for Computing Machinery
SN  - 978-1-4503-7711-9
UR  - https://doi.org/10.1145/3401895.3402092
KW  - big data
KW  - epidemiology
KW  - healthcare surveillance
KW  - syphilis
ER  - 

TY  - CONF
TI  - Multivariate Multi-Step Deep Learning Time Series Approach in Forecasting Parkinson's Disease Future Severity Progression
AU  - Ismail, Nur Hafieza
AU  - Du, Mengnan
AU  - Martinez, Diego
AU  - He, Zhe
T3  - BCB '19
AB  - Parkinson's disease is a neurodegenerative disorder that affects the dopamine neurons production in the middle part of the brain. It is also recognized as the second most common degenerative nerve disorder in the United States after Alzheimer's disease. About 1% of the world population which estimated 7 to 10 million people with an average age of 62 are PD sufferers. Every year, approximately 60,000 Americans are diagnosed with PD, and the researchers believe this number will continue to grow. By providing a computational prognosis tool for PD, using patients' dataset containing clinical PD rating scale based on speech features could alleviate the PD progression. It can help a PD patient in monitoring the progress of unusual symptoms that they are currently facing based on previous and current recorded speech. This paper proposes a multi-step time series approach to forecasting the PD symptoms progression model using a deep neural network method, multichannel convolutional neural network (CNN). The experimental results show that our model could remarkably help in the forecasting of PD progression in the coming week/s.
C1  - New York, NY, USA
C3  - Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics
DA  - 2019///
PY  - 2019
DO  - 10.1145/3307339.3342185
SP  - 383
EP  - 389
PB  - Association for Computing Machinery
SN  - 978-1-4503-6666-3
UR  - https://doi.org/10.1145/3307339.3342185
KW  - deep neural network
KW  - disease progression
KW  - multi-step time series forecasting
KW  - multivariate data
KW  - parkinson's disease
ER  - 

TY  - CONF
TI  - Hypertension Disease Predictions with Various Models Using Data Science Framework
AU  - Alizargar, Azadeh
AU  - Tan, Tan-Hsu
AU  - Chang, Yang-Lang
AU  - Alkhaleefah, Mohammad
T3  - ICMHI '22
AB  - Hypertension is a common disease which may lead to incurable situation if it is not well-treated. By extracting knowledge from large datasets, data mining can be used for the hypertension prediction and diagnosis. In this research, three data mining techniques, the support vector machine (SVM), k-nearest neighbors (k-NN), and decision tree, were developed to predict the hypertension with two datasets. These techniques were implemented on Google Colab using Python codes. Experimental results illustrate that the accuracy and area under the receiver operating characteristic (ROC) curve performance of all these three techniques on first dataset is 81% and 80.64% for the SVM, 74% and 73.97% for the k-NN, and 74% and 73.86% for the decision tree, respectively. The accuracy and ROC curve performance of these three approaches on the second dataset were 73% and 72.48% for the SVM, 76% and 75.75% for the k-NN model, and 75% and 74.49% for the decision tree, respectively. The results indicate that the SVM, k-NN, and decision tree, are effective in predicting the hypertension based on the obtained important features of the patients, and the k-NN is among the best choice.
C1  - New York, NY, USA
C3  - Proceedings of the 6th International Conference on Medical and Health Informatics
DA  - 2022///
PY  - 2022
DO  - 10.1145/3545729.3545751
SP  - 107
EP  - 112
PB  - Association for Computing Machinery
SN  - 978-1-4503-9630-1
UR  - https://doi.org/10.1145/3545729.3545751
KW  - support vector machine
KW  - decision tree
KW  - Classification algorithm
KW  - hypertension
KW  - k-nearest neighbors
ER  - 

TY  - CONF
TI  - Discovering de Facto Diagnosis Specialties
AU  - Lu, Xun
AU  - Zhang, Aston
AU  - Gunter, Carl A.
AU  - Fabbri, Daniel
AU  - Liebovitz, David
AU  - Malin, Bradley
T3  - BCB '15
AB  - In health care institutions, medical specialty information may be lacking or inaccurate. Diagnosis histories offer information on which medical specialties may exist in practice, regardless of whether they have official codes. We refer to such specialties that are predicted with high certainty by diagnosis histories de facto diagnosis specialties. We aim to discover de facto diagnosis specialties under a general discovery–evaluation framework. Specifically, we employ a semi-supervised learning model and an unsupervised learning method for discovery. We further employ four supervised learning models for evaluation. We use one year of diagnosis histories from a major medical center, which consists of two data sets: one is fine-grained and the other is general. The semi-supervised learning model discovers a specialty for Breast Cancer on the fine-grained data set; while the unsupervised learning method confirms this discovery and suggests another specialty for Obesity on the larger general data set. The evaluation results reinforce that these two specialties can be recognized accurately by supervised learning models in comparison with 12 common diagnosis specialties defined by the Health Care Provider Taxonomy Code Set.
C1  - New York, NY, USA
C3  - Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics
DA  - 2015///
PY  - 2015
DO  - 10.1145/2808719.2808720
SP  - 7
EP  - 16
PB  - Association for Computing Machinery
SN  - 978-1-4503-3853-0
UR  - https://doi.org/10.1145/2808719.2808720
KW  - electronic health record
KW  - data mining
KW  - medical informatics
ER  - 

TY  - JOUR
TI  - Explanation-Driven HCI Model to Examine the Mini-Mental State for Alzheimer’s Disease
AU  - Loveleen, Gaur
AU  - Mohan, Bhandari
AU  - Shikhar, Bhadwal Singh
AU  - Nz, Jhanjhi
AU  - Shorfuzzaman, Mohammad
AU  - Masud, Mehedi
T2  - ACM Trans. Multimedia Comput. Commun. Appl.
AB  - Directing research on Alzheimer’s towards only early prediction and accuracy cannot be considered a feasible approach towards tackling a ubiquitous degenerative disease today. Applying deep learning (DL), Explainable artificial intelligence(XAI) and advancing towards the human-computer interface(HCI) model can be a leap forward in medical research. This research aims to propose a robust explainable HCI model using shapley additive explanation (SHAP), local interpretable model-agnostic explanations (LIME) and DL algorithms. The use of DL algorithms: logistic regression(80.87%), support vector machine (85.8%), k-nearest neighbour(87.24%), multilayer perceptron(91.94%), decision tree(100%) and explainability can help exploring untapped avenues for research in medical sciences that can mould the future of HCI models. The outcomes of the proposed model depict higher prediction accuracy bringing efficient computer interface in decision making, and suggests a high level of relevance in the field of medical and clinical research.
DA  - 2022/04//
PY  - 2022
DO  - 10.1145/3527174
SN  - 1551-6857
UR  - https://doi.org/10.1145/3527174
KW  - Machine Learning
KW  - Deep Learning
KW  - SHAP
KW  - Alzheimer’s Prediction
KW  - Explainable AI
KW  - Human Computer Interface
KW  - LIME
ER  - 

TY  - CONF
TI  - Medical Data Classification Using Binary Brain Storm Optimization Algorithm
AU  - Ogwo, Ogwo
AU  - Turabieh, Hamza
AU  - Sheta, Alaa
AU  - King, Scott
T3  - AIRC '19
AB  - With the growing access to technology in the medical domain, an increased volume of medical data is recorded. The size and complexity of these data make the process of analysis of meaningful discoveries of beneficial patterns more challenging. This problem has attracted numerous researchers around the world. Statistical methods have been employed to handle medical data for diagnosis purposes. Unfortunately, these methods were less capable of dealing with these massive and complex datasets. To solve this problem, we suggest a process to classify medical data which includes feature selection and classification using a number of supervised learning techniques. Binary Brain Storm Optimization (BBSO) is used for feature selection, which is a population search approach that simulates the process of electing the best idea (solution), among others. We simulated six different classifiers: Naive-Bayes, K-Nearest Neighbor, Support Vector Machine, Linear Discriminant Analysis, Decision Tree and Random Forest. Five datasets adopted from the UCI Machine Learning Repository, (Breast Cancer, Diabetes, Heart Disease, Chronic Kidney, and SPECT), are employed as a benchmark test data. The performance of BBSO is evaluated using accuracy on the datasets using the various classifiers. Experimental results show that the proposed approach improves the classification performance for better medical diagnosis.
C1  - New York, NY, USA
C3  - Proceedings of the 2019 International Conference on Artificial Intelligence, Robotics and Control
DA  - 2020///
PY  - 2020
DO  - 10.1145/3388218.3388224
SP  - 44
EP  - 52
PB  - Association for Computing Machinery
SN  - 978-1-4503-7671-6
UR  - https://doi.org/10.1145/3388218.3388224
KW  - classification
KW  - medical diagnosis
KW  - feature selection
ER  - 

TY  - CONF
TI  - Dipole: Diagnosis Prediction in Healthcare via Attention-Based Bidirectional Recurrent Neural Networks
AU  - Ma, Fenglong
AU  - Chitta, Radha
AU  - Zhou, Jing
AU  - You, Quanzeng
AU  - Sun, Tong
AU  - Gao, Jing
T3  - KDD '17
AB  - Predicting the future health information of patients from the historical Electronic Health Records (EHR) is a core research task in the development of personalized healthcare. Patient EHR data consist of sequences of visits over time, where each visit contains multiple medical codes, including diagnosis, medication, and procedure codes. The most important challenges for this task are to model the temporality and high dimensionality of sequential EHR data and to interpret the prediction results. Existing work solves this problem by employing recurrent neural networks (RNNs) to model EHR data and utilizing simple attention mechanism to interpret the results. However, RNN-based approaches suffer from the problem that the performance of RNNs drops when the length of sequences is large, and the relationships between subsequent visits are ignored by current RNN-based approaches. To address these issues, we propose Dipole, an end-to-end, simple and robust model for predicting patients' future health information. Dipole employs bidirectional recurrent neural networks to remember all the information of both the past visits and the future visits, and it introduces three attention mechanisms to measure the relationships of different visits for the prediction. With the attention mechanisms, Dipole can interpret the prediction results effectively. Dipole also allows us to interpret the learned medical code representations which are confirmed positively by medical experts. Experimental results on two real world EHR datasets show that the proposed Dipole can significantly improve the prediction accuracy compared with the state-of-the-art diagnosis prediction approaches and provide clinically meaningful interpretation.
C1  - New York, NY, USA
C3  - Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
DA  - 2017///
PY  - 2017
DO  - 10.1145/3097983.3098088
SP  - 1903
EP  - 1911
PB  - Association for Computing Machinery
SN  - 978-1-4503-4887-4
UR  - https://doi.org/10.1145/3097983.3098088
KW  - attention mechanism
KW  - healthcare informatics
KW  - bidirectional recurrent neural networks
ER  - 

TY  - CONF
TI  - A Generalized Linear Model for Cardiovascular Complications Prediction in PD Patients
AU  - Fernandez-Lozano, Carlos
AU  - Valente, Rafael Alonso
AU  - Díaz, Manuel Fidalgo
AU  - Pazos, Alejandro
T3  - DATA '18
AB  - This study was conducted using machine learning models to identify patient non-invasive information for cardiovascular complications prediction in peritoneal dialysis patients. Nowadays is well known that cardiovascular diseases are the key to mortality in patients undergoing peritoneal dialysis as the risk of cardiovascular disease increases with the progression of renal failure. Primary aim is to establish variables most associated with cardiovascular complications. To achieve this goal four different machine learning techniques were used. We found that the best classification algorithm was a Generalized Linear Model, which achieved AUC values above 96% using a small subset of the original variables following a feature selection approach. Our approach allows us to increase the interpretability of the combinations of traditional factors, advanced chronic kidney disease factors and peritoneal dialysis factors all related with cardiovascular risk profile. The final model is based primarily in the traditional factors.
C1  - New York, NY, USA
C3  - Proceedings of the First International Conference on Data Science, E-Learning and Information Systems
DA  - 2018///
PY  - 2018
DO  - 10.1145/3279996.3280039
PB  - Association for Computing Machinery
SN  - 978-1-4503-6536-9
UR  - https://doi.org/10.1145/3279996.3280039
KW  - machine learning
KW  - feature selection
KW  - cardiovascular risk prediction
KW  - glmnet
KW  - peritoneal dialysis
ER  - 

TY  - CONF
TI  - Pool of Experts: Realtime Querying Specialized Knowledge in Massive Neural Networks
AU  - Kim, Hakbin
AU  - Choi, Dong-Wan
T3  - SIGMOD '21
AB  - In spite of the great success of deep learning technologies, training and delivery of a practically serviceable model is still a highly time-consuming process. Furthermore, a resulting model is usually too generic and heavyweight, and hence essentially goes through another expensive model compression phase to fit in a resource-limited device like embedded systems. Inspired by the fact that a machine learning task specifically requested by mobile users is often much simpler than it is supported by a massive generic model, this paper proposes a framework, called Pool of Experts (PoE), that instantly builds a lightweight and task-specific model without any training process. For a realtime model querying service, PoE first extracts a pool of primitive components, called experts, from a well-trained and sufficiently generic network by exploiting a novel conditional knowledge distillation method, and then performs our train-free knowledge consolidation to quickly combine necessary experts into a lightweight network for a target task. Thanks to this train-free property, in our thorough empirical study, PoE can build a fairly accurate yet compact model in a realtime manner, whereas it takes a few minutes per query for the other training methods to achieve a similar level of the accuracy.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 International Conference on Management of Data
DA  - 2021///
PY  - 2021
DO  - 10.1145/3448016.3457326
SP  - 2244
EP  - 2252
PB  - Association for Computing Machinery
SN  - 978-1-4503-8343-1
UR  - https://doi.org/10.1145/3448016.3457326
KW  - knowledge distillation
KW  - model compression
KW  - model specialization
KW  - model unification
ER  - 

TY  - CONF
TI  - Hurtful Words: Quantifying Biases in Clinical Contextual Word Embeddings
AU  - Zhang, Haoran
AU  - Lu, Amy X.
AU  - Abdalla, Mohamed
AU  - McDermott, Matthew
AU  - Ghassemi, Marzyeh
T3  - CHIL '20
AB  - In this work, we examine the extent to which embeddings may encode marginalized populations differently, and how this may lead to a perpetuation of biases and worsened performance on clinical tasks. We pretrain deep embedding models (BERT) on medical notes from the MIMIC-III hospital dataset, and quantify potential disparities using two approaches. First, we identify dangerous latent relationships that are captured by the contextual word embeddings using a fill-in-the-blank method with text from real clinical notes and a log probability bias score quantification. Second, we evaluate performance gaps across different definitions of fairness on over 50 downstream clinical prediction tasks that include detection of acute and chronic conditions. We find that classifiers trained from BERT representations exhibit statistically significant differences in performance, often favoring the majority group with regards to gender, language, ethnicity, and insurance status. Finally, we explore shortcomings of using adversarial debiasing to obfuscate subgroup information in contextual word embeddings, and recommend best practices for such deep embedding models in clinical settings.
C1  - New York, NY, USA
C3  - Proceedings of the ACM Conference on Health, Inference, and Learning
DA  - 2020///
PY  - 2020
DO  - 10.1145/3368555.3384448
SP  - 110
EP  - 120
PB  - Association for Computing Machinery
SN  - 978-1-4503-7046-2
UR  - https://doi.org/10.1145/3368555.3384448
KW  - algorithmic fairness
KW  - clinical notes
KW  - contextual language models
KW  - machine learning for health
KW  - natural language processing
ER  - 

TY  - CONF
TI  - DCT-CenterMask: A Real-Time Instance Segmentation Network for Kidney Ultrasound Images
AU  - Wang, Pengfei
AU  - Zhao, Ximei
T3  - ICBDT '22
AB  - Ultrasound is the preferred imaging method to detect chronic kidney disease. It has the characteristics of real-time dynamic imaging. The existing real-time instance segmentation model can meet the real-time requirements of 30fps of segmentation speed, but there is still a certain gap in segmentation accuracy compared with the instance segmentation model. In this paper, a real-time instance segmentation model based on CenterMask is proposed. Using the characteristics that discrete cosine transform (DCT) can reduce the energy loss in the process of down sampling the ground truth mask, a new mask representation is trained on the premise of maintaining the real-time segmentation speed. We also integrate deformable convolution and a new attention mechanism into the network to improve the overall receptive field range of the network and the ability to learn detailed features. For further research, we made a COCO format kidney instance segmentation dataset. Through experiments, our model has improved the mask AP accuracy by about 6% compared with the original model.
C1  - New York, NY, USA
C3  - Proceedings of the 5th International Conference on Big Data Technologies
DA  - 2022///
PY  - 2022
DO  - 10.1145/3565291.3565331
SP  - 248
EP  - 254
PB  - Association for Computing Machinery
SN  - 978-1-4503-9687-5
UR  - https://doi.org/10.1145/3565291.3565331
KW  - CenterMask
KW  - Deformable convolution network
KW  - Discrete cosine transform
KW  - Real-time instance segmentation
KW  - SimAM attention mechanism
ER  - 

TY  - JOUR
TI  - A Systematic Review on Literature-Based Discovery: General Overview, Methodology, &amp; Statistical Analysis
AU  - Thilakaratne, Menasha
AU  - Falkner, Katrina
AU  - Atapattu, Thushari
T2  - ACM Comput. Surv.
AB  - The vast nature of scientific publications brings out the importance of Literature-Based Discovery (LBD) research that is highly beneficial to accelerate knowledge acquisition and the research development process. LBD is a knowledge discovery workflow that automatically detects significant, implicit knowledge associations hidden in fragmented knowledge areas by analysing existing scientific literature. Therefore, the LBD output not only assists in formulating scientifically sensible, novel research hypotheses but also encourages the development of cross-disciplinary research. In this systematic review, we provide an in-depth analysis of the computational techniques used in the LBD process using a novel, up-to-date, and detailed classification. Moreover, we also summarise the key milestones of the discipline through a timeline of topics. To provide a general overview of the discipline, the review outlines LBD validation checks, major LBD tools, application areas, domains, and generalisability of LBD methodologies. We also outline the insights gathered through our statistical analysis that capture the trends in LBD literature. To conclude, we discuss the prevailing research deficiencies in the discipline by highlighting the challenges and opportunities of future LBD research.
DA  - 2019/12//
PY  - 2019
DO  - 10.1145/3365756
VL  - 52
IS  - 6
SN  - 0360-0300
UR  - https://doi.org/10.1145/3365756
KW  - systematic review
KW  - knowledge discovery
KW  - hypotheses generation
KW  - LBD
KW  - literature mining
KW  - Literature-based discovery
KW  - text mining
ER  - 

TY  - CONF
TI  - Write It Like You See It: Detectable Differences in Clinical Notes by Race Lead to Differential Model Recommendations
AU  - Adam, Hammaad
AU  - Yang, Ming Ying
AU  - Cato, Kenrick
AU  - Baldini, Ioana
AU  - Senteio, Charles
AU  - Celi, Leo Anthony
AU  - Zeng, Jiaming
AU  - Singh, Moninder
AU  - Ghassemi, Marzyeh
T3  - AIES '22
AB  - Clinical notes are becoming an increasingly important data source for machine learning (ML) applications in healthcare. Prior research has shown that deploying ML models can perpetuate existing biases against racial minorities, as bias can be implicitly embedded in data. In this study, we investigate the level of implicit race information available to ML models and human experts and the implications of model-detectable differences in clinical notes. Our work makes three key contributions. First, we find that models can identify patient self-reported race from clinical notes even when the notes are stripped of explicit indicators of race. Second, we determine that human experts are not able to accurately predict patient race from the same redacted clinical notes. Finally, we demonstrate the potential harm of this implicit information in a simulation study, and show that models trained on these race-redacted clinical notes can still perpetuate existing biases in clinical treatment decisions.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society
DA  - 2022///
PY  - 2022
DO  - 10.1145/3514094.3534203
SP  - 7
EP  - 21
PB  - Association for Computing Machinery
SN  - 978-1-4503-9247-1
UR  - https://doi.org/10.1145/3514094.3534203
KW  - clinical notes
KW  - natural language processing
KW  - health equity
ER  - 

TY  - CONF
TI  - Cascading Classifiers for Named Entity Recognition in Clinical Notes
AU  - Wang, Yefeng
AU  - Patrick, Jon
T3  - WBIE '09
AB  - Clinical named entities convey great deal of knowledge in clinical notes. This paper investigates named entity recognition from clinical notes using machine learning approaches. We present a cascading system that uses a Conditional Random Fields model, a Support Vector Machine and a Maximum Entropy to reclassify the identified entities in order to reduce misclassification. Voting strategy was employed to determine the class of the recognised entities between the three classifiers. The experiments were conducted on a corpus of 311 manually annotated admission summaries form an Intensive Care Unit. The recognition of 10 types of clinical named entities using 10 fold cross-validation achieved an overall results of 83.3 F-score. The reclassifier effectively increased the performance over stand-alone CRF models by 3.35 F-score.
C1  - USA
C3  - Proceedings of the Workshop on Biomedical Information Extraction
DA  - 2009///
PY  - 2009
SP  - 42
EP  - 49
PB  - Association for Computational Linguistics
SN  - 978-954-452-013-7
KW  - machine learning
KW  - classifier ensemble
KW  - clinical information extraction
KW  - named entity recognition
KW  - two phase model
ER  - 

TY  - CONF
TI  - Predicting Risk of Getting Smoking-Related Cancer: A Comparison of Three Prediction Models
AU  - Hassan, Fadratul Hafinaz
AU  - Wye, Au Yong Kah
AU  - Yusof, Sharifah Syafiqah Syed
AU  - Xiang, Teh Yi
T3  - LOPAL '18
AB  - Smoking1 brings the biggest cause of cancer and deaths every year. It is essential for the smoker to aware of the bad effect of smoking and quit smoking. Therefore, cancer prediction tools are used to help in early diagnosis of cancer for the smoker for them to change their lifestyle to lower the risk of getting cancer in the future. Recently, there are many research study on early detection and diagnosis of cancer by using machine learning techniques. The cancer prediction algorithms that discussed here are decision tree algorithm, linear regression algorithm and support vector machine algorithm. These algorithms are widely used in the development of cancer prediction model. The strengths, limitations, and accuracy of each cancer prediction model are compared and analysed. In conclusion, linear regression algorithm shown the best result for the cancer prediction based on the analysis done on each cancer prediction algorithm.
C1  - New York, NY, USA
C3  - Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications
DA  - 2018///
PY  - 2018
DO  - 10.1145/3230905.3230950
PB  - Association for Computing Machinery
SN  - 978-1-4503-5304-5
UR  - https://doi.org/10.1145/3230905.3230950
KW  - prediction model
KW  - support vector machine
KW  - Decision tree
KW  - linear regression
ER  - 

TY  - CONF
TI  - A Large-Scale and Extensible Platform for Precision Medicine Research
AU  - Belghait, Fodil
AU  - April, Alain
AU  - Hamet, Pavel
AU  - Tremblay, Johanne
AU  - Desrosiers, Christian
T3  - DPH2019
AB  - The massive adoption of high-throughput genomics, deep sequencing technologies and big data technologies have made possible the era of precision medicine. However, the volume of data and its complexity remain important challenges for precision medicine research, hindering development in this field. The literature on precision medicine research describes a few platforms to support specific types of studies, but none of these offer researchers the level of customization required to meet their specific needs [1]. Methods: We propose to design and develop a platform able to import and integrate a very large volume of genetics, clinical, demographical and environmental data in a cloud computing infrastructure. In our previous publication, we presented an approach that can customize existing data models to fit any precision medicine research data requirement [1] and the requirement for future large-scale precision medicine platforms, in terms of data extensibility and the scalability of processing on demand. We also proposed a framework to meet the specific requirement of any precision medicine research [2]. In this paper, we describe how this new framework was implemented and trialed by the precision medicine researchers at the Centre Hospitalier Universitaire de l'Université de Montréal (CHUM). Results: The data analysis simulations showed that the random forest algorithm presents better accuracy results. We obtained an F1-Score of 72% for random forest, 69% using linear regression and 62% using the neural network algorithm. Conclusion: The results suggest that the proposed precision medicine data analysis platform allows researchers to configure, prepare the analysis environment and customize the platform data model to their specific research in very optimal delays, at very low cost and with minimal technical skills.
C1  - New York, NY, USA
C3  - Proceedings of the 9th International Conference on Digital Public Health
DA  - 2019///
PY  - 2019
DO  - 10.1145/3357729.3357742
SP  - 47
EP  - 54
PB  - Association for Computing Machinery
SN  - 978-1-4503-7208-4
UR  - https://doi.org/10.1145/3357729.3357742
KW  - big data
KW  - bioinformatics
KW  - clinical databases
KW  - genomics
KW  - precision medicine
ER  - 

TY  - CONF
TI  - A Hybrid Descriptor to Improve Kidney Pathologies Classification
AU  - Silva, Laiara Cristina da
AU  - Machado, Vinícius Ponte
AU  - de Melo Sousa Veras, Rodrigo
AU  - de Sá Urtiga Aita, Keylla Maria
AU  - Monte, Semiramis Jamil Hadad do
AU  - Aldeman, Nayze Lucena Sangreman
T3  - SAC '22
AB  - The importance of glomerular function in kidney physiology characterizes glomerular diseases as the main problem in nephrology. So finding and classifying glomerular disorders are fundamental steps for diagnosing many kidney diseases. This paper conducted an extensive study to determine the best set of features for glomerular image representation. Our feature extraction methodology, which includes clinical data, texture, and global descriptors, resulted in 8486 features. Besides, we compared four classifiers to propose a method that helps the specialist define a renal pathology diagnosis. The proposed method achieved an accuracy of 98.46% and a Kappa index of 98.42% using the Random Forest Classifier. We concluded that a combination of clinical data and global image features facilitates accurate disease classification.
C1  - New York, NY, USA
C3  - Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing
DA  - 2022///
PY  - 2022
DO  - 10.1145/3477314.3507321
SP  - 653
EP  - 659
PB  - Association for Computing Machinery
SN  - 978-1-4503-8713-2
UR  - https://doi.org/10.1145/3477314.3507321
KW  - random forest
KW  - machine learning
KW  - decision tree
KW  - svm
KW  - clinical data
KW  - image descriptors
KW  - mlp
KW  - renal pathologies
ER  - 

TY  - CONF
TI  - Clinical Application of Intelligent Prediction Model for Atrial Fibrillation in Hypertensive Patients
AU  - Zhang, Min
AU  - Yang, Huiying
AU  - Zhang, Shulong
AU  - Feng, Xueying
AU  - Wang, Zumin
AU  - Qin, Jing
T3  - ISAIMS '20
AB  - Hypertension is one of the most significant risk factors for atrial fibrillation (AF). However, few effective methods are available to support accurate prediction on the potential risk of atrial fibrillation among hypertensive patients currently. The aim of this paper is to illustrate a machine learning technology for constructing an atrial fibrillation intelligent prediction model. Eventually, the model can be employed to predict the risk of atrial fibrillation in hypertensive patients.A total of 2,067 diagnosed hypertensive patients (including 721 hypertensive patients complicated with atrial fibrillation) by Heart Center of Affiliated Zhongshan Hospital of Dalian University from January 2015 to January 2018 were enrolled in this study. As result, the atrial fibrillation prediction model was constructed based on the C5.0 decision tree classification algorithm. Moreover, compared with other machine learning classification algorithms, C5.0 has similar performance to random forest (RF), but is better than support vector machine(SVM), Logical Regression(LR), CHAID, and K nearest neighbor(KNN) classification algorithms. The proposed predict model has high accuracy of atrial fibrillation risk prediction for hypertension patients.
C1  - New York, NY, USA
C3  - Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences
DA  - 2020///
PY  - 2020
DO  - 10.1145/3429889.3429933
SP  - 231
EP  - 236
PB  - Association for Computing Machinery
SN  - 978-1-4503-8860-3
UR  - https://doi.org/10.1145/3429889.3429933
KW  - Hypertension
KW  - Atrial fibrillation
KW  - C5.0 decision tree
KW  - Prediction model
ER  - 

TY  - CONF
TI  - Joint Learning for Biomedical NER and Entity Normalization: Encoding Schemes, Counterfactual Examples, and Zero-Shot Evaluation
AU  - Noh, Jiho
AU  - Kavuluru, Ramakanth
T3  - BCB '21
AB  - Named entity recognition (NER) and normalization (EN) form an indispensable first step to many biomedical natural language processing applications. In biomedical information science, recognizing entities (e.g., genes, diseases, or drugs) and normalizing them to concepts in standard terminologies or thesauri (e.g., Entrez, ICD-10, or RxNorm) is crucial for identifying more informative relations among them that drive disease etiology, progression, and treatment. In this effort we pursue two high level strategies to improve biomedical ER and EN. The first is to decouple standard entity encoding tags (e.g., "B-Drug" for the beginning of a drug) into type tags (e.g., "Drug") and positional tags (e.g., "B"). A second strategy is to use additional counterfactual training examples to handle the issue of models learning spurious correlations between surrounding context and normalized concepts in training data. We conduct elaborate experiments using the MedMentions dataset, the largest dataset of its kind for ER and EN in biomedicine. We find that our first strategy performs better in entity normalization when compared with the standard coding scheme. The second data augmentation strategy uniformly improves performance in span detection, typing, and normalization. The gains from counterfactual examples are more prominent when evaluating in zero-shot settings, for concepts that have never been encountered during training.
C1  - New York, NY, USA
C3  - Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics
DA  - 2021///
PY  - 2021
DO  - 10.1145/3459930.3469533
PB  - Association for Computing Machinery
SN  - 978-1-4503-8450-6
UR  - https://doi.org/10.1145/3459930.3469533
KW  - deep neural networks
KW  - named entity recognition
KW  - biomedical natural language processing
KW  - entity normalization
KW  - information extraction
ER  - 

TY  - CONF
TI  - Automated ICD-9-CM Medical Coding of Diabetic Patient's Clinical Reports
AU  - Pereira, Vitor
AU  - Matos, Sérgio
AU  - Oliveira, José Luís
T3  - DATA '18
AB  - The assignment of ICD-9-CM codes to patient's clinical reports is a costly and wearing process manually done by medical personnel, estimated to cost about $25 billion per year in the United States. To develop a system that automates this process has been an ambition of researchers but is still an unsolved problem due to the inherent difficulties in processing unstructured clinical text.This problem is here formulated as a multi-label supervised learning one where the independent variable is the report's text and the dependent the several assigned ICD-9-CM labels. Different variations of two neural network based models, the Bag-of-Tricks and the Convolutional Neural Network (CNN) are investigated. The models are trained on the diabetic patient subset of the freely available MIMIC-III dataset.The results show that a CNN with three parallel convolutional layers achieves F1 scores of 44.51% for five digit codes and 51.73% for three digit, rolled up, codes. Although fully automated coding is not achievable, these results suggest that automated classification could be used to aid clinical staff by selecting the most probable codes.
C1  - New York, NY, USA
C3  - Proceedings of the First International Conference on Data Science, E-Learning and Information Systems
DA  - 2018///
PY  - 2018
DO  - 10.1145/3279996.3280019
PB  - Association for Computing Machinery
SN  - 978-1-4503-6536-9
UR  - https://doi.org/10.1145/3279996.3280019
KW  - electronic health records
KW  - classification
KW  - supervised learning
ER  - 

TY  - CONF
TI  - Stability Selection Using a Genetic Algorithm and Logistic Linear Regression on Healthcare Records
AU  - Zamuda, Aleš
AU  - Zarges, Christine
AU  - Stiglic, Gregor
AU  - Hrovat, Goran
T3  - GECCO '17
AB  - This paper presents a Genetic Algorithm (GA) application to measuring feature importance in machine learning (ML) from a large-scale database. Too many input features may cause over-fitting, therefore a feature selection is desirable. Some ML algorithms have feature selection embedded, e.g., lasso penalized linear regression or random forests. Others do not include such functionality and are sensitive to over-fitting, e.g., unregularized linear regression. The latter algorithms require that proper features are chosen before learning.Therefore, we propose a novel stability selection (SS) approach using GA-based feature selection. The proposed SS approach iteratively applies GA on a subsample of records and features. Each GA individual represents a binary vector of selected features in the subsample. An unregularized logistic linear regression model is then trained and tested using GA-selected features through cross-validation of the subsamples. GA fitness is evaluated by area under the curve (AUC) and optimized during a GA run.AUC is assessed with an unregularized logistic regression model on multiple-subsampled healthcare records, collected under the Healthcare Cost, and Utilization Project (HCUP), utilizing the National (Nationwide) Inpatient Sample (NIS) database.Reported results show that averaging feature importance from top-4 SS and the SS using GA (GASS), improves these AUC results.
C1  - New York, NY, USA
C3  - Proceedings of the Genetic and Evolutionary Computation Conference Companion
DA  - 2017///
PY  - 2017
DO  - 10.1145/3067695.3076077
SP  - 143
EP  - 144
PB  - Association for Computing Machinery
SN  - 978-1-4503-4939-0
UR  - https://doi.org/10.1145/3067695.3076077
KW  - feature selection
KW  - cross-validation
KW  - disease risk prediction
KW  - feature importance
KW  - genetic algorithm
KW  - healthcare cost utility project
KW  - healthcare records
KW  - logistic generalized linear regression
KW  - stability selection
ER  - 

TY  - JOUR
TI  - Mining Electronic Health Records (EHRs): A Survey
AU  - Yadav, Pranjul
AU  - Steinbach, Michael
AU  - Kumar, Vipin
AU  - Simon, Gyorgy
T2  - ACM Comput. Surv.
AB  - The continuously increasing cost of the US healthcare system has received significant attention. Central to the ideas aimed at curbing this trend is the use of technology in the form of the mandate to implement electronic health records (EHRs). EHRs consist of patient information such as demographics, medications, laboratory test results, diagnosis codes, and procedures. Mining EHRs could lead to improvement in patient health management as EHRs contain detailed information related to disease prognosis for large patient populations. In this article, we provide a structured and comprehensive overview of data mining techniques for modeling EHRs. We first provide a detailed understanding of the major application areas to which EHR mining has been applied and then discuss the nature of EHR data and its accompanying challenges. Next, we describe major approaches used for EHR mining, the metrics associated with EHRs, and the various study designs. With this foundation, we then provide a systematic and methodological organization of existing data mining techniques used to model EHRs and discuss ideas for future research.
DA  - 2018/01//
PY  - 2018
DO  - 10.1145/3127881
VL  - 50
IS  - 6
SN  - 0360-0300
UR  - https://doi.org/10.1145/3127881
KW  - machine learning
KW  - data mining
KW  - healthcare informatics
KW  - EHRs
KW  - Healthcare analytics
ER  - 

TY  - CONF
TI  - An Association Rule Mining Algorithm for Clinical Decision Support
AU  - Cui, Jiaming
AU  - Zhao, Shuo
AU  - Sun, Xin
T3  - ICCAI '22
AB  - The use of Machine Learning (ML) in healthcare has enormous potential for improving disease detection, clinical decision support. Association Rule Mining (ARM), as a classical machine learning method, provides researchers with important opportunities for data analysis and potential relationship discovery. In this paper, the weighted Apriori algorithm (MW-Apriori) is proposed to discovery the association rules from the disease diagnostic data. Furthermore, in order to obtain more accurate associations on a higher level, we design an improved k-nearest neighbor (CW-KNN) algorithm applied as a pre-step, which can divide the large number of medical texts into specific classification. MW-Apriori, can mine the hidden and potentially useful knowledge and rules behind huge amount of uncertain information. Based on the high correlation of classified data, we have also obtained high-quality association rules between symptoms and diseases. Experiment results on the multiple data sets have proved the superior performance of MW-Apriori algorithm.
C1  - New York, NY, USA
C3  - Proceedings of the 8th International Conference on Computing and Artificial Intelligence
DA  - 2022///
PY  - 2022
DO  - 10.1145/3532213.3532234
SP  - 137
EP  - 143
PB  - Association for Computing Machinery
SN  - 978-1-4503-9611-0
UR  - https://doi.org/10.1145/3532213.3532234
KW  - Association Rule Mining
KW  - Clinical Decision Support
KW  - Knowledge Discovery
KW  - Text Classification
ER  - 

TY  - JOUR
TI  - Crowdsourcing Ground Truth for Medical Relation Extraction
AU  - Dumitrache, Anca
AU  - Aroyo, Lora
AU  - Welty, Chris
T2  - ACM Trans. Interact. Intell. Syst.
AB  - Cognitive computing systems require human labeled data for evaluation and often for training. The standard practice used in gathering this data minimizes disagreement between annotators, and we have found this results in data that fails to account for the ambiguity inherent in language. We have proposed the CrowdTruth method for collecting ground truth through crowdsourcing, which reconsiders the role of people in machine learning based on the observation that disagreement between annotators provides a useful signal for phenomena such as ambiguity in the text. We report on using this method to build an annotated data set for medical relation extraction for the cause and treat relations, and how this data performed in a supervised training experiment. We demonstrate that by modeling ambiguity, labeled data gathered from crowd workers can (1) reach the level of quality of domain experts for this task while reducing the cost, and (2) provide better training data at scale than distant supervision. We further propose and validate new weighted measures for precision, recall, and F-measure, which account for ambiguity in both human and machine performance on this task.
DA  - 2018/07//
PY  - 2018
DO  - 10.1145/3152889
VL  - 8
IS  - 2
SN  - 2160-6455
UR  - https://doi.org/10.1145/3152889
KW  - clinical natural language processing
KW  - crowd truth
KW  - crowdtruth
KW  - Ground truth
KW  - inter-annotator disagreement
KW  - natural language ambiguity
KW  - relation extraction
ER  - 

TY  - CONF
TI  - Meta-Analysis for a Therapeutic Target Involved in the Activation of the Genes Associated with C3 Glomerulopathy
AU  - Ettetuani, Boutaina
AU  - Chahboune, Rajaa
AU  - Moussa, Ahmed
T3  - SMC '19
AB  - The term C3 glomerulopathy is a group of related conditions that cause the kidneys to malfunction, characterized by the presence of glomerular deposits composed of C3 in the absence of significant amounts of Ig. On the basis of electron microscopy appearance, the subsets of C3 glomerulopathy include dense deposit disease (DDD) and C3 glomerulonephritis (C3GN). Affected individuals may have particularly low levels of a protein called complement component 3 (C3) in the blood. Glomerular diseases include many conditions with a variety of genetic and environmental causes, but they fall into two major categories; Glomerulonephritis and Glomerulosclerosis. Although glomerulonephritis and glomerulosclerosis have different causes, they can both lead to kidney failure, also called end-stage renal disease (ESRD); it is the final stage of chronic kidney disease. It can be caused by other health problems that have done permanent damage to kidneys little by little, over time. In this study we examined Gene Ontology (GO) terms, that are used to assess the results of microarray experiments. However, we propose testing groups of GO terms rather than individual terms, to improve the interpretation of gene set enrichment reduce dependence between statistical tests and improve the interpretation of results. The data analyzed here focus on the differences in microarray gene expression associated with C3 glomerulopathy in two disease subtypes.
C1  - New York, NY, USA
C3  - Proceedings of the New Challenges in Data Sciences: Acts of the Second Conference of the Moroccan Classification Society
DA  - 2019///
PY  - 2019
DO  - 10.1145/3314074.3314095
PB  - Association for Computing Machinery
SN  - 978-1-4503-6129-3
UR  - https://doi.org/10.1145/3314074.3314095
KW  - C3 Glomerulopathy
KW  - Disease
KW  - Gene Expression
KW  - Gene Ontology
KW  - kidney Function
KW  - Microarray
ER  - 

TY  - CONF
TI  - OnyxRay: A Mobile-Based Nail Diseases Detection Using Custom Vision Machine Learning
AU  - Pinoliad, Sholomon L.
AU  - Dichoso, Duanne Austin N.
AU  - Caballero, Arlene R.
AU  - Albina, Erlito M.
T3  - ICIEI '20
AB  - Nails serves as a great way to discover several underlying systemic diseases of a person. There are different nail abnormalities that are associated with systemic diseases. The objective of the study is to utilize mobile camera that will allow users to capture of nail. Through this, the application will suggest several systemic diseases found based on the uploaded nail image by the user. The methods applied in this study includes the use of Custom Vision API that process images to analyze certain nail abnormalities. This study concludes that systemic diseases can be identify through nail abnormalities, so that the proponents developed an application that will analyze a fingernail image and will provide systemic diseases suggestions.
C1  - New York, NY, USA
C3  - Proceedings of the 5th International Conference on Information and Education Innovations
DA  - 2020///
PY  - 2020
DO  - 10.1145/3411681.3411698
SP  - 126
EP  - 133
PB  - Association for Computing Machinery
SN  - 978-1-4503-7575-7
UR  - https://doi.org/10.1145/3411681.3411698
KW  - Mobile Application
KW  - Custom Vision
KW  - Nail Abnormalities
KW  - Systemic Diseases
ER  - 

TY  - JOUR
TI  - Attention-Based Unsupervised Keyphrase Extraction and Phrase Graph for COVID-19 Medical Literature Retrieval
AU  - Ding, Haoran
AU  - Luo, Xiao
T2  - ACM Trans. Comput. Healthcare
AB  - Searching, reading, and finding information from the massive medical text collections are challenging. A typical biomedical search engine is not feasible to navigate each article to find critical information or keyphrases. Moreover, few tools provide a visualization of the relevant phrases to the query. However, there is a need to extract the keyphrases from each document for indexing and efficient search. The transformer-based neural networks—BERT has been used for various natural language processing tasks. The built-in self-attention mechanism can capture the associations between words and phrases in a sentence. This research investigates whether the self-attentions can be utilized to extract keyphrases from a document in an unsupervised manner and identify relevancy between phrases to construct a query relevancy phrase graph to visualize the search corpus phrases on their relevancy and importance. The comparison with six baseline methods shows that the self-attention-based unsupervised keyphrase extraction works well on a medical literature dataset. This unsupervised keyphrase extraction model can also be applied to other text data. The query relevancy graph model is applied to the COVID-19 literature dataset and to demonstrate that the attention-based phrase graph can successfully identify the medical phrases relevant to the query terms.
DA  - 2021/10//
PY  - 2021
DO  - 10.1145/3473939
VL  - 3
IS  - 1
SN  - 2691-1957
UR  - https://doi.org/10.1145/3473939
KW  - COVID-19
KW  - deep learning
KW  - Keyphrase extraction
KW  - medical information retrieval
ER  - 

TY  - CONF
TI  - KGDAL: Knowledge Graph Guided Double Attention LSTM for Rolling Mortality Prediction for AKI-D Patients
AU  - Liu, Lucas Jing
AU  - Ortiz-Soriano, Victor
AU  - Neyra, Javier A.
AU  - Chen, Jin
T3  - BCB '21
AB  - With the rapid accumulation of electronic health record (EHR) data, deep learning (DL) models have exhibited promising performance on patient risk prediction. Recent advances have also demonstrated the effectiveness of knowledge graphs (KG) in providing valuable prior knowledge for further improving DL model performance. However, it is still unclear how KG can be utilized to encode highorder relations among clinical concepts and how DL models can make full use of the encoded concept relations to solve real-world healthcare problems and to interpret the outcomes. We propose a novel knowledge graph guided double attention LSTM model named KGDAL for rolling mortality prediction for critically ill patients with acute kidney injury requiring dialysis (AKI-D). KGDAL constructs a KG-based two-dimension attention in both time and feature spaces. In the experiment with two large healthcare datasets, we compared KGDAL with a variety of rolling mortality prediction models and conducted an ablation study to test the effectiveness, efficacy, and contribution of different attention mechanisms. The results showed that KGDAL clearly outperformed all the compared models. Also, KGDAL-derived patient risk trajectories may assist healthcare providers to make timely decisions and actions. The source code, sample data, and manual of KGDAL are available at https://github.com/lucasliu0928/KGDAL.
C1  - New York, NY, USA
C3  - Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics
DA  - 2021///
PY  - 2021
DO  - 10.1145/3459930.3469513
PB  - Association for Computing Machinery
SN  - 978-1-4503-8450-6
UR  - https://doi.org/10.1145/3459930.3469513
KW  - attention mechanism
KW  - deep learning
KW  - knowledge graph
KW  - rolling mortality prediction
ER  - 

TY  - CONF
TI  - Risk Prediction on Electronic Health Records with Prior Medical Knowledge
AU  - Ma, Fenglong
AU  - Gao, Jing
AU  - Suo, Qiuling
AU  - You, Quanzeng
AU  - Zhou, Jing
AU  - Zhang, Aidong
T3  - KDD '18
AB  - Predicting the risk of potential diseases from Electronic Health Records (EHR) has attracted considerable attention in recent years, especially with the development of deep learning techniques. Compared with traditional machine learning models, deep learning based approaches achieve superior performance on risk prediction task. However, none of existing work explicitly takes prior medical knowledge (such as the relationships between diseases and corresponding risk factors) into account. In medical domain, knowledge is usually represented by discrete and arbitrary rules. Thus, how to integrate such medical rules into existing risk prediction models to improve the performance is a challenge. To tackle this challenge, we propose a novel and general framework called PRIME for risk prediction task, which can successfully incorporate discrete prior medical knowledge into all of the state-of-the-art predictive models using posterior regularization technique. Different from traditional posterior regularization, we do not need to manually set a bound for each piece of prior medical knowledge when modeling desired distribution of the target disease on patients. Moreover, the proposed PRIME can automatically learn the importance of different prior knowledge with a log-linear model.Experimental results on three real medical datasets demonstrate the effectiveness of the proposed framework for the task of risk prediction
C1  - New York, NY, USA
C3  - Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining
DA  - 2018///
PY  - 2018
DO  - 10.1145/3219819.3220020
SP  - 1910
EP  - 1919
PB  - Association for Computing Machinery
SN  - 978-1-4503-5552-0
UR  - https://doi.org/10.1145/3219819.3220020
KW  - healthcare informatics
KW  - posterior regularization
KW  - prior medical knowledge
ER  - 

TY  - JOUR
TI  - Electronic Health Record Analysis via Deep Poisson Factor Models
AU  - Henao, Ricardo
AU  - Lu, James T.
AU  - Lucas, Joseph E.
AU  - Ferranti, Jeffrey
AU  - Carin, Lawrence
T2  - J. Mach. Learn. Res.
AB  - Electronic Health Record (EHR) phenotyping utilizes patient data captured through normal medical practice, to identify features that may represent computational medical phenotypes. These features may be used to identify at-risk patients and improve prediction of patient morbidity and mortality. We present a novel deep multi-modality architecture for EHR analysis (applicable to joint analysis of multiple forms of EHR data), based on Poisson Factor Analysis (PFA) modules. Each modality, composed of observed counts, is represented as a Poisson distribution, parameterized in terms of hidden binary units. Information from different modalities is shared via a deep hierarchy of common hidden units. Activation of these binary units occurs with probability characterized as Bernoulli-Poisson link functions, instead of more traditional logistic link functions. In addition, we demonstrate that PFA modules can be adapted to discriminative modalities. To compute model parameters, we derive efficient Markov Chain Monte Carlo (MCMC) inference that scales efficiently, with significant computational gains when compared to related models based on logistic link functions. To explore the utility of these models, we apply them to a subset of patients from the Duke-Durham patient cohort. We identified a cohort of over 16,000 patients with Type 2 Diabetes Mellitus (T2DM) based on diagnosis codes and laboratory tests out of our patient population of over 240,000. Examining the common hidden units uniting the PFA modules, we identify patient features that represent medical concepts. Experiments indicate that our learned features are better able to predict mortality and morbidity than clinical features identified previously in a large-scale clinical trial.
DA  - 2016/01//
PY  - 2016
VL  - 17
IS  - 1
SP  - 6422
EP  - 6453
SN  - 1532-4435
KW  - electronic health records
KW  - deep learning
KW  - multi-modality learning
KW  - phenotyping
KW  - poisson factor model
ER  - 

TY  - CONF
TI  - Fairness for Unobserved Characteristics: Insights from Technological Impacts on Queer Communities
AU  - Tomasev, Nenad
AU  - McKee, Kevin R.
AU  - Kay, Jackie
AU  - Mohamed, Shakir
T3  - AIES '21
AB  - Advances in algorithmic fairness have largely omitted sexual orientation and gender identity. We explore queer concerns in privacy, censorship, language, online safety, health, and employment to study the positive and negative effects of artificial intelligence on queer communities. These issues underscore the need for new directions in fairness research that take into account a multiplicity of considerations, from privacy preservation, context sensitivity and process fairness, to an awareness of sociotechnical impact and the increasingly important role of inclusive and participatory research processes. Most current approaches for algorithmic fairness assume that the target characteristics for fairness—frequently, race and legal gender—can be observed or recorded. Sexual orientation and gender identity are prototypical instances of unobserved characteristics, which are frequently missing, unknown or fundamentally unmeasurable. This paper highlights the importance of developing new approaches for algorithmic fairness that break away from the prevailing assumption of observed characteristics.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society
DA  - 2021///
PY  - 2021
DO  - 10.1145/3461702.3462540
SP  - 254
EP  - 265
PB  - Association for Computing Machinery
SN  - 978-1-4503-8473-5
UR  - https://doi.org/10.1145/3461702.3462540
KW  - machine learning
KW  - algorithmic fairness
KW  - gender identity
KW  - marginalised groups
KW  - queer communities
KW  - sexual orientation
ER  - 

TY  - CONF
TI  - Comparison Method for Handling Missing Data in Clinical Studies
AU  - Nugroho, Heru
AU  - Utama, Nugraha Priya
AU  - Surendro, Kridanto
T3  - ICSCA 2020
AB  - Missing data is an issue that cannot be avoided. Most data mining algorithms cannot work with data that consist of missing values. Complete case analysis, single imputation, multiple imputations, and kNN imputation are some methods that can be used to handle the missing data. Each method has is own advantages and disadvantages. This paper compares of these methods using datasets in clinical studies, chronic kidney disease, Indian Pima diabetes, thyroid, and hepatitis. The accuracy of each method was compared using several classifiers. The experimental results show that kNN imputation method provides better accuracy than other methods.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 9th International Conference on Software and Computer Applications
DA  - 2020///
PY  - 2020
DO  - 10.1145/3384544.3384594
SP  - 46
EP  - 50
PB  - Association for Computing Machinery
SN  - 978-1-4503-7665-5
UR  - https://doi.org/10.1145/3384544.3384594
KW  - Accuracy
KW  - KNN Imputation
KW  - Clinical Studies
KW  - Comparison
KW  - Missing Data
ER  - 

TY  - CONF
TI  - Phenotypical Ontology Driven Framework for Multi-Task Learning
AU  - Ghalwash, Mohamed
AU  - Yao, Zijun
AU  - Chakraporty, Prithwish
AU  - Codella, James
AU  - Sow, Daby
T3  - CHIL '21
AB  - Despite the large number of patients in Electronic Health Records (EHRs), the subset of usable data for modeling outcomes of specific phenotypes are often imbalanced and of modest size. This can be attributed to the uneven coverage of medical concepts in EHRs. We propose OMTL, an Ontology-driven Multi-Task Learning framework, that is designed to overcome such data limitations.The key contribution of our work is the effective use of knowledge from a predefined well-established medical relationship graph (ontology) to construct a novel deep learning network architecture that mirrors this ontology. This enables common representations to be shared across related phenotypes, and was found to improve the learning performance. The proposed OMTL naturally allows for multi-task learning of different phenotypes on distinct predictive tasks. These phenotypes are tied together by their semantic relationship according to the external medical ontology. Using the publicly available MIMIC-III database, we evaluate OMTL and demonstrate its efficacy on several real patient outcome predictions over state-of-the-art multi-task learning schemes. The results of evaluating the proposed approach on six experiments show improvement in the area under ROC curve by 9% and by 8% in the area under precision-recall curve.
C1  - New York, NY, USA
C3  - Proceedings of the Conference on Health, Inference, and Learning
DA  - 2021///
PY  - 2021
DO  - 10.1145/3450439.3451881
SP  - 183
EP  - 192
PB  - Association for Computing Machinery
SN  - 978-1-4503-8359-2
UR  - https://doi.org/10.1145/3450439.3451881
KW  - embedding
KW  - knowledge-driven
KW  - multitask
ER  - 

TY  - CONF
TI  - DP-GAT: A Framework for Image-Based Disease Progression Prediction
AU  - Foo, Alex
AU  - Hsu, Wynne
AU  - Lee, Mong Li
AU  - Tan, Gavin S. W.
T3  - KDD '22
AB  - Predicting disease progression is key to provide stratified patient care and enable good utilization of healthcare resources. The availability of longitudinal images has enabled image-based disease progression prediction. In this work, we propose a framework called DP-GAT to identify regions containing significant biological structures and model the relationships among these regions as a graph along with their respective contexts. We perform reasoning via Graph Attention Network to generate representations that enable accurate disease progression prediction. We further extend DP-GAT to perform 3D medical volume segmentation. Experiments on real world medical image datasets demonstrate the advantage of our approach over strong baseline methods for both disease progression prediction and 3D segmentation tasks.
C1  - New York, NY, USA
C3  - Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
DA  - 2022///
PY  - 2022
DO  - 10.1145/3534678.3539113
SP  - 2903
EP  - 2912
PB  - Association for Computing Machinery
SN  - 978-1-4503-9385-0
UR  - https://doi.org/10.1145/3534678.3539113
KW  - deep learning
KW  - disease progression
KW  - medical imaging
ER  - 

TY  - CONF
TI  - Comorbidity Constructs for Patients with Congestive Heart Failure and Their Effect on Hospital Outcomes of Care
AU  - Zikos, Dimitrios
AU  - Delellis, Nailya
AU  - Zimeras, Stelios
AU  - Ragina, Neli
AU  - Afolayan-Oloye, Olabisi I.
T3  - PETRA '19
AB  - We present a study of the cumulative effect of comorbidities on hospital length of stay (LOS), and hospital mortality, for patients with Congestive Heart Failure (CHF). This condition can be life-threatening, while a burdened disease profile significantly increases the risk for negative outcomes. Our hypothesis is that coexisting conditions often co-interact, in various ways, and these interactions can have a variable effect on outcomes; clinical decision makers should, therefore, be able to recognize these joint effects. In order to study the CHF comorbidities, we used medical claims data from CMS. Firstly, we conducted cluster analysis to find the common hospital comorbidities for CHF admissions. We then extracted the most frequent cluster: metabolism disorders, anemia, hypertension with complications, coronary atherosclerosis, chronic kidney disease and calculated conditional probabilities in a modular manner for all combinations within this cluster. We furthermore estimated the cumulative effect of these comorbidity combinations on the two outcomes under study in a step by step, modular manner. Results were visualized with directed acyclic graphs.
C1  - New York, NY, USA
C3  - Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments
DA  - 2019///
PY  - 2019
DO  - 10.1145/3316782.3322745
SP  - 196
EP  - 203
PB  - Association for Computing Machinery
SN  - 978-1-4503-6232-0
UR  - https://doi.org/10.1145/3316782.3322745
KW  - clustering
KW  - health informatics
KW  - comorbidities
KW  - congestive heart failure
ER  - 

TY  - CONF
TI  - DETERRENT: Knowledge Guided Graph Attention Network for Detecting Healthcare Misinformation
AU  - Cui, Limeng
AU  - Seo, Haeseung
AU  - Tabar, Maryam
AU  - Ma, Fenglong
AU  - Wang, Suhang
AU  - Lee, Dongwon
T3  - KDD '20
AB  - To provide accurate and explainable misinformation detection, it is often useful to take an auxiliary source (e.g., social context and knowledge base) into consideration. Existing methods use social contexts such as users' engagements as complementary information to improve detection performance and derive explanations. However, due to the lack of sufficient professional knowledge, users seldom respond to healthcare information, which makes these methods less applicable. In this work, to address these shortcomings, we propose a novel knowledge guided graph attention network for detecting health misinformation better. Our proposal, named as DETERRENT, leverages on the additional information from medical knowledge graph by propagating information along with the network, incorporates a Medical Knowledge Graph and an Article-Entity Bipartite Graph, and propagates the node embeddings through Knowledge Paths. In addition, an attention mechanism is applied to calculate the importance of entities to each article, and the knowledge guided article embeddings are used for misinformation detection. DETERRENT addresses the limitation on social contexts in the healthcare domain and is capable of providing useful explanations for the results of detection. Empirical validation using two real-world datasets demonstrated the effectiveness of DETERRENT. Comparing with the best results of eight competing methods, in terms of F1 Score, DETERRENT outperforms all methods by at least 4.78% on the diabetes dataset and 12.79% on cancer dataset. We release the source code of DETERRENT at: https://github.com/cuilimeng/DETERRENT.
C1  - New York, NY, USA
C3  - Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining
DA  - 2020///
PY  - 2020
DO  - 10.1145/3394486.3403092
SP  - 492
EP  - 502
PB  - Association for Computing Machinery
SN  - 978-1-4503-7998-4
UR  - https://doi.org/10.1145/3394486.3403092
KW  - graph neural network
KW  - fake news
KW  - healthcare misinformation
KW  - medical knowledge graph
ER  - 

TY  - CONF
TI  - Causal AI with Real World Data: Do Statins Protect from Alzheimer's Disease Onset?
AU  - Prosperi, Mattia
AU  - Ghosh, Shantanu
AU  - Chen, Zhaoyi
AU  - Salemi, Marco
AU  - Lyu, Tianchen
AU  - Zhao, Jinying
AU  - Bian, Jiang
T3  - ICMHI '21
AB  - Causal artificial intelligence aims at developing bias-robust models that can be used to intervene on, rather than just be predictive, of risks or outcomes. However, learning interventional models from observational data, including electronic health records (EHR), is challenging due to inherent bias, e.g., protopathic, confounding, collider. When estimating the effects of treatment interventions, classical approaches like propensity score matching are often used, but they pose limitations with large feature sets, nonlinear/nonparallel treatment group assignments, and collider bias. In this work, we used data from a large EHR consortium –OneFlorida– and evaluated causal statistical/machine learning methods for determining the effect of statin treatment on the risk of Alzheimer's disease, a debated clinical research question. We introduced a combination of directed acyclic graph (DAG) learning and comparison with expert's design, with calculation of the generalized adjustment criterion (GAC), to find an optimal set of covariates for estimation of treatment effects –ameliorating collider bias. The DAG/CAC approach was assessed together with traditional propensity score matching, inverse probability weighting, virtual-twin/counterfactual random forests, and deep counterfactual networks. We showed large heterogeneity in effect estimates upon different model configurations. Our results did not exclude a protective effect of statins, where the DAG/GAC point estimate aligned with the maximum credibility estimate, although the 95% credibility interval included a null effect, warranting further studies and replication.
C1  - New York, NY, USA
C3  - Proceedings of the 5th International Conference on Medical and Health Informatics
DA  - 2021///
PY  - 2021
DO  - 10.1145/3472813.3473206
SP  - 296
EP  - 303
PB  - Association for Computing Machinery
SN  - 978-1-4503-8984-6
UR  - https://doi.org/10.1145/3472813.3473206
KW  - machine learning
KW  - Bayesian network
KW  - biomedical informatics
KW  - Causal artificial intelligence
KW  - directed acyclic graph
KW  - electronic medical records
KW  - generalized adjustment criterion
KW  - treatment effect
ER  - 

TY  - CONF
TI  - Safety and Performance, Why Not Both? Bi-Objective Optimized Model Compression toward AI Software Deployment
AU  - Zhu, Jie
AU  - Wang, Leye
AU  - Han, Xiao
T3  - ASE '22
AB  - The size of deep learning models in artificial intelligence (AI) software is increasing rapidly, which hinders the large-scale deployment on resource-restricted devices (e.g., smartphones). To mitigate this issue, AI software compression plays a crucial role, which aims to compress model size while keeping high performance. However, the intrinsic defects in the big model may be inherited by the compressed one. Such defects may be easily leveraged by attackers, since the compressed models are usually deployed in a large number of devices without adequate protection. In this paper, we try to address the safe model compression problem from a safety-performance co-optimization perspective. Specifically, inspired by the test-driven development (TDD) paradigm in software engineering, we propose a test-driven sparse training framework called SafeCompress. By simulating the attack mechanism as the safety test, SafeCompress can automatically compress a big model to a small one following the dynamic sparse training paradigm. Further, considering a representative attack, i.e., membership inference attack (MIA), we develop a concrete safe model compression mechanism, called MIA-SafeCompress. Extensive experiments are conducted to evaluate MIA-SafeCompress on five datasets for both computer vision and natural language processing tasks. The results verify the effectiveness and generalization of our method. We also discuss how to adapt SafeCompress to other attacks besides MIA, demonstrating the flexibility of SafeCompress.
C1  - New York, NY, USA
C3  - Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering
DA  - 2023///
PY  - 2023
DO  - 10.1145/3551349.3556906
PB  - Association for Computing Machinery
SN  - 978-1-4503-9475-8
UR  - https://doi.org/10.1145/3551349.3556906
KW  - AI software safe compression
KW  - membership inference attack
KW  - test-driven development
ER  - 

TY  - CONF
TI  - Capturing Feature-Level Irregularity in Disease Progression Modeling
AU  - Zheng, Kaiping
AU  - Wang, Wei
AU  - Gao, Jinyang
AU  - Ngiam, Kee Yuan
AU  - Ooi, Beng Chin
AU  - Yip, Wei Luen James
T3  - CIKM '17
AB  - Disease progression modeling (DPM) analyzes patients' electronic medical records (EMR) to predict the health state of patients, which facilitates accurate prognosis, early detection and treatment of chronic diseases. However, EMR are irregular because patients visit hospital irregularly based on the need of treatment. For each visit, they are typically given different diagnoses, prescribed various medications and lab tests. Consequently, EMR exhibit irregularity at the feature level. To handle this issue, we propose a model based on the Gated Recurrent Unit by decaying the effect of previous records using fine-grained feature-level time span information, and learn the decaying parameters for different features to take into account their different behaviours like decaying speeds under irregularity. Extensive experimental results in both an Alzheimer's disease dataset and a chronic kidney disease dataset demonstrate that our proposed model of capturing feature-level irregularity can effectively improve the accuracy of DPM.
C1  - New York, NY, USA
C3  - Proceedings of the 2017 ACM on Conference on Information and Knowledge Management
DA  - 2017///
PY  - 2017
DO  - 10.1145/3132847.3132944
SP  - 1579
EP  - 1588
PB  - Association for Computing Machinery
SN  - 978-1-4503-4918-5
UR  - https://doi.org/10.1145/3132847.3132944
KW  - healthcare
KW  - data analytics
KW  - gated recurrent unit
KW  - time series
ER  - 

TY  - CONF
TI  - Deep State-Space Generative Model For Correlated Time-to-Event Predictions
AU  - Xue, Yuan
AU  - Zhou, Denny
AU  - Du, Nan
AU  - Dai, Andrew M.
AU  - Xu, Zhen
AU  - Zhang, Kun
AU  - Cui, Claire
T3  - KDD '20
AB  - Capturing the inter-dependencies among multiple types of clinically-critical events is critical not only to accurate future event prediction, but also to better treatment planning. In this work, we propose a deep latent state-space generative model to capture the interactions among different types of correlated clinical events (e.g., kidney failure, mortality) by explicitly modeling the temporal dynamics of patients' latent states. Based on these learned patient states, we further develop a new general discrete-time formulation of the hazard rate function to estimate the survival distribution of patients with significantly improved accuracy. Extensive evaluations over real EMR data show that our proposed model compares favorably to various state-of-the-art baselines. Furthermore, our method also uncovers meaningful insights about the latent correlations among mortality and different types of organ failures.
C1  - New York, NY, USA
C3  - Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining
DA  - 2020///
PY  - 2020
DO  - 10.1145/3394486.3403206
SP  - 1552
EP  - 1562
PB  - Association for Computing Machinery
SN  - 978-1-4503-7998-4
UR  - https://doi.org/10.1145/3394486.3403206
KW  - survival analysis
KW  - generative model
KW  - state space model
ER  - 

TY  - CONF
TI  - MedRetriever: Target-Driven Interpretable Health Risk Prediction via Retrieving Unstructured Medical Text
AU  - Ye, Muchao
AU  - Cui, Suhan
AU  - Wang, Yaqing
AU  - Luo, Junyu
AU  - Xiao, Cao
AU  - Ma, Fenglong
T3  - CIKM '21
AB  - The broad adoption of electronic health record (EHR) systems and the advances of deep learning technology have motivated the development of health risk prediction models, which mainly depend on the expressiveness and temporal modeling capacity of deep neural networks (DNNs) to improve prediction performance. Some further augment the prediction by using external knowledge, however, a great deal of EHR information inevitably loses during the knowledge mapping. In addition, prediction made by existing models usually lacks reliable interpretation, which undermines their reliability in guiding clinical decision-making. To solve these challenges, we propose MedRetriever, an effective and flexible framework that leverages unstructured medical text collected from authoritative websites to augment health risk prediction as well as to provide understandable interpretation. Besides, MedRetriever explicitly takes the target disease documents into consideration, which provide key guidance for the model to learn in a target-driven direction, i.e., from the target disease to the input EHR. To specify, MedRetriever can flexibly choose its backbone from major predictive models to learn the EHR embedding for each visit. After that, the EHR embedding and features of target disease documents are aggregated into a query by self-attention to retrieve highly relevant text segments from the medical text pool, which is stored in the dynamically updated text memory. Finally, the comprehensive EHR embedding and the text memory are used for prediction and interpretation. We evaluate MedRetriever against nine state-of-the-art approaches across three real-world EHR datasets, which consistently achieves the best performance in AUC and recall metrics and outperforms the best baseline by at least 4.8% in recall on three test datasets. Furthermore, we conduct case studies to show the easy-to-understand interpretation by MedRetriever.
C1  - New York, NY, USA
C3  - Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management
DA  - 2021///
PY  - 2021
DO  - 10.1145/3459637.3482273
SP  - 2414
EP  - 2423
PB  - Association for Computing Machinery
SN  - 978-1-4503-8446-9
UR  - https://doi.org/10.1145/3459637.3482273
KW  - electronic health records
KW  - data mining
KW  - external knowledge
KW  - health risk prediction
ER  - 

TY  - CONF
TI  - Leveraging Hierarchy in Medical Codes for Predictive Modeling
AU  - Singh, Anima
AU  - Nadkarni, Girish
AU  - Guttag, John
AU  - Bottinger, Erwin
T3  - BCB '14
AB  - ICD-9 codes are among the most important patient information recorded in electronic health records. They have been shown to be useful for predictive modeling of different adverse outcomes in patients, including diabetes and heart failure. An important characteristic of ICD-9 codes is the hierarchical relationships among different codes. Nevertheless, the most common feature representation used to incorporate ICD-9 codes in predictive models disregards the structural relationships.In this paper, we explore different methods to leverage the hierarchical structure in ICD-9 codes with the goal of improving performance of predictive models. We compare methods that leverage hierarchy by 1) incorporating the information during feature construction, 2) using a learning algorithm that addresses the structure in the ICD-9 codes when building a model, or 3) doing both. We propose and evaluate a novel feature engineering approach to leverage hierarchy, while simultaneously reducing feature dimensionality.Our experiments indicate that significant improvement in predictive performance can be achieved by properly exploiting ICD-9 hierarchy. Using two clinical tasks: predicting chronic kidney disease progression (Task-CKD), and predicting incident heart failure (Task-HF), we show that methods that use hierarchy outperform the conventional approach in F-score (0.44 vs 0.36 for Task-HF and 0.40 vs 0.37 for Task-CKD) and relative risk (4.6 vs 3.3 for Task-HF and 5.9 vs 3.8 for Task-CKD).
C1  - New York, NY, USA
C3  - Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics
DA  - 2014///
PY  - 2014
DO  - 10.1145/2649387.2649407
SP  - 96
EP  - 103
PB  - Association for Computing Machinery
SN  - 978-1-4503-2894-4
UR  - https://doi.org/10.1145/2649387.2649407
KW  - feature hierarchy
KW  - ICD-9 codes
KW  - machine learning in healthcare and medicine
KW  - predictive modeling
ER  - 

TY  - JOUR
TI  - An Improved Ensemble Learning Method for Classifying High-Dimensional and Imbalanced Biomedicine Data
AU  - Yu, Hualong
AU  - Ni, Jun
T2  - IEEE/ACM Trans. Comput. Biol. Bioinformatics
AB  - Training classifiers on skewed data can be technically challenging tasks, especially if the data is high-dimensional simultaneously, the tasks can become more difficult. In biomedicine field, skewed data type often appears. In this study, we try to deal with this problem by combining asymmetric bagging ensemble classifier (as Bagging) that has been presented in previous work and an improved random subspace (RS) generation strategy that is called feature subspace (FSS). Specifically, FSS is a novel method to promote the balance level between accuracy and diversity of base classifiers in as Bagging. In view of the strong generalization capability of support vector machine (SVM), we adopt it to be base classifier. Extensive experiments on four benchmark biomedicine data sets indicate that the proposed ensemble learning method outperforms many baseline approaches in terms of Accuracy, F-measure, G-mean and AUC evaluation criterions, thus it can be regarded as an effective and efficient tool to deal with high-dimensional and imbalanced biomedical data.
DA  - 2014/07//
PY  - 2014
DO  - 10.1109/TCBB.2014.2306838
VL  - 11
IS  - 4
SP  - 657
EP  - 666
SN  - 1545-5963
UR  - https://doi.org/10.1109/TCBB.2014.2306838
KW  - ensemble learning
KW  - bioinformatics
KW  - class imbalance
KW  - high-dimensional biomedicine data
ER  - 

TY  - CONF
TI  - TAdaNet: Task-Adaptive Network for Graph-Enriched Meta-Learning
AU  - Suo, Qiuling
AU  - Chou, Jingyuan
AU  - Zhong, Weida
AU  - Zhang, Aidong
T3  - KDD '20
AB  - Annotated data samples in real-world applications are often limited. Meta-learning, which utilizes prior knowledge learned from related tasks and generalizes to new tasks of limited supervised experience, is an effective approach for few-shot learning. However, standard meta-learning with globally shared knowledge cannot handle the task heterogeneity problem well, i.e., tasks lie in different distributions. Recent advances have explored several ways to trigger task-dependent initial parameters or metrics, in order to customize task-specific information. These approaches learn task contextual information from data, but ignore external domain knowledge that can help in the learning process. In this paper, we propose a task-adaptive network (TAdaNet) that makes use of a domain-knowledge graph to enrich data representations and provide task-specific customization. Specifically, we learn a task embedding that characterizes task relationships and tailors task-specific parameters, resulting in a task-adaptive metric space for classification. Experimental results on a few-shot image classification problem show the effectiveness of the proposed method. We also apply it on a real-world disease classification problem, and show promising results for clinical decision support.
C1  - New York, NY, USA
C3  - Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining
DA  - 2020///
PY  - 2020
DO  - 10.1145/3394486.3403230
SP  - 1789
EP  - 1799
PB  - Association for Computing Machinery
SN  - 978-1-4503-7998-4
UR  - https://doi.org/10.1145/3394486.3403230
KW  - few-shot learning
KW  - meta learning
KW  - predictive healthcare
ER  - 

TY  - CONF
TI  - From Micro to Macro: Data Driven Phenotyping by Densification of Longitudinal Electronic Medical Records
AU  - Zhou, Jiayu
AU  - Wang, Fei
AU  - Hu, Jianying
AU  - Ye, Jieping
T3  - KDD '14
AB  - Inferring phenotypic patterns from population-scale clinical data is a core computational task in the development of personalized medicine. One important source of data on which to conduct this type of research is patient Electronic Medical Records (EMR). However, the patient EMRs are typically sparse and noisy, which creates significant challenges if we use them directly to represent patient phenotypes. In this paper, we propose a data driven phenotyping framework called Pacifier (PAtient reCord densIFIER), where we interpret the longitudinal EMR data of each patient as a sparse matrix with a feature dimension and a time dimension, and derive more robust patient phenotypes by exploring the latent structure of those matrices. Specifically, we assume that each derived phenotype is composed of a subset of the medical features contained in original patient EMR, whose value evolves smoothly over time. We propose two formulations to achieve such goal. One is Individual Basis Approach (IBA), which assumes the phenotypes are different for every patient. The other is Shared Basis Approach (SBA), which assumes the patient population shares a common set of phenotypes. We develop an efficient optimization algorithm that is capable of resolving both problems efficiently. Finally we validate Pacifier on two real world EMR cohorts for the tasks of early prediction of Congestive Heart Failure (CHF) and End Stage Renal Disease (ESRD). Our results show that the predictive performance in both tasks can be improved significantly by the proposed algorithms (average AUC score improved from 0.689 to 0.816 on CHF, and from 0.756 to 0.838 on ESRD respectively, on diagnosis group granularity). We also illustrate some interesting phenotypes derived from our data.
C1  - New York, NY, USA
C3  - Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
DA  - 2014///
PY  - 2014
DO  - 10.1145/2623330.2623711
SP  - 135
EP  - 144
PB  - Association for Computing Machinery
SN  - 978-1-4503-2956-9
UR  - https://doi.org/10.1145/2623330.2623711
KW  - medical informatics
KW  - phenotyping
KW  - densification
KW  - matrix completion
KW  - sparse learning
ER  - 

TY  - CONF
TI  - Use of Regression Models to Predict Glomerular Filtration Rate in Kidney Transplanted Patients
AU  - Loperto, Ilaria
AU  - Scala, Arianna
AU  - Rossano, Lucia
AU  - Carrano, Rosa
AU  - Federico, Stefano
AU  - Triassi, Maria
AU  - Improta, Giovanni
T3  - BECB 2021
AB  - Despite of the numerous progress of modern medicine, organ transplantation is never a free risk procedure. Kidney transplant, in particular, can bring to numerous short and/or long-term problems, like infection or diabetes. Since such problems can appear up to years, a constant monitoring of Kidney transplanted patients is necessary to try and avoid a bad prognosis. Glomerular Filtration rate (GFR) is an important marker to evaluate kidney transplanted patients. Since it is necessary to measure values of GFR over time, new predictive approaches can reveal useful to such scope. In this work we present a Multiple Linear Regression model and a Machine Learning method to correlate GFR with glycaemia (mg/dL) and the dosage of a calcineurin inhibitor. Results show how such model can be useful in a long term evaluation of kidney transplanted patients.
C1  - New York, NY, USA
C3  - 2021 International Symposium on Biomedical Engineering and Computational Biology
DA  - 2022///
PY  - 2022
DO  - 10.1145/3502060.3503627
PB  - Association for Computing Machinery
SN  - 978-1-4503-8411-7
UR  - https://doi.org/10.1145/3502060.3503627
KW  - Machine Learning
KW  - Nephrology
KW  - Glomerular Filtration rate
KW  - Multiple Linear Regression
ER  - 

TY  - CONF
TI  - Supervised Pretraining through Contrastive Categorical Positive Samplings to Improve COVID-19 Mortality Prediction
AU  - Wanyan, Tingyi
AU  - Lin, Mingquan
AU  - Klang, Eyal
AU  - Menon, Kartikeya M.
AU  - Gulamali, Faris F.
AU  - Azad, Ariful
AU  - Zhang, Yiye
AU  - Ding, Ying
AU  - Wang, Zhangyang
AU  - Wang, Fei
AU  - Glicksberg, Benjamin
AU  - Peng, Yifan
T3  - BCB '22
AB  - Clinical EHR data is naturally heterogeneous, where it contains abundant sub-phenotype. Such diversity creates challenges for outcome prediction using a machine learning model since it leads to high intra-class variance. To address this issue, we propose a supervised pre-training model with a unique embedded k-nearest-neighbor positive sampling strategy. We demonstrate the enhanced performance value of this framework theoretically and show that it yields highly competitive experimental results in predicting patient mortality in real-world COVID-19 EHR data with a total of over 7,000 patients admitted to a large, urban health system. Our method achieves a better AUROC prediction score of 0.872, which outperforms the alternative pre-training models and traditional machine learning methods. Additionally, our method performs much better when the training data size is small (345 training instances).
C1  - New York, NY, USA
C3  - Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics
DA  - 2022///
PY  - 2022
DO  - 10.1145/3535508.3545541
PB  - Association for Computing Machinery
SN  - 978-1-4503-9386-7
UR  - https://doi.org/10.1145/3535508.3545541
KW  - intra-class variance
KW  - mortality prediction
KW  - pre-training
KW  - self-supervised learning
KW  - sub-phenotype
KW  - supervised contrastive learning
ER  - 

TY  - JOUR
TI  - Predicting Post-Operative Complications with Wearables: A Case Study with Patients Undergoing Pancreatic Surgery
AU  - Zhang, Jingwen
AU  - Li, Dingwen
AU  - Dai, Ruixuan
AU  - Cos, Heidy
AU  - Williams, Gregory A.
AU  - Raper, Lacey
AU  - Hammill, Chet W.
AU  - Lu, Chenyang
T2  - Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.
AB  - Post-operative complications and hospital readmission are of great concern to surgical patients and health care providers. Wearable devices such as Fitbit wristbands enable long-term and non-intrusive monitoring of patients outside clinical environments. To build accurate predictive models based on wearable data, however, requires effective feature engineering to extract high-level features from time series data collected by the wearable sensors. This paper presents a pipeline for developing clinical predictive models based on wearable sensors. The core of the pipeline is a multi-level feature engineering framework for extracting high-level features from fine-grained time series data. The framework integrates a set of techniques tailored for noisy and incomplete wearable data collected in real-world clinical studies: (1) singular spectrum analysis for extracting high-level features from daily features over the course of the study; (2) a set of daily features that are resilient to missing data in wearable time series data; (3) a K-Nearest Neighbors (KNN) method for imputing short missing heart rate segments; (4) the integration of patients' clinical characteristics and wearable features. We evaluated the feature engineering approach and machine learning models in a clinical study involving 61 patients undergoing pancreatic surgery. Linear support vector machine (SVM) with integrated feature engineering achieved an AUROC of 0.8802 for predicting post-operative readmission or severe complications, which significantly outperformed the existing rule-based model used in clinical practice and other state-of-the-art feature engineering approaches.
DA  - 2022/07//
PY  - 2022
DO  - 10.1145/3534578
VL  - 6
IS  - 2
UR  - https://doi.org/10.1145/3534578
KW  - Machine Learning
KW  - Feature Engineering
KW  - Missing Data
KW  - Post-Surgical Prediction
KW  - Wearable Devices
ER  - 

TY  - JOUR
TI  - EpiMC: Detecting Epistatic Interactions Using Multiple Clusterings
AU  - Wang, Jun
AU  - Zhang, Huiling
AU  - Ren, Wei
AU  - Guo, Maozu
AU  - Yu, Guoxian
T2  - IEEE/ACM Trans. Comput. Biol. Bioinformatics
AB  - Detecting single nucleotide polymorphisms (SNPs) interactions is crucial to identify susceptibility genes associated with complex human diseases in genome-wide association studies. Clustering-based approaches are widely used in reducing search space and exploring potential relationships between SNPs in epistasis analysis. However, these approaches all only use a single measure to filter out nonsignificant SNP combinations, which may be significant ones from another perspective. In this paper, we propose a two-stage approach named EpiMC (Epistatic Interactions detection based on Multiple Clusterings) that employs multiple clusterings to obtain more precise candidate sets and more comprehensively detect high-order interactions based on these sets. In the first stage, EpiMC proposes a matrix factorization based multiple clusterings algorithm to generate multiple diverse clusterings, each of which divide all SNPs into different clusters. This stage aims to reduce the chance of filtering out potential candidates overlooked by a single clustering and groups associated SNPs together from different clustering perspectives. In the next stage, EpiMC considers both the single-locus effects and interaction effects to select high-quality disease associated SNPs, and then uses Jaccard similarity to get candidate sets. Finally, EpiMC uses exhaustive search on the obtained small candidate sets to precisely detect epsitatic interactions. Extensive simulation experiments show that EpiMC has a better performance in detecting high-order interactions than state-of-the-art solutions. On the Wellcome Trust Case Control Consortium (WTCCC) dataset, EpiMC detects several significant epistatic interactions associated with breast cancer (BC) and age-related macular degeneration (AMD), which again corroborate the effectiveness of EpiMC.
DA  - 2022/02//
PY  - 2022
DO  - 10.1109/TCBB.2021.3080462
VL  - 19
IS  - 1
SP  - 243
EP  - 254
SN  - 1545-5963
UR  - https://doi.org/10.1109/TCBB.2021.3080462
ER  - 

TY  - CONF
TI  - Attribute Selection Based on Genetic and Classification Algorithms in the Prediction of Hospitalization Need of COVID-19 Patients
AU  - Colpo, Miriam Pizzatto
AU  - Alves, Bruno Cascaes
AU  - Pereira, Kevin Soares
AU  - Brandão, Anna Flávia Zimmermann
AU  - de Aguiar, Marilton Sanchotene
AU  - Primo, Tiago Thompsen
T3  - SBSI 2021
AB  - The COVID-19 pandemic has been pressuring the whole society and overloading hospital systems. Machine learning models designed to predict hospitalizations, for example, can contribute to better targeting hospital resources. However, as the excess of information, often irrelevant or redundant, can impair the performance of predictive models, we propose in this work a hybrid approach to attribute selection. This method aims to find an optimal attribute subset through a genetic algorithm, which considers the results of a classification model in its evaluation function to improve the hospitalization need prediction of COVID-19 patients. We evaluated this approach in a database of more than 200 thousand COVID-19 patients from the State Health Secretariat of Rio Grande do Sul. We provided an increase of 18% in the classification precision for patients with hospitalization necessities. In a real-time application, this would also mean greater precision in targeting resources, as well as, consequently and mainly, improved service to the infected population.
C1  - New York, NY, USA
C3  - XVII Brazilian Symposium on Information Systems
DA  - 2021///
PY  - 2021
DO  - 10.1145/3466933.3466935
PB  - Association for Computing Machinery
SN  - 978-1-4503-8491-9
UR  - https://doi.org/10.1145/3466933.3466935
KW  - COVID-19
KW  - machine learning
KW  - feature selection
KW  - genetic algorithm
KW  - hospitalization prediction
ER  - 

TY  - CONF
TI  - Data Preparation: A Comparative Analysis of the Fill-in Methods for Handling Missing Data in Clinical Laboratory Datasets: Comparative Analysis of Fill-in Methods for Handling Missing Data
AU  - Rivera, Melody Angelique Camba
AU  - Carpio, Jennifer Torres
AU  - Vinluan, Albert Alcause
T3  - ICBDE '22
AB  - Missing data has been a challenge for research work, especially in the medical field. Several methods to deal with missing data have been recommended, from removing rows or columns to imputation. Determining the appropriate method will need identification of the data's missingness mechanism. This study-in-progress will compare several datasets whose missing data will be imputed using different fill-in methods. The purpose of the comparison is to determine the dataset that can produce the most distinct latent classes of non-communicable disease (NCD) comorbidity when passed through Latent Class Analysis (LCA). Data were extracted and integrated from two clinical laboratories. Dataset copies were produced after de-identifying and cleaning the original dataset with missing values. Each dataset copy will be assigned a specific method to fill in the missing values. When passed through the LCA model selection and evaluation processes, the original dataset will be compared with the other datasets with imputed values. Future work includes choosing the LCA model and developing software that could be used by primary care physicians and local government health centers as a basis in developing personalized NCD comorbidity treatment and management plans for their patients.
C1  - New York, NY, USA
C3  - Proceedings of the 5th International Conference on Big Data and Education
DA  - 2022///
PY  - 2022
DO  - 10.1145/3524383.3524439
SP  - 412
EP  - 418
PB  - Association for Computing Machinery
SN  - 978-1-4503-9579-3
UR  - https://doi.org/10.1145/3524383.3524439
ER  - 

TY  - CONF
TI  - Improving Medical Code Prediction from Clinical Text via Incorporating Online Knowledge Sources
AU  - Bai, Tian
AU  - Vucetic, Slobodan
T3  - WWW '19
AB  - Clinical notes contain detailed information about health status of patients for each of their encounters with a health system. Developing effective models to automatically assign medical codes to clinical notes has been a long-standing active research area. Despite a great recent progress in medical informatics fueled by deep learning, it is still a challenge to find the specific piece of evidence in a clinical note which justifies a particular medical code out of all possible codes. Considering the large amount of online disease knowledge sources, which contain detailed information about signs and symptoms of different diseases, their risk factors, and epidemiology, there is an opportunity to exploit such sources. In this paper we consider Wikipedia as an external knowledge source and propose Knowledge Source Integration (KSI), a novel end-to-end code assignment framework, which can integrate external knowledge during training of any baseline deep learning model. The main idea of KSI is to calculate matching scores between a clinical note and disease related Wikipedia documents, and combine the scores with output of the baseline model. To evaluate KSI, we experimented with automatic assignment of ICD-9 diagnosis codes to the emergency department clinical notes from MIMIC-III data set, aided by Wikipedia documents corresponding to the ICD-9 codes. We evaluated several baseline models, ranging from logistic regression to recently proposed deep learning models known to achieve the state-of-the-art accuracy on clinical notes. The results show that KSI consistently improves the baseline models and that it is particularly successful in assignment of rare codes. In addition, by analyzing weights of KSI models, we can gain understanding about which words in Wikipedia documents provide useful information for predictions.
C1  - New York, NY, USA
C3  - The World Wide Web Conference
DA  - 2019///
PY  - 2019
DO  - 10.1145/3308558.3313485
SP  - 72
EP  - 82
PB  - Association for Computing Machinery
SN  - 978-1-4503-6674-8
UR  - https://doi.org/10.1145/3308558.3313485
KW  - attention mechanism
KW  - healthcare
KW  - document similarity learning
KW  - Multi-label classification
ER  - 

TY  - JOUR
TI  - Use of Electronic Health Data for Disease Prediction: A Comprehensive Literature Review
AU  - Hossain, Md. Ekramul
AU  - Khan, Arif
AU  - Moni, Mohammad Ali
AU  - Uddin, Shahadat
T2  - IEEE/ACM Trans. Comput. Biol. Bioinformatics
AB  - Disease prediction has the potential to benefit stakeholders such as the government and health insurance companies. It can identify patients at risk of disease or health conditions. Clinicians can then take appropriate measures to avoid or minimize the risk and in turn, improve quality of care and avoid potential hospital admissions. Due to the recent advancement of tools and techniques for data analytics, disease risk prediction can leverage large amounts of semantic information, such as demographics, clinical diagnosis and measurements, health behaviours, laboratory results, prescriptions and care utilisation. In this regard, electronic health data can be a potential choice for developing disease prediction models. A significant number of such disease prediction models have been proposed in the literature over time utilizing large-scale electronic health databases, different methods, and healthcare variables. The goal of this comprehensive literature review was to discuss different risk prediction models that have been proposed based on electronic health data. Search terms were designed to find relevant research articles that utilized electronic health data to predict disease risks. Online scholarly databases were searched to retrieve results, which were then reviewed and compared in terms of the method used, disease type, and prediction accuracy. This paper provides a comprehensive review of the use of electronic health data for risk prediction models. A comparison of the results from different techniques for three frequently modelled diseases using electronic health data was also discussed in this study. In addition, the advantages and disadvantages of different risk prediction models, as well as their performance, were presented. Electronic health data have been widely used for disease prediction. A few modelling approaches show very high accuracy in predicting different diseases using such data. These modelling approaches have been used to inform the clinical decision process to achieve better outcomes.
DA  - 2021/04//
PY  - 2021
DO  - 10.1109/TCBB.2019.2937862
VL  - 18
IS  - 2
SP  - 745
EP  - 758
SN  - 1545-5963
UR  - https://doi.org/10.1109/TCBB.2019.2937862
ER  - 

TY  - CONF
TI  - INPREM: An Interpretable and Trustworthy Predictive Model for Healthcare
AU  - Zhang, Xianli
AU  - Qian, Buyue
AU  - Cao, Shilei
AU  - Li, Yang
AU  - Chen, Hang
AU  - Zheng, Yefeng
AU  - Davidson, Ian
T3  - KDD '20
AB  - Building a predictive model based on historical Electronic Health Records (EHRs) for personalized healthcare has become an active research area. Benefiting from the powerful ability of feature extraction, deep learning (DL) approaches have achieved promising performance in many clinical prediction tasks. However, due to the lack of interpretability and trustworthiness, it is difficult to apply DL in real clinical cases of decision making. To address this, in this paper, we propose an interpretable and trustworthy predictive model (INPREM) for healthcare. Firstly, INPREM is designed as a linear model for interpretability while encoding non-linear relationships into the learning weights for modeling the dependencies between and within each visit. This enables us to obtain the contribution matrix of the input variables, which is served as the evidence of the prediction result(s), and help physicians understand why the model gives such a prediction, thereby making the model more interpretable. Secondly, for trustworthiness, we place a random gate (which follows a Bernoulli distribution to turn on or off) over each weight of the model, as well as an additional branch to estimate data noises. With the help of the Monto Carlo sampling and an objective function accounting for data noises, the model can capture the uncertainty of each prediction. The captured uncertainty, in turn, allows physicians to know how confident the model is, thus making the model more trustworthy. We empirically demonstrate that the proposed INPREM outperforms existing approaches with a significant margin. A case study is also presented to show how the contribution matrix and the captured uncertainty are used to assist physicians in making robust decisions.
C1  - New York, NY, USA
C3  - Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining
DA  - 2020///
PY  - 2020
DO  - 10.1145/3394486.3403087
SP  - 450
EP  - 460
PB  - Association for Computing Machinery
SN  - 978-1-4503-7998-4
UR  - https://doi.org/10.1145/3394486.3403087
KW  - attention mechanism
KW  - healthcare informatics
KW  - model interpretability
KW  - model uncertainty
ER  - 

TY  - JOUR
TI  - Data Science and Prediction
AU  - Dhar, Vasant
T2  - Commun. ACM
AB  - Big data promises automated actionable knowledge creation and predictive models for use by both humans and computers.
DA  - 2013/12//
PY  - 2013
DO  - 10.1145/2500499
VL  - 56
IS  - 12
SP  - 64
EP  - 73
SN  - 0001-0782
UR  - https://doi.org/10.1145/2500499
ER  - 

TY  - JOUR
TI  - MediCoSpace: Visual Decision-Support for Doctor-Patient Consultations Using Medical Concept Spaces from EHRs
AU  - van der Linden, Sanne
AU  - Sevastjanova, Rita
AU  - Funk, Mathias
AU  - El-Assady, Mennatallah
T2  - ACM Trans. Manage. Inf. Syst.
AB  - Healthcare systems are under pressure from an aging population, rising costs, and increasingly complex conditions and treatments. Although data are determined to play a bigger role in how doctors diagnose and prescribe treatments, they struggle due to a lack of time and an abundance of structured and unstructured information. To address this challenge, we introduce MediCoSpace, a visual decision-support tool for more efficient doctor-patient consultations. The tool links patient reports to past and present diagnoses, diseases, drugs, and treatments, both for the current patient and other patients in comparable situations. MediCoSpace uses textual medical data, deep-learning supported text analysis and concept spaces to facilitate a visual discovery process. The tool is evaluated by five medical doctors. The results show that MediCoSpace facilitates a promising, yet complex way to discover unlikely relations and thus suggests a path toward the development of interactive visual tools to provide physicians with more holistic diagnoses and personalized, dynamic treatments for patients.
DA  - 2023/01//
PY  - 2023
DO  - 10.1145/3564275
VL  - 14
IS  - 2
SN  - 2158-656X
UR  - https://doi.org/10.1145/3564275
KW  - electronic health records
KW  - natural language processing
KW  - interaction design
KW  - Visual analytics
ER  - 

TY  - CONF
TI  - MedPath: Augmenting Health Risk Prediction via Medical Knowledge Paths
AU  - Ye, Muchao
AU  - Cui, Suhan
AU  - Wang, Yaqing
AU  - Luo, Junyu
AU  - Xiao, Cao
AU  - Ma, Fenglong
T3  - WWW '21
AB  - The broad adoption of electronic health records (EHR) data and the availability of biomedical knowledge graphs (KGs) on the web have provided clinicians and researchers unprecedented resources and opportunities for conducting health risk predictions to improve healthcare quality and medical resource allocation. Existing methods have focused on improving the EHR feature representations using attention mechanisms, time-aware models, or external knowledge. However, they ignore the importance of using personalized information to make predictions. Besides, the reliability of their prediction interpretations needs to be improved since their interpretable attention scores are not explicitly reasoned from disease progression paths. In this paper, we propose MedPath to solve these challenges and augment existing risk prediction models with the ability to use personalized information and provide reliable interpretations inferring from disease progression paths. Firstly, MedPath extracts personalized knowledge graphs (PKGs) containing all possible disease progression paths from observed symptoms to target diseases from a large-scale online medical knowledge graph. Next, to augment existing EHR encoders for achieving better predictions, MedPath learns a PKG embedding by conducting multi-hop message passing from symptom nodes to target disease nodes through a graph neural network encoder. Since MedPath reasons disease progression by paths existing in PKGs, it can provide explicit explanations for the prediction by pointing out how observed symptoms can finally lead to target diseases. Experimental results on three real-world medical datasets show that MedPath is effective in improving the performance of eight state-of-the-art methods with higher F1 scores and AUCs. Our case study also demonstrates that MedPath can greatly improve the explicitness of the risk prediction interpretation.1
C1  - New York, NY, USA
C3  - Proceedings of the Web Conference 2021
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442381.3449860
SP  - 1397
EP  - 1409
PB  - Association for Computing Machinery
SN  - 978-1-4503-8312-7
UR  - https://doi.org/10.1145/3442381.3449860
KW  - risk prediction
KW  - graph neural network
KW  - healthcare informatics
KW  - model interpretability
ER  - 

TY  - CONF
TI  - Company-Oriented Extractive Summarization of Financial News
AU  - Filippova, Katja
AU  - Surdeanu, Mihai
AU  - Ciaramita, Massimiliano
AU  - Zaragoza, Hugo
T3  - EACL '09
AB  - The paper presents a multi-document summarization system which builds company-specific summaries from a collection of financial news such that the extracted sentences contain novel and relevant information about the corresponding organization. The user's familiarity with the company's profile is assumed. The goal of such summaries is to provide information useful for the short-term trading of the corresponding company, i.e., to facilitate the inference from news to stock price movement in the next day. We introduce a novel query (i.e., company name) expansion method and a simple unsupervized algorithm for sentence ranking. The system shows promising results in comparison with a competitive baseline.
C1  - USA
C3  - Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics
DA  - 2009///
PY  - 2009
SP  - 246
EP  - 254
PB  - Association for Computational Linguistics
ER  - 

TY  - CONF
TI  - Finding Relations Between Diseases by Age-Series Based Supervised Link Prediction
AU  - Kaya, Buket
AU  - Poyraz, Mustafa
T3  - ASONAM '15
AB  - In the last few years we are witnessing to an increasing interest in the application of social network analysis and methods to health care information and management systems. The utilization of information technologies that could significantly improve efficiency and effectiveness of health care strategies are very important because of the implications they could have in everyday life of individuals. In this paper, we predict the onset of future diseases on the base of the current health status of patients. The problem of predicting the relations between diseases is a really difficult and, at the same time, an important task. For this purpose, this paper first constructs a weighted disease network and then, it proposes a novel link prediction method with supervised strategy to identify the connections between diseases, building the evolving structure of the disease network with respect to patients' ages. Experiments on a real network demonstrate that the proposed approach can reveal disease correlations accurately and perform well at capturing future disease risks.
C1  - New York, NY, USA
C3  - Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015
DA  - 2015///
PY  - 2015
DO  - 10.1145/2808797.2808812
SP  - 1097
EP  - 1103
PB  - Association for Computing Machinery
SN  - 978-1-4503-3854-7
UR  - https://doi.org/10.1145/2808797.2808812
KW  - health informatics
KW  - supervised learning
KW  - disease networks
KW  - link prediction
ER  - 

TY  - JOUR
TI  - Federated Learning for Electronic Health Records
AU  - Dang, Trung Kien
AU  - Lan, Xiang
AU  - Weng, Jianshu
AU  - Feng, Mengling
T2  - ACM Trans. Intell. Syst. Technol.
AB  - In data-driven medical research, multi-center studies have long been preferred over single-center ones due to a single institute sometimes not having enough data to obtain sufficient statistical power for certain hypothesis testings as well as predictive and subgroup studies. The wide adoption of electronic health records (EHRs) has made multi-institutional collaboration much more feasible. However, concerns over infrastructures, regulations, privacy, and data standardization present a challenge to data sharing across healthcare institutions. Federated Learning (FL), which allows multiple sites to collaboratively train a global model without directly sharing data, has become a promising paradigm to break the data isolation. In this study, we surveyed existing works on FL applications in EHRs and evaluated the performance of current state-of-the-art FL algorithms on two EHR machine learning tasks of significant clinical importance on a real world multi-center EHR dataset.
DA  - 2022/06//
PY  - 2022
DO  - 10.1145/3514500
VL  - 13
IS  - 5
SN  - 2157-6904
UR  - https://doi.org/10.1145/3514500
KW  - electronic health records
KW  - neural networks
KW  - Federated learning
KW  - healthcare
ER  - 

TY  - CONF
TI  - MeDetect: A LOD-Based System for Collective Entity Annotation in Biomedicine
AU  - Tian, Li
AU  - Zhang, Weinan
AU  - Bikakis, Antonis
AU  - Wang, Haofen
AU  - Yu, Yong
AU  - Ni, Yuan
AU  - Cao, Feng
T3  - WI-IAT '13
AB  - With the ever-growing use of textual biomedical data, domain entity annotation has become very important in biomedicine. Previous works on annotating domain entities from biomedical references suffer from several issues, such as a data flexibility problem, language dependency, and limitations with respect to word sense disambiguation. Meanwhile, the Linked Open Data (LOD) Initiative aims at interlinking data from various open knowledge bases. The numbers of entities and properties describing semantic relationships between entities within the linked data cloud have become very large. In this paper, we propose a knowledge-incentive approach for entity annotation in biomedicine, and present Me Detect, a prototype system that we developed based on this approach. With this approach, we over-come the problems of previous works using LOD-based collective annotation. Finally, we present the results of experiments that verify the effectiveness and efficiency of our approach.
C1  - USA
C3  - Proceedings of the 2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 01
DA  - 2013///
PY  - 2013
DO  - 10.1109/WI-IAT.2013.34
SP  - 233
EP  - 240
PB  - IEEE Computer Society
SN  - 978-0-7695-5145-6
UR  - https://doi.org/10.1109/WI-IAT.2013.34
KW  - Bio-Informatics
KW  - Domain Entity Annotation
KW  - Linked Open Data
ER  - 

TY  - CONF
TI  - PerDREP: Personalized Drug Effectiveness Prediction from Longitudinal Observational Data
AU  - Dey, Sanjoy
AU  - Zhang, Ping
AU  - Sow, Daby
AU  - Ng, Kenney
T3  - KDD '19
AB  - In contrast to the one-size-fits-all approach to medicine, precision medicine will allow targeted prescriptions based on the specific profile of the patient thereby avoiding adverse reactions and ineffective but expensive treatments. Longitudinal observational data such as Electronic Health Records (EHRs) have become an emerging data source for personalized medicine. In this paper, we propose a unified computational framework, called PerDREP, to predict the unique response patterns of each individual patient from EHR data. PerDREP models individual responses of each patient to the drug exposure by introducing a linear system to account for patients' heterogeneity, and incorporates a patient similarity graph as a network regularization. We formulate PerDREP as a convex optimization problem and develop an iterative gradient descent method to solve it. In the experiments, we identify the effect of drugs on Glycated hemoglobin test results. The experimental results provide evidence that the proposed method is not only more accurate than state-of-the-art methods, but is also able to automatically cluster patients into multiple coherent groups, thus paving the way for personalized medicine.
C1  - New York, NY, USA
C3  - Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining
DA  - 2019///
PY  - 2019
DO  - 10.1145/3292500.3330928
SP  - 1258
EP  - 1268
PB  - Association for Computing Machinery
SN  - 978-1-4503-6201-6
UR  - https://doi.org/10.1145/3292500.3330928
KW  - data mining
KW  - healthcare informatics
KW  - personalized model
ER  - 

TY  - CONF
TI  - Design Space Exploration and Parameter Tuning for Neuromorphic Applications
AU  - Carlson, Kristofor D.
AU  - Dutt, Nikil
AU  - Nageswaran, Jayram M.
AU  - Krichmar, Jeffrey L.
T3  - CODES+ISSS '13
AB  - Large-scale spiking neural networks (SNNs) have been used to successfully model complex neural circuits that explore various neural phenomena such as learning and memory, vision systems, auditory systems, neural oscillations, and many other important topics of neural function. Additionally, SNNs are particularly well-adapted to run on neuromorphic hardware as spiking events are often sparse, leading to a potentially large reduction in both bandwidth requirements and power usage. The inclusion of realistic plasticity equations, neural dynamics, and recurrent topologies has increased the descriptive power of SNNs but has also made the task of tuning these biologically realistic SNNs difficult. We present an automated parameter-tuning framework capable of tuning large-scale SNNs quickly and efficiently using evolutionary algorithms (EA) and off-the-shelf graphics processing units (GPUs).To test the feasibility of an automated parameter-tuning framework, our group used EAs to tune open parameters in SNNs running concurrently on a GPU. The SNNs were evolved to produce orientation-dependent stimulus responses similar to those found in simple cells of the primary visual cortex (V1) through the formation of self-organizing receptive fields (SORFs). The general evolutionary approach was as follows: A population of neural networks was created, each with a unique set of neural parameter values that defined overall behavior. Each SNN was then ranked based on a fitness value assigned by an objective function in which higher fitness values were given to SNNs that (a) reproduced responses observed in primate visual cortex, and (b) spanned the stimulus space, and (c) had sparse firing rates. The highest ranked individuals were selected, recombined, and mutated to form the offspring for the next generation. This process continued until a desired fitness was reached or until other termination conditions were met (Figure 1a).The automated parameter-tuning framework consisted of three software packages. The framework included: (a) the CARLsim SNN simulator [1], (2) the Evolving Objects (EO) computational framework [2], and (3) a parameter-tuning interface (PTI), developed by our group, to provide an interface between CARLsim and EO (See Figure 1b). The EO computational framework ran the evolutionary algorithm on the user-designated parameters of SNNs in CARLsim. The PTI allowed the objective function to be calculated independent of the EO computation framework. Parameter values were passed from the EO computation framework through the PTI to the SNN in CARLsim where the objective function is calculated. After the objective function was executed, the results were passed from the SNN in CARLsim through the PTI back to the EO computation framework for processing by the EA. With this approach, the fitness function calculation, which involved running each SNN in the population, could be run in parallel on the GPU while the remainder of EA calculations can be performed using the CPU (Figure 1b).A sample SNN with 4,104 neurons was tuned to respond with V1 simple cell-like tuning curves and produce SORFs. A performance analysis comparing the GPU-accelerated implementation to a single-threaded CPU implementation was carried out and showed that the GPU implementation could achieve a 65 times speedup over the CPU implementation. Additionally, the parameter value solutions found in the tuned SNN were stable and robust.The automated parameter-tuning framework presented here will be of use to both the computational neuroscience and neuromorphic engineering communities, making the process of constructing and tuning large-scale SNNs much quicker and easier.
C3  - Proceedings of the Ninth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis
DA  - 2013///
PY  - 2013
PB  - IEEE Press
SN  - 978-1-4799-1417-3
KW  - evolutionary algorithms
KW  - GPUs
KW  - neuromorphic engineering
KW  - spiking neural networks
ER  - 

TY  - CONF
TI  - Towards Longitudinal Analysis of a Population's Electronic Health Records Using Factor Graphs
AU  - Athreya, Arjun P.
AU  - Ngiam, Kee Yuan
AU  - Luo, Zhaojing
AU  - Tai, E Shyong
AU  - Kalbarczyk, Zbigniew
AU  - Iyer, Ravishankar K.
T3  - BDCAT '16
AB  - In this feasibility study, we demonstrate the use of a factor-graph-based probabilistic graphical model approach to process longitudinal data derived from a population's electronic health records (EHR). Processing of EHR allows for fore-casting patient-specific health complications and inference of population-level statistics on several epidemiological factors. As a case-study, we provide preliminary results and demonstrate feasibility of our approach by processing the EHR of a diabetic cohort in Singapore. Our model passes the feasibility test as we are able to forecast a series of health complications of a new patient based on the factor functions inferred from EHR of 100 diabetic patients spanning 10-years. This forecast gives both the caregivers and the patient a better view of the patient's health in the coming years and increases patient's motivation to stay healthy and conform to medication plan. Furthermore, our approach informs commonly occurring health complications in the population that warrant hospital readmissions, which helps a physician/clinician in decide when to intervene to avoid complications in order to improve the patient's quality of life and minimize the cost of care.
C1  - New York, NY, USA
C3  - Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies
DA  - 2016///
PY  - 2016
DO  - 10.1145/3006299.3006309
SP  - 79
EP  - 86
PB  - Association for Computing Machinery
SN  - 978-1-4503-4617-7
UR  - https://doi.org/10.1145/3006299.3006309
ER  - 

TY  - CONF
TI  - Consumer Wearables and Affective Computing for Wellbeing Support
AU  - Saganowski, Stanislaw
AU  - Kazienko, Przemyslaw
AU  - Dziezyc, Maciej
AU  - Jakimow, Patrycja
AU  - Komoszynska, Joanna
AU  - Michalska, Weronika
AU  - Dutkowiak, Anna
AU  - Polak, Adam
AU  - Dziadek, Adam
AU  - Ujma, Michal
T3  - MobiQuitous '20
AB  - Wearables equipped with pervasive sensors enable us to monitor physiological and behavioral signals in our everyday life. We propose the WellAff system able to recognize affective states for wellbeing support. It also includes health care scenarios, in particular patients with chronic kidney disease suffering from bipolar disorders. For the need of a large-scale field study, we revised over 50 off-the-shelf devices in terms of usefulness for emotion, stress, meditation, sleep, and physical activity recognition and analysis. Their usability directly comes from the types of sensors they possess as well as the quality and availability of raw signals. We found there is no versatile device suitable for all purposes. Using Empatica E4 and Samsung Galaxy Watch, we have recorded physiological signals from 11 participants over many weeks. The gathered data enabled us to train a classifier that accurately recognizes strong affective states.
C1  - New York, NY, USA
C3  - MobiQuitous 2020 - 17th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services
DA  - 2021///
PY  - 2021
DO  - 10.1145/3448891.3450332
SP  - 482
EP  - 487
PB  - Association for Computing Machinery
SN  - 978-1-4503-8840-5
UR  - https://doi.org/10.1145/3448891.3450332
KW  - stress
KW  - affect recognition
KW  - affective computing
KW  - armband
KW  - fitband
KW  - meditation
KW  - physical activity
KW  - sleep
KW  - smartwatch
KW  - wearables
KW  - wellaff system
KW  - wellbeing
KW  - wristband
ER  - 

TY  - CONF
TI  - Knowledge Discovery with Networks for Climate Science: Questions and Answers from Ckd Hamburg
AU  - Steinhaeuser, Karsten
T3  - CKD '11
AB  - The use of complex networks has been motivated in climate to understand attributes of large-scale dynamics, for example, correlations between variables and relations among climate oscillators. This talk addresses two specific questions that arose from discussions at the first Climate Knowledge Discovery Workshop held in Hamburg earlier this year. The first part revolves around the ability to represent the salient features of the climate system via networks. We show that the structure of networks constructed from outputs of physically-based climate models generally exhibit less intra-model variability (among members of an initial condition ensemble of the same model) than inter-model variability (between different models run with the same forcing). Moreover, we illustrate the stability of these patterns over a long-term integration of a climate model run. The second part deals with the effect of model resolution on large-scale phenomena. Specifically, we examine some high-level features (dipole patterns) extracted from two runs of the same climate model run at different resolutions using graph-based methods. The findings from both of these investigations lend additional credence to this growing field of network-based representation and analysis of complex systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2011 Workshop on Climate Knowledge Discovery
DA  - 2011///
PY  - 2011
DO  - 10.1145/2110230.2110232
SP  - 1
PB  - Association for Computing Machinery
SN  - 978-1-4503-1181-6
UR  - https://doi.org/10.1145/2110230.2110232
KW  - data mining
KW  - knowledge discovery
KW  - complex networks
ER  - 

TY  - CONF
TI  - Neural Computation for Robust and Holographic Face Detection
AU  - Imani, Mohsen
AU  - Zakeri, Ali
AU  - Chen, Hanning
AU  - Kim, TaeHyun
AU  - Poduval, Prathyush
AU  - Lee, Hyunsei
AU  - Kim, Yeseong
AU  - Sadredini, Elaheh
AU  - Imani, Farhad
T3  - DAC '22
AB  - Face detection is an essential component of many tasks in computer vision with several applications. However, existing deep learning solutions are significantly slow and inefficient to enable face detection on embedded platforms. In this paper, we propose HDFace, a novel framework for highly efficient and robust face detection. HDFace exploits HyperDimensional Computing (HDC) as a neurally-inspired computational paradigm that mimics important brain functionalities towards high-efficiency and noise-tolerant computation. We first develop a novel technique that enables HDC to perform stochastic arithmetic computations over binary hypervectors. Next, we expand these arithmetic for efficient and robust processing of feature extraction algorithms in hyperspace. Finally, we develop an adaptive hyperdimensional classification algorithm for effective and robust face detection. We evaluate the effectiveness of HDFace on large-scale emotion detection and face detection applications. Our results indicate that HDFace provides, on average, 6.1X (4.6X) speedup and 3.0X (12.1X) energy efficiency as compared to neural networks running on CPU (FPGA), respectively.
C1  - New York, NY, USA
C3  - Proceedings of the 59th ACM/IEEE Design Automation Conference
DA  - 2022///
PY  - 2022
DO  - 10.1145/3489517.3530653
SP  - 31
EP  - 36
PB  - Association for Computing Machinery
SN  - 978-1-4503-9142-9
UR  - https://doi.org/10.1145/3489517.3530653
ER  - 

TY  - CONF
TI  - Clinical Risk Prediction with Multilinear Sparse Logistic Regression
AU  - Wang, Fei
AU  - Zhang, Ping
AU  - Qian, Buyue
AU  - Wang, Xiang
AU  - Davidson, Ian
T3  - KDD '14
AB  - Logistic regression is one core predictive modeling technique that has been used extensively in health and biomedical problems. Recently a lot of research has been focusing on enforcing sparsity on the learned model to enhance its effectiveness and interpretability, which results in sparse logistic regression model. However, no matter the original or sparse logistic regression, they require the inputs to be in vector form. This limits the applicability of logistic regression in the problems when the data cannot be naturally represented vectors (e.g., functional magnetic resonance imaging and electroencephalography signals). To handle the cases when the data are in the form of multi-dimensional arrays, we propose MulSLR: Multilinear Sparse Logistic Regression. MulSLR can be viewed as a high order extension of sparse logistic regression. Instead of solving one classification vector as in conventional logistic regression, we solve for K classification vectors in MulSLR (K is the number of modes in the data). We propose a block proximal descent approach to solve the problem and prove its convergence. The convergence rate of the proposed algorithm is also analyzed. Finally we validate the efficiency and effectiveness of MulSLR on predicting the onset risk of patients with Alzheimer's disease and heart failure.
C1  - New York, NY, USA
C3  - Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
DA  - 2014///
PY  - 2014
DO  - 10.1145/2623330.2623755
SP  - 145
EP  - 154
PB  - Association for Computing Machinery
SN  - 978-1-4503-2956-9
UR  - https://doi.org/10.1145/2623330.2623755
KW  - healthcare
KW  - logistic regression
KW  - multilinear
KW  - proximal gradient
ER  - 

TY  - JOUR
TI  - A New Semantic Functional Similarity over Gene Ontology
AU  - Jeong, Jong Cheol
AU  - Chen, Xuewen
T2  - IEEE/ACM Trans. Comput. Biol. Bioinformatics
AB  - Identifying functionally similar or closely related genes and gene products has significant impacts on biological and clinical studies as well as drug discovery. In this paper, we propose an effective and practically useful method measuring both gene and gene product similarity by integrating the topology of gene ontology, known functional domains and their functional annotations. The proposed method is comprehensively evaluated through statistical analysis of the similarities derived from sequence, structure and phylogenetic profiles, and clustering analysis of disease genes clusters. Our results show that the proposed method clearly outperforms other conventional methods. Furthermore, literature analysis also reveals that the proposed method is both statistically and biologically promising for identifying functionally similar genes or gene products. In particular, we demonstrate that the proposed functional similarity metric is capable of discoverying new disease related genes or gene products.
DA  - 2015/03//
PY  - 2015
DO  - 10.1109/TCBB.2014.2343963
VL  - 12
IS  - 2
SP  - 322
EP  - 334
SN  - 1545-5963
UR  - https://doi.org/10.1109/TCBB.2014.2343963
KW  - clustering
KW  - bioinformatics (genome and proteome) databases
KW  - biology and genetics
KW  - similarity measures
ER  - 

TY  - CONF
TI  - Distilling Knowledge from Publicly Available Online EMR Data to Emerging Epidemic for Prognosis
AU  - Ma, Liantao
AU  - Ma, Xinyu
AU  - Gao, Junyi
AU  - Jiao, Xianfeng
AU  - Yu, Zhihao
AU  - Zhang, Chaohe
AU  - Ruan, Wenjie
AU  - Wang, Yasha
AU  - Tang, Wen
AU  - Wang, Jiangtao
T3  - WWW '21
AB  - Due to the characteristics of COVID-19, the epidemic develops rapidly and overwhelms health service systems worldwide. Many patients suffer from life-threatening systemic problems and need to be carefully monitored in ICUs. An intelligent prognosis can help physicians take an early intervention, prevent adverse outcomes, and optimize the medical resource allocation, which is urgently needed, especially in this ongoing global pandemic crisis. However, in the early stage of the epidemic outbreak, the data available for analysis is limited due to the lack of effective diagnostic mechanisms, the rarity of the cases, and privacy concerns. In this paper, we propose a distilled transfer learning framework, which leverages the existing publicly available online Electronic Medical Records to enhance the prognosis for inpatients with emerging infectious diseases. It learns to embed the COVID-19-related medical features based on massive existing EMR data. The transferred parameters are further trained to imitate the teacher model’s representation based on distillation, which embeds the health status more comprehensively on the source dataset. We conduct Length-of-Stay prediction experiments for patients in ICUs on real-world COVID-19 datasets. The experiment results indicate that our proposed model consistently outperforms competitive baseline methods. In order to further verify the scalability of o deal with different clinical tasks on different EMR datasets, we conduct an additional mortality prediction experiment on End-Stage Renal Disease datasets. The extensive experiments demonstrate that an benefit the prognosis for emerging pandemics and other diseases with limited EMR.
C1  - New York, NY, USA
C3  - Proceedings of the Web Conference 2021
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442381.3449855
SP  - 3558
EP  - 3568
PB  - Association for Computing Machinery
SN  - 978-1-4503-8312-7
UR  - https://doi.org/10.1145/3442381.3449855
KW  - Healthcare Informatics
KW  - Prognosis
KW  - Electronic Medical Record
KW  - Transfer Learning
ER  - 

TY  - CONF
TI  - Towards an Architecture for Quality Audit Reporting to Improve Hypertension Management
AU  - Mabotuwana, Thusitha
AU  - Warren, Jim
AU  - Gaikwad, Rekha
AU  - Kenelly, John
AU  - Kenealy, Timothy
T3  - HDKM '08
AB  - This paper illustrates an iterative, data-mining based approach to create a set of specific criteria (or quality indicators) for quality audit reporting from Patient Management System (PMS) data in the context of hypertension management. It represents an initial phase of research towards developing an architecture that can be used to easily specify various quality auditing criteria to support process improvement in chronic disease management. To inform our architectural design requirements we have developed a range of specific quality indicators with the assistance of an expert panel consisting of clinical staff from a general practice in Auckland, New Zealand. These criteria have been formulated into a Quality Audit Report (QAR) laid out under three broad categories of criteria: descriptive, supportive, and cautionary. A key aspect of the criteria we have developed is the incorporation of various temporal issues directly related to the successful management of chronic illness – hypertension in particular, and its comorbidities. We are approaching the phase of having clinicians assess a sample of patients, blind to the standing of the patients with respect to the cautionary QAR criteria, to assess the sensitivity and specificity of our inferences from the PMS data.
C1  - AUS
C3  - Proceedings of the Second Australasian Workshop on Health Data and Knowledge Management - Volume 80
DA  - 2008///
PY  - 2008
SP  - 45
EP  - 54
PB  - Australian Computer Society, Inc.
SN  - 978-1-920682-61-3
KW  - data mining
KW  - chronic disease management
KW  - hypertension
KW  - clinical audit
KW  - quality assurance
KW  - quality indicators
ER  - 

TY  - CONF
TI  - A Spectral Clustering Technique for Studying Post-Transplant Kidney Functions
AU  - Gangopadhyay, Aryya
AU  - Joshi, Ashish
AU  - Wali, Ravinder
T3  - IHI '12
AB  - In this paper, we present a novel method for studying the deterioration of renal functions after kidney transplant. We track the kidney functions of 111 patients for 24 months after the kidney transplant and use the time series data to group the patients into four clusters. We have developed two graph-based algorithms for analyzing the data as a pre-processing step prior to the formation of the clusters. The resultant clusters thus formed are statistically analyzed to determine the socio-demographic and clinical factors that may provide insights into the renal functions after the transplants. We also compare the cluster formation against other manifold learning techniques. The quality of the clusters was assessed using the silhouette function. We discuss how our findings can be used for effective intervention strategies.
C1  - New York, NY, USA
C3  - Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium
DA  - 2012///
PY  - 2012
DO  - 10.1145/2110363.2110388
SP  - 201
EP  - 208
PB  - Association for Computing Machinery
SN  - 978-1-4503-0781-9
UR  - https://doi.org/10.1145/2110363.2110388
KW  - kidney functions
KW  - spectral clustering
ER  - 

TY  - CONF
TI  - A Statistical Medical Summary Translation System
AU  - Chen, Han-Bin
AU  - Huang, Hen-Hsen
AU  - Tan, Ching-Ting
AU  - Tjiu, Jengwei
AU  - Chen, Hsin-Hsi
T3  - IHI '12
AB  - In a hospital, a medical summary is indispensable for both a clinician and a patient. However, it is written in English in some non-English native countries and becomes a barrier for a patient to read. In this paper we propose a framework for rapid acquisition of bilingual medical summaries using machine translation (MT) techniques. We describe a medical summary corpus and some terminological databases prepared for the framework. We then touch on the challenging issues of MT adapted from generic to specific domains, and propose a pattern translation scheme to achieve domain adaptation based on a background statistical MT system. We identify the significant patterns to capture the specific writing styles in a medical summary. The patterns are then translated with the involvements of doctors. Our major concern is to reduce the cost of translation and better allocate the efforts made by the domain experts. The experimental results show the proposed methods are effective in terms of the significance and diversity of the patterns. The approaches to integrate the mined patterns into background MT are also discussed.
C1  - New York, NY, USA
C3  - Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium
DA  - 2012///
PY  - 2012
DO  - 10.1145/2110363.2110378
SP  - 101
EP  - 110
PB  - Association for Computing Machinery
SN  - 978-1-4503-0781-9
UR  - https://doi.org/10.1145/2110363.2110378
KW  - machine translation
KW  - medical summary
KW  - pattern identification
ER  - 

TY  - CONF
TI  - BRIC: Locality-Based Encoding for Energy-Efficient Brain-Inspired Hyperdimensional Computing
AU  - Imani, Mohsen
AU  - Morris, Justin
AU  - Messerly, John
AU  - Shu, Helen
AU  - Deng, Yaobang
AU  - Rosing, Tajana
T3  - DAC '19
AB  - Brain-inspired Hyperdimensional (HD) computing is a new computing paradigm emulating the neuron's activity in high-dimensional space. The first step in HD computing is to map each data point into high-dimensional space (e.g., 10,000), which requires the computation of thousands of operations for each element of data in the original domain. Encoding alone takes about 80% of the execution time of training. In this paper, we propose BRIC, a fully binary Brain-Inspired Classifier based on HD computing for energy-efficient and high-accuracy classification. BRIC introduces a novel encoding module based on random projection with a predictable memory access pattern which can efficiently be implemented in hardware. BRIC is the first HD-based approach which provides data projection with a 1:1 ratio to the original data and enables all training/inference computation to be performed using binary hypervectors. To further improve BRIC efficiency, we develop an online dimension reduction approach which removes insignificant hypervector dimensions during training. Additionally, we designed a fully pipelined FPGA implementation which accelerates BRIC in both training and inference phases. Our evaluation of BRIC a wide range of classification applications show that BRIC can achieve 64.1× and 9.8× (43.8× and 6.1×) energy efficiency and speed up as compared to baseline HD computing during training (inference) while providing the same classification accuracy.
C1  - New York, NY, USA
C3  - Proceedings of the 56th Annual Design Automation Conference 2019
DA  - 2019///
PY  - 2019
DO  - 10.1145/3316781.3317785
PB  - Association for Computing Machinery
SN  - 978-1-4503-6725-7
UR  - https://doi.org/10.1145/3316781.3317785
KW  - Machine learning
KW  - Brain-inspired computing
KW  - Energy efficiency
KW  - Hyperdimensional computing
ER  - 

TY  - CONF
TI  - Disease Predictive Modeling for Healthcare Management System
AU  - Nakhat, Khulood
AU  - Khalique, Fatima
AU  - Khan, Shoab Ahmed
T3  - ICMHI '20
AB  - This study attempts to perform predictive analytics for decision makers in healthcare management systems using surveillance data from multiple sources for formulating intervention programs based on the results. With the availability of big data in health from multiple sources including electronic health records, it is possible to integrate data and perform near real-time predictive analysis for incoming streams of disease incidences. We use a temporal predictive Auto-Regressive Integrated Moving Averaging model (ARIMA) in combination with a minimum size moving window to forecast the disease incidences over a data collection and integration framework. We applied our model for predictive analysis of Hepatitis C incidences in Vehari District of Punjab province in Pakistan. Model performance is evaluated based on Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). The model is capable of finding trends of any disease to aid timely decision making in the healthcare management context.
C1  - New York, NY, USA
C3  - Proceedings of the 4th International Conference on Medical and Health Informatics
DA  - 2020///
PY  - 2020
DO  - 10.1145/3418094.3418134
SP  - 37
EP  - 44
PB  - Association for Computing Machinery
SN  - 978-1-4503-7776-8
UR  - https://doi.org/10.1145/3418094.3418134
KW  - time series
KW  - ARIMA
KW  - forecast
KW  - predictive analysis
KW  - Public health management
KW  - stochastic modeling
ER  - 

TY  - CONF
TI  - Parallel Mining Algorithms for Generalized Association Rules with Classification Hierarchy
AU  - Shintani, Takahiko
AU  - Kitsuregawa, Masaru
T3  - SIGMOD '98
AB  - Association rule mining recently attracted strong attention. Usually, the classification hierarchy over the data items is available. Users are interested in generalized association rules that span different levels of the hierarchy, since sometimes more interesting rules can be derived by taking the hierarchy into account.In this paper, we propose the new parallel algorithms for mining association rules with classification hierarchy on a shared-nothing parallel machine to improve its performance. Our algorithms partition the candidate itemsets over the processors, which exploits the aggregate memory of the system effectively. If the candidate itemsets are partitioned without considering classification hierarchy, both the items and its all the ancestor items have to be transmitted, that causes prohibitively large amount of communications. Our method minimizes interprocessor communication by considering the hierarchy. Moreover, in our algorithm, the available memory space is fully utilized by identifying the frequently occurring candidate itemsets and copying them over all the processors, through which frequent itemsets can be processed locally without any communication. Thus it can effectively reduce the load skew among the processors. Several experiments are done by changing the granule of copying itemsets, from the whole tree, to the small group of the frequent itemsets along the hierarchy. The coarser the grain, the easier the control but it is rather difficult to achieve the sufficient load balance. The finer the grain, the more complicated the control is required but it can balance the load quite well.We implemented proposed algorithms on IBM SP-2. Performance evaluations show that our algorithms are effective for handling skew and attain sufficient speedup ratio.
C1  - New York, NY, USA
C3  - Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data
DA  - 1998///
PY  - 1998
DO  - 10.1145/276304.276308
SP  - 25
EP  - 36
PB  - Association for Computing Machinery
SN  - 0-89791-995-5
UR  - https://doi.org/10.1145/276304.276308
ER  - 

TY  - JOUR
TI  - Parallel Mining Algorithms for Generalized Association Rules with Classification Hierarchy
AU  - Shintani, Takahiko
AU  - Kitsuregawa, Masaru
T2  - SIGMOD Rec.
AB  - Association rule mining recently attracted strong attention. Usually, the classification hierarchy over the data items is available. Users are interested in generalized association rules that span different levels of the hierarchy, since sometimes more interesting rules can be derived by taking the hierarchy into account.In this paper, we propose the new parallel algorithms for mining association rules with classification hierarchy on a shared-nothing parallel machine to improve its performance. Our algorithms partition the candidate itemsets over the processors, which exploits the aggregate memory of the system effectively. If the candidate itemsets are partitioned without considering classification hierarchy, both the items and its all the ancestor items have to be transmitted, that causes prohibitively large amount of communications. Our method minimizes interprocessor communication by considering the hierarchy. Moreover, in our algorithm, the available memory space is fully utilized by identifying the frequently occurring candidate itemsets and copying them over all the processors, through which frequent itemsets can be processed locally without any communication. Thus it can effectively reduce the load skew among the processors. Several experiments are done by changing the granule of copying itemsets, from the whole tree, to the small group of the frequent itemsets along the hierarchy. The coarser the grain, the easier the control but it is rather difficult to achieve the sufficient load balance. The finer the grain, the more complicated the control is required but it can balance the load quite well.We implemented proposed algorithms on IBM SP-2. Performance evaluations show that our algorithms are effective for handling skew and attain sufficient speedup ratio.
DA  - 1998/06//
PY  - 1998
DO  - 10.1145/276305.276308
VL  - 27
IS  - 2
SP  - 25
EP  - 36
SN  - 0163-5808
UR  - https://doi.org/10.1145/276305.276308
ER  - 

TY  - JOUR
TI  - Enabling Cost-Effective Population Health Monitoring By Exploiting Spatiotemporal Correlation: An Empirical Study
AU  - Chen, Dawei
AU  - Wang, Jiangtao
AU  - Ruan, Wenjie
AU  - Ni, Qiang
AU  - Helal, Sumi
T2  - ACM Trans. Comput. Healthcare
AB  - Because of its important role in health policy-shaping, population health monitoring (PHM) is considered a fundamental block for public health services. However, traditional public health data collection approaches, such as clinic-visit-based data integration or health surveys, could be very costly and time-consuming. To address this challenge, this article proposes a cost-effective approach called Compressive Population Health (CPH), where a subset of a given area is selected in terms of regions within the area for data collection in the traditional way, while leveraging inherent spatial correlations of neighboring regions to perform data inference for the rest of the area. By alternating selected regions longitudinally, this approach can validate and correct previously assessed spatial correlations. To verify whether the idea of CPH is feasible, we conduct an in-depth study based on spatiotemporal morbidity rates of chronic diseases in more than 500 regions around London for over 10 years. We introduce our CPH approach and present three extensive analytical studies. The first confirms that significant spatiotemporal correlations do exist. In the second study, by deploying multiple state-of-the-art data recovery algorithms, we verify that these spatiotemporal correlations can be leveraged to do data inference accurately using only a small number of samples. Finally, we compare different methods for region selection for traditional data collection and show how such methods can further reduce the overall cost while maintaining high PHM quality.
DA  - 2021/01//
PY  - 2021
DO  - 10.1145/3428665
VL  - 2
IS  - 2
SN  - 2691-1957
UR  - https://doi.org/10.1145/3428665
KW  - compressive sensing
KW  - data analysis
KW  - Population health
KW  - spatiotemporal correlation
ER  - 

TY  - CONF
TI  - Resolving the Bias in Electronic Medical Records
AU  - Zheng, Kaiping
AU  - Gao, Jinyang
AU  - Ngiam, Kee Yuan
AU  - Ooi, Beng Chin
AU  - Yip, Wei Luen James
T3  - KDD '17
AB  - Electronic Medical Records (EMR) are the most fundamental resources used in healthcare data analytics. Since people visit hospital more frequently when they feel sick and doctors prescribe lab examinations when they feel necessary, we argue that there could be a strong bias in EMR observations compared with the hidden conditions of patients. Directly using such EMR for analytical tasks without considering the bias may lead to misinterpretation. To this end, we propose a general method to resolve the bias by transforming EMR to regular patient hidden condition series using a Hidden Markov Model (HMM) variant. Compared with the biased EMR series with irregular time stamps, the unbiased regular time series is much easier to be processed by most analytical models and yields better results. Extensive experimental results demonstrate that our bias resolving method imputes missing data more accurately than baselines and improves the performance of the state-of-the-art methods on typical medical data analytics.
C1  - New York, NY, USA
C3  - Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
DA  - 2017///
PY  - 2017
DO  - 10.1145/3097983.3098149
SP  - 2171
EP  - 2180
PB  - Association for Computing Machinery
SN  - 978-1-4503-4887-4
UR  - https://doi.org/10.1145/3097983.3098149
KW  - healthcare
KW  - data analytics
KW  - time series
ER  - 

TY  - JOUR
TI  - ScRAE: Deterministic Regularized Autoencoders With Flexible Priors for Clustering Single-Cell Gene Expression Data
AU  - Mondal, Arnab Kumar
AU  - Asnani, Himanshu
AU  - Singla, Parag
AU  - AP, Prathosh
T2  - IEEE/ACM Trans. Comput. Biol. Bioinformatics
AB  - Clustering single-cell RNA sequence (scRNA-seq) data poses statistical and computational challenges due to their high-dimensionality and data-sparsity, also known as ‘dropout’ events. Recently, Regularized Auto-Encoder (RAE) based deep neural network models have achieved remarkable success in learning robust low-dimensional representations. The basic idea in RAEs is to learn a non-linear mapping from the high-dimensional data space to a low-dimensional latent space and vice-versa, simultaneously imposing a distributional prior on the latent space, which brings in a regularization effect. This paper argues that RAEs suffer from the infamous problem of bias-variance trade-off in their naive formulation. While a simple AE wita latent regularization results in data over-fitting, a very strong prior leads to under-representation and thus bad clustering. To address the above issues, we propose a modified RAE framework (called the scRAE) for effective clustering of the single-cell RNA sequencing data. scRAE consists of deterministic AE with a flexibly learnable prior generator network, which is jointly trained with the AE. This facilitates scRAE to trade-off better between the bias and variance in the latent space. We demonstrate the efficacy of the proposed method through extensive experimentation on several real-world single-cell Gene expression datasets. The code for our work is available at <uri>https://github.com/arnabkmondal/scRAE</uri>.
DA  - 2021/07//
PY  - 2021
DO  - 10.1109/TCBB.2021.3098394
VL  - 19
IS  - 5
SP  - 2996
EP  - 3007
SN  - 1545-5963
UR  - https://doi.org/10.1109/TCBB.2021.3098394
ER  - 

TY  - CONF
TI  - Counterfactual Phenotyping with Censored Time-to-Events
AU  - Nagpal, Chirag
AU  - Goswami, Mononito
AU  - Dufendach, Keith
AU  - Dubrawski, Artur
T3  - KDD '22
AB  - Estimation of treatment efficacy of real-world clinical interventions involves working with continuous time-to-event outcomes such as time-to-death, re-hospitalization, or a composite event that may be subject to censoring. Counterfactual reasoning in such scenarios requires decoupling the effects of confounding physiological characteristics that affect baseline survival rates from the effects of the interventions being assessed. In this paper, we present a latent variable approach to model heterogeneous treatment effects by proposing that an individual can belong to one of latent clusters with distinct response characteristics. We show that this latent structure can mediate the base survival rates and help determine the effects of an intervention. We demonstrate the ability of our approach to discover actionable phenotypes of individuals based on their treatment response on multiple large randomized clinical trials originally conducted to assess appropriate treatment strategies to reduce cardiovascular risk.
C1  - New York, NY, USA
C3  - Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
DA  - 2022///
PY  - 2022
DO  - 10.1145/3534678.3539110
SP  - 3634
EP  - 3644
PB  - Association for Computing Machinery
SN  - 978-1-4503-9385-0
UR  - https://doi.org/10.1145/3534678.3539110
KW  - survival analysis
KW  - phenotyping
KW  - heterogeneous treatment effects
ER  - 

TY  - CONF
TI  - Development of a Checklist for the Prevention of Intradialytic Hypotension in Hemodialysis Care: Design Considerations Based on Activity Theory
AU  - Kuo, Pei-Yi
AU  - Saran, Rajiv
AU  - Argentina, Marissa
AU  - Heung, Michael
AU  - Bragg-Gresham, Jennifer L.
AU  - Chatoth, Dinesh
AU  - Gillespie, Brenda
AU  - Krein, Sarah
AU  - Wingard, Rebecca
AU  - Zheng, Kai
AU  - Veinot, Tiffany C.
T3  - CHI '19
AB  - Hemodialysis is life-saving therapy for end-stage renal disease; yet, 20% of hemodialysis sessions are complicated by intradialytic hypotension ("IDH"). There is a need for approaches to preventing IDH that account for their implementation contexts. Using Activity Theory, we outline the design of a digital diagnostic checklist to identify patients at risk of IDH. Checklists were chosen a priori as an outcome due to prior evidence of effectiveness. Drawing on individual interviews with 20 clinicians and three focus groups with 17 patients, we describe four activity systems within hemodialysis care. We then outline a novel design process that includes co-design activities with clinicians, and four rapid-cycle iterations that progressively incorporated activity system elements into checklist design. We contribute a new type of checklist design to HCI: one that supports diagnostic thinking rather than consistent task completion. We further broaden checklist design by including a formal role for patients in checklist completion.
C1  - New York, NY, USA
C3  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
DA  - 2019///
PY  - 2019
DO  - 10.1145/3290605.3300872
SP  - 1
EP  - 14
PB  - Association for Computing Machinery
SN  - 978-1-4503-5970-2
UR  - https://doi.org/10.1145/3290605.3300872
KW  - activity theory
KW  - checklist
KW  - clinicians
KW  - co-design
KW  - dialysis
ER  - 

TY  - CONF
TI  - Acquisition of Knowledge from Data
AU  - Wiederhold, G C M
AU  - Walker, M
AU  - Blum, R
AU  - Downs, S
T3  - ISMIS '86
AB  - The work described here addresses two problems: information overload of database users, and knowledge acquisition for use in Al systems. We have implemented programs that use artificial intelligence techniques to prepare high-level, intelligent summaries of databases, and that use empirical databases in turn, in combination with statistical and Al methods, to generate new domain knowledge base. Both programs are examples of the aquisition of knowledge from data: the Summarization Module fuses large amounts of data succinctly, the Discovery Module extracts new knowledge present implicitly in data. We describe the implementation of our programs and outline planned extensions which combine both approaches. This work is distinguished from current knowledge engineering approaches in that we prime the system with expert knowledge, and then use factual data to learn more about the domain.
C1  - New York, NY, USA
C3  - Proceedings of the ACM SIGART International Symposium on Methodologies for Intelligent Systems
DA  - 1986///
PY  - 1986
DO  - 10.1145/12808.12817
SP  - 74
EP  - 84
PB  - Association for Computing Machinery
SN  - 0-89791-206-3
UR  - https://doi.org/10.1145/12808.12817
ER  - 

TY  - BOOK
TI  - ICMHI '22: Proceedings of the 6th International Conference on Medical and Health Informatics
CY  - New York, NY, USA
DA  - 2022///
PY  - 2022
PB  - Association for Computing Machinery
SN  - 978-1-4503-9630-1
ER  - 

TY  - JOUR
TI  - An Investigation of Content Representation Using Text Grammars
AU  - Rama, D. V.
AU  - Srinivasan, Padmini
T2  - ACM Trans. Inf. Syst.
AB  - We extend prior work on a model for natural language text representation and retrieval using a linguistic device called text grammar. We demonstrate the value of this approach in accessing relevant items from a collection of empirical abstracts in a medical domain. The advantage, when compared to traditional keyword retrieval, is that this approach is a significant move towards knowledge representation and retrieval. Text representation in this model includes keywords and their conceptual roles in the text. In particular, it involves extracting TOPIC predicates representing the research issue addressed and DESIGN predicates representing important methodological features of the empirical study. Preliminary experimentation shows that keywords exhibit a variety of text-grammar roles in a text database. Second, as intuitively expected, retrieval using TOPIC predicates identifies a smaller subset of texts than Boolean retrieval does. These empirical results along with the theoretical work indicate that the representation and retrieval strategies proposed have a significant potential. Finally, EMPIRICIST, a prototype system is described. In it the text representation predicates are implemented as a network while retrieval is through constrained-spreading activation strategies.
DA  - 1993/01//
PY  - 1993
DO  - 10.1145/151480.151490
VL  - 11
IS  - 1
SP  - 51
EP  - 75
SN  - 1046-8188
UR  - https://doi.org/10.1145/151480.151490
KW  - text grammar
KW  - text representation for medical abstracts
KW  - text representation for retrieval
ER  - 

TY  - JOUR
TI  - Energy-Saving Service Offloading for the Internet of Medical Things Using Deep Reinforcement Learning
AU  - Jiang, Jielin
AU  - Guo, Jiajie
AU  - Khan, Maqbool
AU  - Cui, Yan
AU  - Lin, Wenmin
T2  - ACM Trans. Sen. Netw.
AB  - As a critical branch of the Internet of Things (IoT) in the medicine industry, the Internet of Medical Things (IoMT) significantly improves the quality of healthcare due to its real-time monitoring and low medical cost. Benefiting from edge and cloud computing, IoMT is provided with more computing and storage resources near the terminal to meet the low-delay requirements of computation-intensive services. However, the service offloading from health monitoring units (HMUs) to edge servers generates additional energy consumption. Fortunately, artificial intelligence (AI), which has developed rapidly in recent years, has proved effective in some resource allocation applications. Taking both energy consumption and delay into account, we propose an energy-aware service offloading algorithm under an end-edge-cloud collaborative IoMT system with Asynchronous Advantage Actor-Critic (A3C), named ECAC. Technically, ECAC uses the structural similarity between the natural distributed IoMT system and A3C, whose parameters are asynchronously updated. Besides, due to the typical delay-sensitivity mechanism and time-energy correction, ECAC can adjust dynamically to the diverse service types and system requirements. Finally, the effectiveness of ECAC for IoMT is proved on real data.
DA  - 2022/08//
PY  - 2022
DO  - 10.1145/3560265
SN  - 1550-4859
UR  - https://doi.org/10.1145/3560265
KW  - asynchronous advantage actor-critic
KW  - deep reinforcement learning
KW  - internet of medical things
KW  - service offloading
ER  - 

TY  - CONF
TI  - Analysis of Kidney Stone Detection by Reaction Diffusion Level Set Segmentation and Xilinx System Generator
AU  - Viswanath, K.
AU  - Gunasundari, R.
AU  - Hussain, Syed Aathif
T3  - ICARCSET '15
AB  - The abnormalities of the kidney can be identified by ultrasound imaging has structural abnormalities like kidney swelling, change in its position appearance, formation of stones, cysts, cancerous cells, congenital anomalies and blockage of urine etc. For surgical operations it is very important to identify the exact and accurate location of stone in the kidney. The ultrasound images are of low contrast and contain speckle noise. This makes the detection of kidney abnormalities rather challenging task. Thus preprocessing of ultrasound images is carried out on Xilinx System generator (XSG) and implemented on Vertex-5 (XC5VLX50T) FPGA to remove speckle noise. In preprocessing, first image restoration is done to reduce speckle noise then it is applied to Gabor filter for smoothening. Next the resultant image is enhanced using convolution. The preprocessed ultrasound image is segmented using reaction diffusion (RD) level set segmentation (LSS), since it yields better results. it uses a two-step splitting methods to iteratively solve the RD-LSS equation, first step is iterating LSS equation, and then solving the diffusion equation. The second step is to regularize the level set function which is the obtained from first step for better stability. The RD is included for LSS for eliminating of anti-leakages on image boundary. The RD-LSS does not require any expensive re-initialization and it is very high speed of operation. The RD-LSS results are compared with distance regularized level set evolution (DRLSE1), DRLSE1 and DRLSE2. Extracted region of the kidney after segmentation is applied to Symlets, Biorthogonal (bio3.7, bio3.9 &amp; bio4.4) and Daubechies wavelet subbands to extract energy levels. These energy level gives an indication about presence of stone in that particular location which significantly vary from that of normal energy level. These energy levels are trained by Multilayer Perceptron (MLP) and Back Propagation (BP) ANN to identify the type of stone with an accuracy of 99.1%.
C1  - New York, NY, USA
C3  - Proceedings of the 2015 International Conference on Advanced Research in Computer Science Engineering &amp; Technology (ICARCSET 2015)
DA  - 2015///
PY  - 2015
DO  - 10.1145/2743065.2743101
PB  - Association for Computing Machinery
SN  - 978-1-4503-3441-9
UR  - https://doi.org/10.1145/2743065.2743101
KW  - Kidney Stone detection
KW  - Level Set Segmentation
KW  - Lifting Scheme wavelet transform
KW  - Multilayer Perceptron (MLP) and Back Propagation (BP)
KW  - XSG and Ultrasound imaging
ER  - 

TY  - CONF
TI  - A Fuzzy Expert System for Coronary Artery Disease Diagnosis
AU  - Jain, Prerna
AU  - Kaur, Amandeep
T3  - ICAICR '19
AB  - Medicinal services industry is the one of most rapidly developing industry in the world. As sicknesses growing rapidly, the information of the patients will be extended. Heart illness is the one of developing infection. Coronary artery heart disease is the huge purpose behind the grimness in the advanced society. The early diagnosis will be improve the therapeutic divisions. The aim of this study is to design a fuzzy expert system for diagnosis of coronary artery heart disease. With the assistance of this framework, the specialists will be finding the patient early and the chance of re-admission to the emergency clinics will be diminishes. The framework has nine information documented and one yield recorded.
C1  - New York, NY, USA
C3  - Proceedings of the Third International Conference on Advanced Informatics for Computing Research
DA  - 2019///
PY  - 2019
DO  - 10.1145/3339311.3339358
PB  - Association for Computing Machinery
SN  - 978-1-4503-6652-6
UR  - https://doi.org/10.1145/3339311.3339358
KW  - fuzzy expert system
KW  - healthcare
KW  - coronary artery heart disease
KW  - fuzzy logic
KW  - rule based system
ER  - 

TY  - BOOK
TI  - ICBRA '22: Proceedings of the 9th International Conference on Bioinformatics Research and Applications
CY  - New York, NY, USA
DA  - 2022///
PY  - 2022
PB  - Association for Computing Machinery
SN  - 978-1-4503-9686-8
ER  - 

TY  - CONF
TI  - Probabilistic Programming
AU  - Gordon, Andrew D.
AU  - Henzinger, Thomas A.
AU  - Nori, Aditya V.
AU  - Rajamani, Sriram K.
T3  - FOSE 2014
AB  - Probabilistic programs are usual functional or imperative programs with two added constructs: (1) the ability to draw values at random from distributions, and (2) the ability to condition values of variables in a program via observations. Models from diverse application areas such as computer vision, coding theory, cryptographic protocols, biology and reliability analysis can be written as probabilistic programs. Probabilistic inference is the problem of computing an explicit representation of the probability distribution implicitly specified by a probabilistic program. Depending on the application, the desired output from inference may vary—we may want to estimate the expected value of some function f with respect to the distribution, or the mode of the distribution, or simply a set of samples drawn from the distribution. In this paper, we describe connections this research area called “Probabilistic Programming" has with programming languages and software engineering, and this includes language design, and the static and dynamic analysis of programs. We survey current state of the art and speculate on promising directions for future research.
C1  - New York, NY, USA
C3  - Future of Software Engineering Proceedings
DA  - 2014///
PY  - 2014
DO  - 10.1145/2593882.2593900
SP  - 167
EP  - 181
PB  - Association for Computing Machinery
SN  - 978-1-4503-2865-4
UR  - https://doi.org/10.1145/2593882.2593900
KW  - Machine learning
KW  - Probabilistic programming
KW  - Program analysis
ER  - 

TY  - CONF
TI  - Finding Progression Stages in Time-Evolving Event Sequences
AU  - Yang, Jaewon
AU  - McAuley, Julian
AU  - Leskovec, Jure
AU  - LePendu, Paea
AU  - Shah, Nigam
T3  - WWW '14
AB  - Event sequences, such as patients' medical histories or users' sequences of product reviews, trace how individuals progress over time. Identifying common patterns, or progression stages, in such event sequences is a challenging task because not every individual follows the same evolutionary pattern, stages may have very different lengths, and individuals may progress at different rates. In this paper, we develop a model-based method for discovering common progression stages in general event sequences. We develop a generative model in which each sequence belongs to a class, and sequences from a given class pass through a common set of stages, where each sequence evolves at its own rate. We then develop a scalable algorithm to infer classes of sequences, while also segmenting each sequence into a set of stages. We evaluate our method on event sequences, ranging from patients' medical histories to online news and navigational traces from the Web. The evaluation shows that our methodology can predict future events in a sequence, while also accurately inferring meaningful progression stages, and effectively grouping sequences based on common progression patterns. More generally, our methodology allows us to reason about how event sequences progress over time, by discovering patterns and categories of temporal evolution in large-scale datasets of events.
C1  - New York, NY, USA
C3  - Proceedings of the 23rd International Conference on World Wide Web
DA  - 2014///
PY  - 2014
DO  - 10.1145/2566486.2568044
SP  - 783
EP  - 794
PB  - Association for Computing Machinery
SN  - 978-1-4503-2744-2
UR  - https://doi.org/10.1145/2566486.2568044
KW  - time series
KW  - event sequences
KW  - user modeling
ER  - 

TY  - CONF
TI  - Pseudo Graph Convolutional Network for Vehicle ReID
AU  - Qian, Wen
AU  - He, Zhiqun
AU  - Peng, Silong
AU  - Chen, Chen
AU  - Wu, Wei
T3  - MM '21
AB  - Image-based Vehicle ReID methods have suffered from limited information caused by viewpoints, illumination, and occlusion as they usually use a single image as input. Graph convolutional methods (GCN) can alleviate the aforementioned problem by aggregating neighbor samples' information to enhance the feature representation. However, it's uneconomical and computational for the inference processes of GCN-based methods since they need to iterate over all samples for searching the neighbor nodes. In this paper, we propose the first Pseudo-GCN Vehicle ReID method (PGVR) which enables a CNN-based module to performs competitively to GCN-based methods and has a faster and lightweight inference process. To enable the Pseudo-GCN mechanism, a two-branch network and a graph-based knowledge distillation are proposed. The two-branch network consists of a CNN-based student branch and a GCN-based teacher branch. The GCN-based teacher branch adopts a ReID-based GCN to learn the topological optimization ability under the supervision of ReID tasks during training time. Moreover, the graph-based knowledge distillation explicitly transfers the topological optimization ability from the teacher branch to the student branch which acknowledges all nodes. We evaluate our proposed method PGVR on three mainstream Vehicle ReID benchmarks and demonstrate that PGVR achieves state-of-the-art performance.
C1  - New York, NY, USA
C3  - Proceedings of the 29th ACM International Conference on Multimedia
DA  - 2021///
PY  - 2021
DO  - 10.1145/3474085.3475462
SP  - 3162
EP  - 3171
PB  - Association for Computing Machinery
SN  - 978-1-4503-8651-7
UR  - https://doi.org/10.1145/3474085.3475462
KW  - knowledge distillation
KW  - distillation evaluation metric
KW  - graph convolutional network
KW  - Pseudo-GCN vehicle ReID
KW  - vehicle ReID
ER  - 

TY  - CONF
TI  - DeepMood: Modeling Mobile Phone Typing Dynamics for Mood Detection
AU  - Cao, Bokai
AU  - Zheng, Lei
AU  - Zhang, Chenwei
AU  - Yu, Philip S.
AU  - Piscitello, Andrea
AU  - Zulueta, John
AU  - Ajilore, Olu
AU  - Ryan, Kelly
AU  - Leow, Alex D.
T3  - KDD '17
AB  - The increasing use of electronic forms of communication presents new opportunities in the study of mental health, including the ability to investigate the manifestations of psychiatric diseases unobtrusively and in the setting of patients' daily lives. A pilot study to explore the possible connections between bipolar affective disorder and mobile phone usage was conducted. In this study, participants were provided a mobile phone to use as their primary phone. This phone was loaded with a custom keyboard that collected metadata consisting of keypress entry time and accelerometer movement. Individual character data with the exceptions of the backspace key and space bar were not collected due to privacy concerns. We propose an end-to-end deep architecture based on late fusion, named DeepMood, to model the multi-view metadata for the prediction of mood scores. Experimental results show that 90.31% prediction accuracy on the depression score can be achieved based on session-level mobile phone typing dynamics which is typically less than one minute. It demonstrates the feasibility of using mobile phone metadata to infer mood disturbance and severity.
C1  - New York, NY, USA
C3  - Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
DA  - 2017///
PY  - 2017
DO  - 10.1145/3097983.3098086
SP  - 747
EP  - 755
PB  - Association for Computing Machinery
SN  - 978-1-4503-4887-4
UR  - https://doi.org/10.1145/3097983.3098086
KW  - bipolar disorder
KW  - recurrent network
KW  - sequence prediction
KW  - typing dynamics
ER  - 

TY  - CONF
TI  - Adaptive Neural Recovery for Highly Robust Brain-like Representation
AU  - Poduval, Prathyush
AU  - Ni, Yang
AU  - Kim, Yeseong
AU  - Ni, Kai
AU  - Kumar, Raghavan
AU  - Cammarota, Rossario
AU  - Imani, Mohsen
T3  - DAC '22
AB  - Today's machine learning platforms have major robustness issues dealing with insecure and unreliable memory systems. In conventional data representation, bit flips due to noise or attack can cause value explosion, which leads to incorrect learning prediction. In this paper, we propose RobustHD, a robust and noise-tolerant learning system based on HyperDimensional Computing (HDC), mimicking important brain functionalities. Unlike traditional binary representation, RobustHD exploits a redundant and holographic representation, ensuring all bits have the same impact on the computation. RobustHD also proposes a runtime framework that adaptively identifies and regenerates the faulty dimensions in an unsupervised way. Our solution not only provides security against possible bit-flip attacks but also provides a learning solution with high robustness to noises in the memory. We performed a cross-stacked evaluation from a conventional platform to emerging processing in-memory architecture. Our evaluation shows that under 10% random bit flip attack, RobustHD provides a maximum of 0.53% quality loss, while deep learning solutions are losing over 26.2% accuracy.
C1  - New York, NY, USA
C3  - Proceedings of the 59th ACM/IEEE Design Automation Conference
DA  - 2022///
PY  - 2022
DO  - 10.1145/3489517.3530659
SP  - 367
EP  - 372
PB  - Association for Computing Machinery
SN  - 978-1-4503-9142-9
UR  - https://doi.org/10.1145/3489517.3530659
ER  - 

TY  - CONF
TI  - Learning Recommender Systems with Implicit Feedback via Soft Target Enhancement
AU  - Cheng, Mingyue
AU  - Yuan, Fajie
AU  - Liu, Qi
AU  - Ge, Shenyang
AU  - Li, Zhi
AU  - Yu, Runlong
AU  - Lian, Defu
AU  - Yuan, Senchao
AU  - Chen, Enhong
T3  - SIGIR '21
AB  - One-hot encoder accompanied by a softmax loss has become the default configuration to deal with the multiclass problem, and is also prevalent in deep learning (DL) based recommender systems (RS). The standard learning process of such methods is to fit the model outputs to a one-hot encoding of the ground truth, referred to as the hard target. However, it is known that these hard targets largely ignore the ambiguity of unobserved feedback in RS, and thus may lead to sub-optimal generalization performance. In this work, we propose SoftRec, a new RS optimization framework to enhance item recommendation. The core idea is that we add additional supervisory signals - well-designed soft targets - for each instance so as to better guide the recommender learning. Meanwhile, we carefully investigate the impacts of specific soft target distributions by instantiating the SoftRec with a series of strategies, including item-based, user-based, and model-based. To verify the effectiveness of SoftRec, we conduct extensive experiments on two public recommendation datasets by using various deep recommendation architectures. The experimental results show that our methods achieve superior performance compared with the standard optimization approaches. Moreover, SoftRec could also exhibit strong performance in cold-start scenarios where user-item interaction has higher sparsity.
C1  - New York, NY, USA
C3  - Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval
DA  - 2021///
PY  - 2021
DO  - 10.1145/3404835.3462863
SP  - 575
EP  - 584
PB  - Association for Computing Machinery
SN  - 978-1-4503-8037-9
UR  - https://doi.org/10.1145/3404835.3462863
KW  - implicit feedback
KW  - item recommendation
KW  - soft target
ER  - 

TY  - CONF
TI  - Net Benefit, Calibration, Threshold Selection, and Training Objectives for Algorithmic Fairness in Healthcare
AU  - Pfohl, Stephen
AU  - Xu, Yizhe
AU  - Foryciarz, Agata
AU  - Ignatiadis, Nikolaos
AU  - Genkins, Julian
AU  - Shah, Nigam
T3  - FAccT '22
AB  - A growing body of work uses the paradigm of algorithmic fairness to frame the development of techniques to anticipate and proactively mitigate the introduction or exacerbation of health inequities that may follow from the use of model-guided decision-making. We evaluate the interplay between measures of model performance, fairness, and the expected utility of decision-making to offer practical recommendations for the operationalization of algorithmic fairness principles for the development and evaluation of predictive models in healthcare. We conduct an empirical case-study via development of models to estimate the ten-year risk of atherosclerotic cardiovascular disease to inform statin initiation in accordance with clinical practice guidelines. We demonstrate that approaches that incorporate fairness considerations into the model training objective typically do not improve model performance or confer greater net benefit for any of the studied patient populations compared to the use of standard learning paradigms followed by threshold selection concordant with patient preferences, evidence of intervention effectiveness, and model calibration. These results hold when the measured outcomes are not subject to differential measurement error across patient populations and threshold selection is unconstrained, regardless of whether differences in model performance metrics, such as in true and false positive error rates, are present. In closing, we argue for focusing model development efforts on developing calibrated models that predict outcomes well for all patient populations while emphasizing that such efforts are complementary to transparent reporting, participatory design, and reasoning about the impact of model-informed interventions in context.
C1  - New York, NY, USA
C3  - 2022 ACM Conference on Fairness, Accountability, and Transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533166
SP  - 1039
EP  - 1052
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533166
KW  - fairness
KW  - healthcare
KW  - cardiovascular disease
ER  - 

TY  - CONF
TI  - Scalable Edge-Based Hyperdimensional Learning System with Brain-like Neural Adaptation
AU  - Zou, Zhuowen
AU  - Kim, Yeseong
AU  - Imani, Farhad
AU  - Alimohamadi, Haleh
AU  - Cammarota, Rosario
AU  - Imani, Mohsen
T3  - SC '21
AB  - In the Internet of Things (IoT) domain, many applications are running machine learning algorithms to assimilate the data collected in the swarm of devices. Sending all data to the powerful computing environment, e.g., cloud, poses significant efficiency and scalability issues. A promising way is to distribute the learning tasks onto the IoT hierarchy, often referred to edge computing; however, the existing sophisticated algorithms such as deep learning are often overcomplex to run on less-powerful and unreliable embedded IoT devices. Hyperdimensional Computing (HDC) is a brain-inspired learning approach for efficient and robust learning on today's embedded devices. Encoding, or transforming the input data into high-dimensional representation, is the key first step of HDC before performing a learning task. All existing HDC approaches use a static encoder; thus, they still require very high dimensionality, resulting in significant efficiency loss for the edge devices with limited resources. In this paper, we have developed NeuralHD, a new HDC approach with a dynamic encoder for adaptive learning. Inspired by human neural regeneration study in neuroscience, NeuralHD identifies insignificant dimensions and regenerates those dimensions to enhance the learning capability and robustness. We also present a scalable learning framework to distribute NeuralHD computation over edge devices in IoT systems. Our solution enables edge devices capable of real-time learning from both labeled and unlabeled data. Our evaluation on a wide range of practical classification tasks shows that NeuralHD provides 5.7X and 6.1X (12.3X and 14.1X) faster and more energy-efficient training compared to the HD-based algorithms (DNNs) running on the same platform. NeuralHD also provides 4.2X and 11.6X higher robustness to noise in the unreliable network and hardware of IoT environments as compared to DNNs.
C1  - New York, NY, USA
C3  - Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis
DA  - 2021///
PY  - 2021
DO  - 10.1145/3458817.3480958
PB  - Association for Computing Machinery
SN  - 978-1-4503-8442-1
UR  - https://doi.org/10.1145/3458817.3480958
ER  - 

TY  - CONF
TI  - Evolutionary Computation for Feature Selection and Feature Construction
AU  - Xue, Bing
AU  - Zhang, Mengjie
T3  - GECCO '22
C1  - New York, NY, USA
C3  - Proceedings of the Genetic and Evolutionary Computation Conference Companion
DA  - 2022///
PY  - 2022
DO  - 10.1145/3520304.3533659
SP  - 1200
EP  - 1229
PB  - Association for Computing Machinery
SN  - 978-1-4503-9268-6
UR  - https://doi.org/10.1145/3520304.3533659
ER  - 

TY  - JOUR
TI  - Exploring Multiobjective Optimization for Multiview Clustering
AU  - Saha, Sriparna
AU  - Mitra, Sayantan
AU  - Kramer, Stefan
T2  - ACM Trans. Knowl. Discov. Data
AB  - We present a new multiview clustering approach based on multiobjective optimization. In contrast to existing clustering algorithms based on multiobjective optimization, it is generally applicable to data represented by two or more views and does not require specifying the number of clusters a priori. The approach builds upon the search capability of a multiobjective simulated annealing based technique, AMOSA, as the underlying optimization technique. In the first version of the proposed approach, an internal cluster validity index is used to assess the quality of different partitionings obtained using different views. A new way of checking the compatibility of these different partitionings is also proposed and this is used as another objective function. A new encoding strategy and some new mutation operators are introduced. Finally, a new way of computing a consensus partitioning from multiple individual partitions obtained on multiple views is proposed. As a baseline and for comparison, two multiobjective based ensemble clustering techniques are proposed to combine the outputs of different simple clustering approaches. The efficacy of the proposed clustering methods is shown for partitioning several real-world datasets having multiple views. To show the practical usefulness of the method, we present results on web-search result clustering, where the task is to find a suitable partitioning of web snippets.
DA  - 2018/05//
PY  - 2018
DO  - 10.1145/3182181
VL  - 12
IS  - 4
SN  - 1556-4681
UR  - https://doi.org/10.1145/3182181
KW  - multiobjective optimization
KW  - Multiview classification
KW  - search result clustering
KW  - simulated annealing
ER  - 

TY  - BOOK
TI  - CKD '11: Proceedings of the 2011 Workshop on Climate Knowledge Discovery
AB  - Numerical simulation based science follows a new paradigm: its knowledge discovery process rests upon massive amounts of data. Climate scientists generate data faster than can be interpreted and need to prepare for further exponential data increases. Current analysis approaches are primarily focused on traditional methods, best suited for large-scale phenomena and coarseresolution data sets. Tools that employ a combination of high-performance analytics, with algorithms motivated by network science, nonlinear dynamics and statistics, as well as data mining and machine learning, could provide unique insights into challenging features of the Earth system, including extreme events and chaotic regimes. The breakthroughs needed to address these challenges will come from collaborative efforts involving several disciplines, including end-user scientists, computer and computational scientists, computing engineers, and mathematicians.The SC11 Climate Knowledge Discovery workshop at Supercomputing '11 brought together experts from various domains to investigate the use and application of large-scale graph analytics, semantic technologies and knowledge discovery algorithms in climate science. The workshop was the second in a series of planned workshops, the first workshop being held in Hamburg in March 2011.The goal of the workshop was to provide more than a broad overview of the topic, but establish a platform for the ongoing discussions in this field. It was designed to be highly interactive to concentrate on the challenges facing the climate community: "What steps are required to realize the potential of data analytics, semantic technologies and knowledge discovery algorithms in climate science? Where methods and technologies already exist, how can they be leveraged? Where gaps are identified, what steps must be taken to address them?"
CY  - New York, NY, USA
DA  - 2011///
PY  - 2011
PB  - Association for Computing Machinery
SN  - 978-1-4503-1181-6
ER  - 

TY  - CONF
TI  - Probabilistic Graphical Models for Climate Data Analysis
AU  - Banerjee, Arindam
T3  - CKD '11
AB  - The prominence and usage of probabilistic graphical models for data analysis have increased substantially over the past decade. Unlike traditional models in statistical machine learning, graphical models capture statistical dependencies between variables making them suitable for many problems. In this talk, I will discuss two applications of graphical models to climate data analysis problems, including progress and open questions. The first application is on abrupt change detection, with emphasis on detecting significant droughts in the past century. The second application is on predictive modeling of land variables based on ocean variables. Building on the second application, a framework for constructing statistical climate networks will be presented. Key challenges from both the computational and climate science perspective in realizing the potential of these methods will also be discussed.
C1  - New York, NY, USA
C3  - Proceedings of the 2011 Workshop on Climate Knowledge Discovery
DA  - 2011///
PY  - 2011
DO  - 10.1145/2110230.2110235
SP  - 3
PB  - Association for Computing Machinery
SN  - 978-1-4503-1181-6
UR  - https://doi.org/10.1145/2110230.2110235
KW  - climate data
ER  - 

TY  - CONF
TI  - A Mobile Expert System Using Fuzzy Logic for Diagnosing Kidney Diseases
AU  - Rivera, Ricardo F.
AU  - Pagaduan, Roxanne A.
AU  - Caliwag, Jasmin A.
AU  - Reyes, Felizardo C.
AU  - Castillo, Reynaldo E.
T3  - ICISS '19
AB  - This paper presents the work of developing a mobile expert system to provide information about the different kinds of kidney diseases. The mobile application developed is capable of diagnosing the user based on their health condition. The source of data used to build the expert system was collected from various related literature studies and interviews with experts in kidney diseases or Nephrology.The information provided by the application will help users in identifying early signs and symptoms of kidney diseases and will also improve their knowledge regarding these diseases. Detection of kidney diseases at a primary stage is essential to increase people's survival rate. The Decision Tree and Fuzzy logic algorithm were utilized to filter different symptoms and provide an accurate result of the diagnostic testing. The mobile application was evaluated by Nephrologist, Patients, and Mobile developers through its functionality, usability, and reliability using the ISO 9126 Software Quality Characteristics standard. The research evaluation results showed that the mobile application was able to achieve the defined objectives from the outcome of the survey given to the experts and other evaluators.
C1  - New York, NY, USA
C3  - Proceedings of the 2nd International Conference on Information Science and Systems
DA  - 2019///
PY  - 2019
DO  - 10.1145/3322645.3322703
SP  - 161
EP  - 165
PB  - Association for Computing Machinery
SN  - 978-1-4503-6103-3
UR  - https://doi.org/10.1145/3322645.3322703
KW  - Decision Tree
KW  - Fuzzy Logic
KW  - Expert System
KW  - Mobile Computing
ER  - 

TY  - JOUR
TI  - Enhanced Knowledge-Leverage-Based TSK Fuzzy System Modeling for Inductive Transfer Learning
AU  - Deng, Zhaohong
AU  - Jiang, Yizhang
AU  - Ishibuchi, Hisao
AU  - Choi, Kup-Sze
AU  - Wang, Shitong
T2  - ACM Trans. Intell. Syst. Technol.
AB  - The knowledge-leverage-based Takagi–Sugeno–Kang fuzzy system (KL-TSK-FS) modeling method has shown promising performance for fuzzy modeling tasks where transfer learning is required. However, the knowledge-leverage mechanism of the KL-TSK-FS can be further improved. This is because available training data in the target domain are not utilized for the learning of antecedents and the knowledge transfer mechanism from a source domain to the target domain is still too simple for the learning of consequents when a Takagi–Sugeno–Kang fuzzy system (TSK-FS) model is trained in the target domain. The proposed method, that is, the enhanced KL-TSK-FS (EKL-TSK-FS), has two knowledge-leverage strategies for enhancing the parameter learning of the TSK-FS model for the target domain using available information from the source domain. One strategy is used for the learning of antecedent parameters, while the other is for consequent parameters. It is demonstrated that the proposed EKL-TSK-FS has higher transfer learning abilities than the KL-TSK-FS. In addition, the EKL-TSK-FS has been further extended for the scene of the multisource domain.
DA  - 2016/07//
PY  - 2016
DO  - 10.1145/2903725
VL  - 8
IS  - 1
SN  - 2157-6904
UR  - https://doi.org/10.1145/2903725
KW  - missing data
KW  - transfer learning
KW  - Enhanced KL-TSK-FS
KW  - fuzzy modeling
KW  - fuzzy systems
KW  - knowledge leverage
ER  - 

TY  - CONF
TI  - Repeated Knowledge Distillation with Confidence Masking to Mitigate Membership Inference Attacks
AU  - Mazzone, Federico
AU  - van den Heuvel, Leander
AU  - Huber, Maximilian
AU  - Verdecchia, Cristian
AU  - Everts, Maarten
AU  - Hahn, Florian
AU  - Peter, Andreas
T3  - AISec'22
AB  - Machine learning models are often trained on sensitive data, such as medical records or bank transactions, posing high privacy risks. In fact, membership inference attacks can use the model parameters or predictions to determine whether a given data point was part of the training set. One of the most promising mitigations in literature is Knowledge Distillation (KD). This mitigation consists of first training a teacher model on the sensitive private dataset, and then transferring the teacher knowledge to a student model, by the mean of a surrogate dataset. The student model is then deployed in place of the teacher model. Unfortunately, KD on its own does not provide users much flexibility, meant as the possibility to arbitrarily decide how much utility to sacrifice to get membership-privacy. To address this problem, we propose a novel approach that combines KD with confidence score masking. Concretely, we repeat the distillation procedure multiple times in series and, during each distillation, perturb the teacher predictions using confidence masking techniques. We show that our solution provides more flexibility than standard KD, as it allows users to tune the number of distillation rounds and the strength of the masking function. We implement our approach in a tool, RepKD, and assess our mitigation against white- and black-box attacks on multiple models and datasets. Even when the surrogate dataset is different from the private one (which we believe to be a more realistic setting than is commonly found in literature), our mitigation is able to make the black-box attack completely ineffective and significantly reduce the accuracy of the white-box attack at the cost of only 0.6% test accuracy loss.
C1  - New York, NY, USA
C3  - Proceedings of the 15th ACM Workshop on Artificial Intelligence and Security
DA  - 2022///
PY  - 2022
DO  - 10.1145/3560830.3563721
SP  - 13
EP  - 24
PB  - Association for Computing Machinery
SN  - 978-1-4503-9880-0
UR  - https://doi.org/10.1145/3560830.3563721
KW  - knowledge distillation
KW  - membership inference attack
KW  - confidence score masking
KW  - defense
KW  - mitigation
ER  - 

TY  - JOUR
TI  - Real-Time, All-Frequency Shadows in Dynamic Scenes
AU  - Annen, Thomas
AU  - Dong, Zhao
AU  - Mertens, Tom
AU  - Bekaert, Philippe
AU  - Seidel, Hans-Peter
AU  - Kautz, Jan
T2  - ACM Trans. Graph.
AB  - Shadow computation in dynamic scenes under complex illumination is a challenging problem. Methods based on precomputation provide accurate, real-time solutions, but are hard to extend to dynamic scenes. Specialized approaches for soft shadows can deal with dynamic objects but are not fast enough to handle more than one light source. In this paper, we present a technique for rendering dynamic objects under arbitrary environment illumination, which does not require any precomputation. The key ingredient is a fast, approximate technique for computing soft shadows, which achieves several hundred frames per second for a single light source. This allows for approximating environment illumination with a sparse collection of area light sources and yields real-time frame rates.
DA  - 2008/08//
PY  - 2008
DO  - 10.1145/1360612.1360633
VL  - 27
IS  - 3
SP  - 1
EP  - 8
SN  - 0730-0301
UR  - https://doi.org/10.1145/1360612.1360633
KW  - convolution
KW  - environment maps
KW  - soft shadows
ER  - 

TY  - JOUR
TI  - A Hidden Absorbing Semi-Markov Model for Informatively Censored Temporal Data: Learning and Inference
AU  - Alaa, Ahmed M.
AU  - Van Der Schaar, Mihaela
T2  - J. Mach. Learn. Res.
AB  - Modeling continuous-time physiological processes that manifest a patient's evolving clinical states is a key step in approaching many problems in healthcare. In this paper, we develop the Hidden Absorbing Semi-Markov Model (HASMM): a versatile probabilistic model that is capable of capturing the modern electronic health record (EHR) data. Unlike existing models, the HASMM accommodates irregularly sampled, temporally correlated, and informatively censored physiological data, and can describe non-stationary clinical state transitions. Learning the HASMM parameters from the EHR data is achieved via a novel forward-filtering backward-sampling Monte-Carlo EM algorithm that exploits the knowledge of the end-point clinical outcomes (informative censoring) in the EHR data, and implements the E-step by sequentially sampling the patients' clinical states in the reverse-time direction while conditioning on the future states. Real-time inferences are drawn via a forward-filtering algorithm that operates on a virtually constructed discrete-time embedded Markov chain that mirrors the patient's continuous-time state trajectory. We demonstrate the prognostic utility of the HASMM in a critical care prognosis setting using a real-world dataset for patients admitted to the Ronald Reagan UCLA Medical Center. In particular, we show that using HASMMs, a patient's clinical deterioration can be predicted 8-9 hours prior to intensive care unit admission, with a 22% AUC gain compared to the Rothman index, which is the state-of-the-art critical care risk scoring technology.
DA  - 2018/01//
PY  - 2018
VL  - 19
IS  - 1
SP  - 108
EP  - 169
SN  - 1532-4435
KW  - medical informatics
KW  - hidden semi-markov models
KW  - Monte Carlo methods
ER  - 

TY  - CONF
TI  - The Transformational Capabilities Offered by Ontologies in Climate Science
AU  - Kleese van Dam, Kerstin
AU  - Stephan, Eric G.
T3  - CKD '11
AB  - Climate science has long discovered the benefits of standardization of formats to support discovery, analysis and comparison of climate results. Having started at a time when hierarchical metadata models where state of the art, much of their standards, tools and infrastructures relies today on these type of systems. These metadata based systems are highly sophisticated and keep excellent pace with the increasing volumes of data. However data has not only grown in size, but also in complexity. An increasing diversity of methods and devices for data generation and capture, as well as the often multi-scale, multi-physics and trans-disciplinary nature of many science challenges, has led to the need to work with very heterogeneous data. In this environment metadata based systems do have one distinct downside, they are not easily expandable, and while extensions are possible to incorporate additional descriptions from the same domain, the same cannot be said for integrations across method and domain boundaries. Ontologies can here deliver invaluable, flexible help to provide bridges (crosswalks) between different formats, domains and knowledge representations. Offering the ability to build these bridges step by step as the understanding grows between the two communities, they deliver a very practical approach to delivering early solutions. Ontologies combined with rules and provenance information can furthermore be used to drive automated analysis and transformation, between different objects. Finally ontologies allow the scientists to capture, highlight, analyze and share their insights about correlations within large data collections. This talk will present a number of illustrating use cases and solution approaches.
C1  - New York, NY, USA
C3  - Proceedings of the 2011 Workshop on Climate Knowledge Discovery
DA  - 2011///
PY  - 2011
DO  - 10.1145/2110230.2110238
SP  - 5
PB  - Association for Computing Machinery
SN  - 978-1-4503-1181-6
UR  - https://doi.org/10.1145/2110230.2110238
KW  - knowledge discovery
KW  - data analysis
KW  - climate research
KW  - ontologies
KW  - semantic technologies
ER  - 

TY  - CONF
TI  - Evolutionary Computation for Feature Selection and Feature Construction
AU  - Xue, Bing
AU  - Zhang, Mengjie
T3  - GECCO '21
C1  - New York, NY, USA
C3  - Proceedings of the Genetic and Evolutionary Computation Conference Companion
DA  - 2021///
PY  - 2021
DO  - 10.1145/3449726.3461415
SP  - 1141
EP  - 1168
PB  - Association for Computing Machinery
SN  - 978-1-4503-8351-6
UR  - https://doi.org/10.1145/3449726.3461415
ER  - 

TY  - CONF
TI  - Ignorance is Almost Bliss: Near-Optimal Stochastic Matching With Few Queries
AU  - Blum, Avrim
AU  - Dickerson, John P.
AU  - Haghtalab, Nika
AU  - Procaccia, Ariel D.
AU  - Sandholm, Tuomas
AU  - Sharma, Ankit
T3  - EC '15
AB  - The stochastic matching problem deals with finding a maximum matching in a graph whose edges are unknown but can be accessed via queries. This is a special case of stochastic k-set packing, where the problem is to find a maximum packing of sets, each of which exists with some probability. In this paper, we provide edge and set query algorithms for these two problems, respectively, that provably achieve some fraction of the omniscient optimal solution. Our main theoretical result for the stochastic matching (i.e., 2-set packing) problem is the design of an adaptive algorithm that queries only a constant number of edges per vertex and achieves a (1-ε) fraction of the omniscient optimal solution, for an arbitrarily small ε &gt; 0. Moreover, this adaptive algorithm performs the queries in only a constant number of rounds. We complement this result with a non-adaptive (i.e., one round of queries) algorithm that achieves a (0.5 - ε) fraction of the omniscient optimum. We also extend both our results to stochastic k-set packing by designing an adaptive algorithm that achieves a (2/k - ε) fraction of the omniscient optimal solution, again with only O(1) queries per element. This guarantee is close to the best known polynomial-time approximation ratio of 3/k+1 -ε for the deterministic k-set packing problem [Furer 2013].We empirically explore the application of (adaptations of) these algorithms to the kidney exchange problem, where patients with end-stage renal failure swap willing but incompatible donors. We show on both generated data and on real data from the first 169 match runs of the UNOS nationwide kidney exchange that even a very small number of non-adaptive edge queries per vertex results in large gains in expected successful matches.
C1  - New York, NY, USA
C3  - Proceedings of the Sixteenth ACM Conference on Economics and Computation
DA  - 2015///
PY  - 2015
DO  - 10.1145/2764468.2764479
SP  - 325
EP  - 342
PB  - Association for Computing Machinery
SN  - 978-1-4503-3410-5
UR  - https://doi.org/10.1145/2764468.2764479
KW  - algorithms
KW  - economics
KW  - experimentation
KW  - kidney exchange
KW  - stochastic k-set packing
KW  - stochastic matching
ER  - 

TY  - CONF
TI  - Evolutionary Computation for Feature Selection and Feature Construction
AU  - Xue, Bing
AU  - Zhang, Mengjie
T3  - GECCO '20
C1  - New York, NY, USA
C3  - Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion
DA  - 2020///
PY  - 2020
DO  - 10.1145/3377929.3389857
SP  - 1283
EP  - 1312
PB  - Association for Computing Machinery
SN  - 978-1-4503-7127-8
UR  - https://doi.org/10.1145/3377929.3389857
ER  - 

TY  - JOUR
TI  - The Man Who Had Them All
AU  - Kartoun, Uri
T2  - Interactions
DA  - 2017/06//
PY  - 2017
DO  - 10.1145/3096966
VL  - 24
IS  - 4
SP  - 22
EP  - 23
SN  - 1072-5520
UR  - https://doi.org/10.1145/3096966
ER  - 

TY  - CONF
TI  - A Generic Knowledge Based Medical Diagnosis Expert System
AU  - Huang, Xin
AU  - Tang, Xuejiao
AU  - Zhang, Wenbin
AU  - Zhang, Ji
AU  - Zhang, Mingli
AU  - Gan, Wensheng
AU  - Pei, Shichao
AU  - Liu, Zhen
AU  - Huang, Yiyi
T3  - iiWAS2021
AB  - In this paper, we design and implement a generic medical knowledge based system (MKBS) for identifying diseases from several symptoms. In this system, some important aspects like knowledge bases system, knowledge representation, inference engine have been addressed. The system asks users different questions and inference engines will use the certainty factor to prune out low possible solutions. The proposed disease diagnosis system also uses a graphical user interface (GUI) to facilitate users to interact with the expert system. Our expert system is generic and flexible, which can be integrated with any rule bases system in disease diagnosis.
C1  - New York, NY, USA
C3  - The 23rd International Conference on Information Integration and Web Intelligence
DA  - 2022///
PY  - 2022
DO  - 10.1145/3487664.3487728
SP  - 462
EP  - 466
PB  - Association for Computing Machinery
SN  - 978-1-4503-9556-4
UR  - https://doi.org/10.1145/3487664.3487728
ER  - 

TY  - CONF
TI  - Data-Driven Semantic Analysis for Multilingual WSD and Lexical Selection in Translation
AU  - Apidianaki, Marianna
T3  - EACL '09
AB  - A common way of describing the senses of ambiguous words in multilingual Word Sense Disambiguation (WSD) is by reference to their translation equivalents in another language. The theoretical soundness of the senses induced in this way can, however, be doubted. This type of cross-lingual sense identification has implications for multilingual WSD and MT evaluation as well. In this article, we first present some arguments in favour of a more thorough analysis of the semantic information that may be induced by the equivalents of ambiguous words found in parallel corpora. Then, we present an unsupervised WSD method and a lexical selection method that exploit the results of a data-driven sense induction method. Finally, we show how this automatically acquired information can be exploited for a multilingual WSD and MT evaluation more sensitive to lexical semantics.
C1  - USA
C3  - Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics
DA  - 2009///
PY  - 2009
SP  - 77
EP  - 85
PB  - Association for Computational Linguistics
ER  - 

TY  - CONF
TI  - Application of Correlate Summation to Data Clustering in the Estrogen- and Salt-Sensitive Female MRen2.Lewis Rat
AU  - Westwood, Brian
AU  - Chappell, Mark
T3  - TMBIO '06
AB  - Principal Component Analysis (PCA) can identify key variables among complex data sets, but requires special statistical packages. Alternatively, Discrete and Aggregate Correlate Summation (DCΣ/ACΣ) identifies the most covariant variables relative to mean clustering for grouped and individual data, respectively. We compared these analyses regarding the influence of estrogen and salt on the hypertensive phenotype of the female mRen2.Lewis strain. DCΣx compares changes in correlation between two groups for each variable versus all of the others, relative to mean shift. ACΣx determines which variable has the highest total correlation to the other variables, relative to its mean reduced (normalized) standard deviation (nSD). To compare correlate summation to PCA, the absolute weights of the first principal component (EVEC1x were multiplied by the nSD as in ACΣx. Nine variables including proteinuria, serum ACE, plasma Ang II, renin, heart weight to body weight ratio and systolic blood pressure were analyzed with respect to normal and high salt diets, as well as to estrogen intact and depleted conditions. DCΣx results for both arms of the study were significantly correlated with EVEC1x (r=0.72, p&lt;0.05). Combination of both DCΣx results (DCΣx-cum increased correlation to EVEC1x (r=0.84, p&lt;0.005); however, ACΣx and EVEC1x exhibited the highest correlation (r=0.99, p&lt;0.0001). The first principal component correlated to the key DCΣx-cum and ACΣx variable urinary protein (r=0.85, p&lt;0.0001, n=28). Correlate summation analysis (CSA) provides an alternative method to yield important variables and data clustering which appear to be comparable to PCA. Parameter correlations were further investigated using power relationships, revealing two levels of control for proteinuria vs. both Ang II and HW/BW. In the female mRen2.Lewis, CSA revealed proteinuria, ACE and renin activities as key markers in response to salt loading and estrogen depletion. Moreover, this approach may indicate an underlying mechanism affecting two variables.
C1  - New York, NY, USA
C3  - Proceedings of the 1st International Workshop on Text Mining in Bioinformatics
DA  - 2006///
PY  - 2006
DO  - 10.1145/1183535.1183542
SP  - 21
EP  - 26
PB  - Association for Computing Machinery
SN  - 1-59593-526-6
UR  - https://doi.org/10.1145/1183535.1183542
KW  - estrogen
KW  - correlation
KW  - salt
ER  - 

TY  - JOUR
TI  - Reinforcement Learning for Closed-Loop Propofol Anesthesia: A Study in Human Volunteers
AU  - Moore, Brett L.
AU  - Pyeatt, Larry D.
AU  - Kulkarni, Vivekanand
AU  - Panousis, Periklis
AU  - Padrez, Kevin
AU  - Doufas, Anthony G.
T2  - J. Mach. Learn. Res.
AB  - Clinical research has demonstrated the efficacy of closed-loop control of anesthesia using the bispectral index of the electroencephalogram as the controlled variable. These controllers have evolved to yield patient-specific anesthesia, which is associated with improved patient outcomes. Despite progress, the problem of patient-specific anesthesia remains unsolved. A variety of factors confound good control, including variations in human physiology, imperfect measures of drug effect, and delayed, hysteretic response to drug delivery. Reinforcement learning (RL) appears to be uniquely equipped to overcome these challenges; however, the literature offers no precedent for RL in anesthesia. To begin exploring the role RL might play in improving anesthetic care, we investigated the method's application in the delivery of patient-specific, propofol-induced hypnosis in human volunteers. When compared to performance metrics reported in the anesthesia literature, RL demonstrated patient-specific control marked by improved accuracy and stability. Furthermore, these results suggest that RL may be considered a viable alternative for solving other difficult closed-loop control problems in medicine. More rigorous clinical study, beyond the confines of controlled human volunteer studies, is needed to substantiate these findings.
DA  - 2014/01//
PY  - 2014
VL  - 15
IS  - 1
SP  - 655
EP  - 696
SN  - 1532-4435
KW  - anesthesia
KW  - bispectral index
KW  - closed-loop control
KW  - hypnosis
KW  - propofol
KW  - reinforcement learning
ER  - 

TY  - CONF
TI  - PACE: Learning Effective Task Decomposition for Human-in-the-Loop Healthcare Delivery
AU  - Zheng, Kaiping
AU  - Chen, Gang
AU  - Herschel, Melanie
AU  - Ngiam, Kee Yuan
AU  - Ooi, Beng Chin
AU  - Gao, Jinyang
T3  - SIGMOD '21
AB  - Human-in-the-loop data analysis involves both machine learning models and humans in analytic tasks. In healthcare applications, human-in-the-loop data analysis is crucial in that the model can handle "easy" tasks and hand over "hard" ones to medical experts for assistance and medical judgment, where easy tasks are the ones for which the model can provide high accuracy and hard tasks vice versa. In this process, how to decompose tasks in an effective manner is an important stage. To achieve task decomposition, classification with a reject option is a solution. However, existing studies either directly implement a reject option or dive into the theoretical details of the rejection mechanism. Different from such studies, we aim to optimize general classifiers with a reject option and hence, optimize task decomposition for healthcare applications.To this end, we first introduce task decomposition for healthcare applications, which is a crucial stage in human-in-the-loop healthcare delivery. We then devise a framework PACE to learn effective task decomposition concentrating on delivering high performance on the easy tasks. PACE is two-level: on the macro level, PACE employs the Self-Paced Learning method to select easy tasks for each training iteration; on the micro level, PACE adapts the weights of selected tasks through its weighted loss revision strategy. Experimental results in two real-world healthcare datasets show that PACE outperforms baselines in terms of their performance on the easy tasks which are expected to be solved by the learning model.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 International Conference on Management of Data
DA  - 2021///
PY  - 2021
DO  - 10.1145/3448016.3457281
SP  - 2156
EP  - 2168
PB  - Association for Computing Machinery
SN  - 978-1-4503-8343-1
UR  - https://doi.org/10.1145/3448016.3457281
KW  - healthcare
KW  - human-in-the-loop
KW  - task decomposition
ER  - 

TY  - CONF
TI  - Real-Time, All-Frequency Shadows in Dynamic Scenes
AU  - Annen, Thomas
AU  - Dong, Zhao
AU  - Mertens, Tom
AU  - Bekaert, Philippe
AU  - Seidel, Hans-Peter
AU  - Kautz, Jan
T3  - SIGGRAPH '08
AB  - Shadow computation in dynamic scenes under complex illumination is a challenging problem. Methods based on precomputation provide accurate, real-time solutions, but are hard to extend to dynamic scenes. Specialized approaches for soft shadows can deal with dynamic objects but are not fast enough to handle more than one light source. In this paper, we present a technique for rendering dynamic objects under arbitrary environment illumination, which does not require any precomputation. The key ingredient is a fast, approximate technique for computing soft shadows, which achieves several hundred frames per second for a single light source. This allows for approximating environment illumination with a sparse collection of area light sources and yields real-time frame rates.
C1  - New York, NY, USA
C3  - ACM SIGGRAPH 2008 Papers
DA  - 2008///
PY  - 2008
DO  - 10.1145/1399504.1360633
PB  - Association for Computing Machinery
SN  - 978-1-4503-0112-1
UR  - https://doi.org/10.1145/1399504.1360633
KW  - convolution
KW  - environment maps
KW  - soft shadows
ER  - 

TY  - CONF
TI  - Cardiovascular Risk Prediction Models: A Scoping Review
AU  - Sajeev, Shelda
AU  - Maeder, Anthony
T3  - ACSW 2019
AB  - Background: The prevention of cardiovascular disease is a public health priority as it is associated with increasing morbidity and mortality worldwide.Objective: A scoping review of the existing cardiovascular risk prediction models, to provide a basis for suggesting future research directions.Methods: PubMed and Scopus were searched from 2008 to 2018 for review papers investigating the formulation and effectiveness of risk prediction models for cardiovascular disease.Results: 229 references were screened of which 4 articles were included in the review, describing development of 436 prediction models. Most of the work reported was from USA and Europe.Conclusions: Availability of larger datasets from Electronic Health Records for more comprehensive and targeted risk prediction, and advancement in data analysis and modeling methods like machine learning to enable cohort directed approaches, has prompted researchers and clinicians to rethink risk modeling.
C1  - New York, NY, USA
C3  - Proceedings of the Australasian Computer Science Week Multiconference
DA  - 2019///
PY  - 2019
DO  - 10.1145/3290688.3290725
PB  - Association for Computing Machinery
SN  - 978-1-4503-6603-8
UR  - https://doi.org/10.1145/3290688.3290725
KW  - data analysis
KW  - cardiovascular disease
KW  - modeling
KW  - risk factors
ER  - 

TY  - CONF
TI  - Interprocedural Optimization: Eliminating Unnecessary Recompilation
AU  - Cooper, Keith D.
AU  - Kennedy, Ken
AU  - Torczon, Linda
T3  - SIGPLAN '86
AB  - While efficient new algorithms for interprocedural data flow analysis have made these techniques practical for use in production compilation systems, a new problem has arisen: collecting and using interprocedural information in a compiler introduces subtle dependences among the procedures of a program. If the compiler depends on interprocedural information to optimize a given module, a subsequent editing change to another module in the program may change the interprocedural information and necessitate recompilation. To avoid having to recompile every module in a program in response to a single editing change to one module, we must develop techniques to more precisely determine which compilations have actually been invalidated by a change to the program's source. This paper presents a general recompilation test to determine which procedures must be compiled in response to a series of editing changes. Three different implementation strategies, which demonstrate the fundamental tradeoff between the cost of analysis and the precision of the resulting test, are also discussed.
C1  - New York, NY, USA
C3  - Proceedings of the 1986 SIGPLAN Symposium on Compiler Construction
DA  - 1986///
PY  - 1986
DO  - 10.1145/12276.13317
SP  - 58
EP  - 67
PB  - Association for Computing Machinery
SN  - 0-89791-197-0
UR  - https://doi.org/10.1145/12276.13317
ER  - 

TY  - JOUR
TI  - Interprocedural Optimization: Eliminating Unnecessary Recompilation
AU  - Cooper, Keith D.
AU  - Kennedy, Ken
AU  - Torczon, Linda
T2  - SIGPLAN Not.
AB  - While efficient new algorithms for interprocedural data flow analysis have made these techniques practical for use in production compilation systems, a new problem has arisen: collecting and using interprocedural information in a compiler introduces subtle dependences among the procedures of a program. If the compiler depends on interprocedural information to optimize a given module, a subsequent editing change to another module in the program may change the interprocedural information and necessitate recompilation. To avoid having to recompile every module in a program in response to a single editing change to one module, we must develop techniques to more precisely determine which compilations have actually been invalidated by a change to the program's source. This paper presents a general recompilation test to determine which procedures must be compiled in response to a series of editing changes. Three different implementation strategies, which demonstrate the fundamental tradeoff between the cost of analysis and the precision of the resulting test, are also discussed.
DA  - 1986/07//
PY  - 1986
DO  - 10.1145/13310.13317
VL  - 21
IS  - 7
SP  - 58
EP  - 67
SN  - 0362-1340
UR  - https://doi.org/10.1145/13310.13317
ER  - 

TY  - CONF
TI  - Detecting Multiple Outliers in Regression Data Using Genetic Algorithms
AU  - Crawford, Kelly D.
AU  - Vasicek, Daniel J.
AU  - Wainwright, Roger L.
T3  - SAC '95
C1  - New York, NY, USA
C3  - Proceedings of the 1995 ACM Symposium on Applied Computing
DA  - 1995///
PY  - 1995
DO  - 10.1145/315891.316017
SP  - 351
EP  - 356
PB  - Association for Computing Machinery
SN  - 0-89791-658-1
UR  - https://doi.org/10.1145/315891.316017
KW  - regression
KW  - genetic algorithm
KW  - least squares
KW  - outhier
ER  - 

TY  - BOOK
TI  - WWW '19: The World Wide Web Conference
AB  - It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.
CY  - New York, NY, USA
DA  - 2019///
PY  - 2019
PB  - Association for Computing Machinery
SN  - 978-1-4503-6674-8
ER  - 

TY  - CONF
TI  - Probabilistic Character Motion Synthesis Using a Hierarchical Deep Latent Variable Model
AU  - Ghorbani, S.
AU  - Wloka, C.
AU  - Etemad, A.
AU  - Brubaker, M. A.
AU  - Troje, N. F.
T3  - SCA '20
AB  - We present a probabilistic framework to generate character animations based on weak control signals, such that the synthesized motions are realistic while retaining the stochastic nature of human movement. The proposed architecture, which is designed as a hierarchical recurrent model, maps each sub-sequence of motions into a stochastic latent code using a variational autoencoder extended over the temporal domain. We also propose an objective function which respects the impact of each joint on the pose and compares the joint angles based on angular distance. We use two novel quantitative protocols and human qualitative assessment to demonstrate the ability of our model to generate convincing and diverse periodic and non-periodic motion sequences without the need for strong control signals.
C1  - Goslar, DEU
C3  - Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation
DA  - 2020///
PY  - 2020
DO  - 10.1111/cgf.14116
PB  - Eurographics Association
UR  - https://doi.org/10.1111/cgf.14116
ER  - 

TY  - CONF
TI  - Modeling Memristor Radiation Interaction Events and the Effect on Neuromorphic Learning Circuits
AU  - Dahl, Sumedha Gandharava
AU  - Ivans, Robert
AU  - Cantley, Kurtis D.
T3  - ICONS '18
AB  - An ideal memristor model is modified to include the effects of radiation interactions with the device. Modeling is done in Cadence Virtuoso design suite using Verilog-A. Simulations include the effect of radiation events that could change the state of device or can ionize the device to create e-h+ pairs or change the off-state resistance of the device. Combination of these events occurring simultaneously is also studied. Simulation results are compared with the experimental results published in existing research papers. Finally, transient simulation of a three-input, two-output spiking electronic neural network with memristive synapses is performed. Varying amounts of energy deposited by radiation are modeled, and it is observed that radiation exposure dramatically alters the synaptic weight evolution.
C1  - New York, NY, USA
C3  - Proceedings of the International Conference on Neuromorphic Systems
DA  - 2018///
PY  - 2018
DO  - 10.1145/3229884.3229885
PB  - Association for Computing Machinery
SN  - 978-1-4503-6544-4
UR  - https://doi.org/10.1145/3229884.3229885
KW  - Memristor
KW  - neuromorphic
KW  - radiation
ER  - 

TY  - CONF
TI  - Argumentation with Goals for Clinical Decision Support in Multimorbidity
AU  - Oliveira, Tiago
AU  - Dauphin, Jérémie
AU  - Satoh, Ken
AU  - Tsumoto, Shusaku
AU  - Novais, Paulo
T3  - AAMAS '18
AB  - The present work proposes a computational argumentation system equipped with goal seeking to combine independently generated recommendations for handling multimorbidity.
C1  - Richland, SC
C3  - Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems
DA  - 2018///
PY  - 2018
SP  - 2031
EP  - 2033
PB  - International Foundation for Autonomous Agents and Multiagent Systems
KW  - clinical decision support
KW  - argumentation
KW  - conflict resolution
ER  - 

TY  - CONF
TI  - Visualization Analysis of Cardiovascular Risk Factors Based on Knowledge Mapping
AU  - Yan, Ling
AU  - Zhou, Zuojian
AU  - Hu, Yun
AU  - Song, Yihua
AU  - Zhou, Weihong
T3  - ICMHI '19
AB  - To provide guidance for the study of risk factors of cardiovascular disease in China, we analyze and compared the research hotspots and frontiers of cardiovascular risk factors in China and outside China in this paper. The CiteSpace software is used to visualize the research literature on cardiovascular risk factors from Cnki(China Knowledge Network Internet) and Web of Science core collection database from 2010-2018, and to draw the knowledge map of high-frequency words, research institutions and countries. In this project, a total of 318 Chinese literatures and 9,009 English literatures were screened out as research objects. The number of annual publications assumes an increasing tendency, most of which are concentrated in developed countries or regions. Study of cardiovascular risk factors outside China is significantly taken more seriously than in China. This study shows that the research frontier has already been extended to lifestyle, particulate air pollution, socioeconomic status, depression, stroke and breast cancer, etc. Moreover, children, adolescents and women have been more concerned in the target population. In consideration of the main reasons for cardiovascular disease—Hypertension, diabetes, coronary heart disease, obesity, atherosclerosis and metabolic syndrome, etc., we should pay close attention to the influence on the risk of cardiovascular disease produced by psychological state, living habits, regional differences, air pollution and so on. Meanwhile, we also need to take the screening and prevention of cardiovascular risk more seriously among adolescents.
C1  - New York, NY, USA
C3  - Proceedings of the 3rd International Conference on Medical and Health Informatics
DA  - 2019///
PY  - 2019
DO  - 10.1145/3340037.3340059
SP  - 26
EP  - 31
PB  - Association for Computing Machinery
SN  - 978-1-4503-7199-5
UR  - https://doi.org/10.1145/3340037.3340059
KW  - cardiovascular risk factors
KW  - knowledge map
KW  - visualization analysis
ER  - 

TY  - JOUR
TI  - PCID: A Novel Approach for Predicting Disease Comorbidity by Integrating Multi-Scale Data
AU  - He, Feng
AU  - Zhu, Guanghui
AU  - Wang, Yin-Ying
AU  - Zhao, Xing-Ming
AU  - Huang, De-Shuang
T2  - IEEE/ACM Trans. Comput. Biol. Bioinformatics
AB  - Disease comorbidity is the presence of one or more diseases along with a primary disorder, which causes additional pain to patients and leads to the failure of standard treatments compared with single diseases. Therefore, the identification of potential comorbidity can help prevent those comorbid diseases when treating a primary disease. Unfortunately, most of current known disease comorbidities are discovered occasionally in clinic, and our knowledge about comorbidity is far from complete. Despite the fact that many efforts have been made to predict disease comorbidity, the prediction accuracy of existing computational approaches needs to be improved. By investigating the factors underlying disease comorbidity, e.g., mutated genes and rewired protein-protein interactions PPIs, we here present a novel algorithm to predict disease comorbidity by integrating multi-scale data ranging from genes to phenotypes. Benchmark results on real data show that our approach outperforms existing algorithms, and some of our novel predictions are validated with those reported in literature, indicating the effectiveness and predictive power of our approach. In addition, we identify some pathway and PPI patterns that underlie the co-occurrence between a primary disease and certain disease classes, which can help explain how the comorbidity is initiated from molecular perspectives.
DA  - 2017/05//
PY  - 2017
DO  - 10.1109/TCBB.2016.2550443
VL  - 14
IS  - 3
SP  - 678
EP  - 686
SN  - 1545-5963
UR  - https://doi.org/10.1109/TCBB.2016.2550443
ER  - 

TY  - CONF
TI  - Wound and Episode Level Readmission Risk or Weeks to Readmit: Why Do Patients Get Readmitted? How Long Does It Take for a Patient to Get Readmitted?
AU  - Oota, Subba Reddy
AU  - Rahman, Nafisur
AU  - Mohammed, Shahid Saleem
AU  - Galitz, Jeffrey
AU  - Liu, Minghsun
T3  - CODS-COMAD '21
AB  - The Affordable care Act of 2010 had introduced the readmission reduction program in 2012 to reduce avoidable readmissions to control rising healthcare costs. Wound care impacts 15%&nbsp;[16] of Medicare beneficiaries, making it one of the major contributors of medicare health care cost. Health plans have been exploring proactive health care services that can prevent wound recurrences and readmissions from controlling wound care costs. With the rising costs of the Wound care industry, it has become of paramount importance to reduce wound recurrences &amp; patient readmissions. What factors are responsible for a Wound to recur, which ultimately leads to hospitalization or readmission? Is there a way to identify the patients at risk of readmission before the occurrence using data-driven analysis? Patient readmission risk management has become critical for patients suffering from chronic wounds such as diabetic ulcers, pressure ulcers, and vascular ulcers. Understanding the risk &amp; the factors that cause patient readmission can help care providers and patients avoid wound recurrences. Our work focuses on identifying patients who are at high risk of readmission and determining the time period within which a patient might get readmitted. Frequent readmissions add financial stress to the patient &amp; Health plan and deteriorate the patient’s quality of life. Having this information can allow a provider to set up preventive measures that can delay, if not prevent, patients’ readmission. On a combined wound &amp; episode-level dataset of patient’s wound care information, our extended autoprognosis achieves a recall of 0.92 and a precision of 0.92 for predicting a patient’s readmission risk. For new patient class, precision and recall are as high as 0.91 and 0.98, respectively. We can also predict the amount of time (in weeks) it might take after a patient’s discharge event for a readmission event to occur through our model with a mean absolute error of 2.3 weeks.
C1  - New York, NY, USA
C3  - Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)
DA  - 2021///
PY  - 2021
DO  - 10.1145/3430984.3431005
SP  - 359
EP  - 365
PB  - Association for Computing Machinery
SN  - 978-1-4503-8817-7
UR  - https://doi.org/10.1145/3430984.3431005
KW  - Machine Learning
KW  - AutoPrognosis
KW  - Chronic Wound management
KW  - Cost Control
KW  - Health care
KW  - Patient’s readmission risk
KW  - Readmission prevention
KW  - Wound care
ER  - 

TY  - CONF
TI  - Kernel Slicing: Scalable Online Training with Conjunctive Features
AU  - Yoshinaga, Naoki
AU  - Kitsuregawa, Masaru
T3  - COLING '10
AB  - This paper proposes an efficient online method that trains a classifier with many conjunctive features. We employ kernel computation called kernel slicing, which explicitly considers conjunctions among frequent features in computing the polynomial kernel, to combine the merits of linear and kernel-based training. To improve the scalability of this training, we reuse the temporal margins of partial feature vectors and terminate unnecessary margin computations. Experiments on dependency parsing and hyponymy-relation extraction demonstrated that our method could train a classifier orders of magnitude faster than kernel-based online learning, while retaining its space efficiency.
C1  - USA
C3  - Proceedings of the 23rd International Conference on Computational Linguistics
DA  - 2010///
PY  - 2010
SP  - 1245
EP  - 1253
PB  - Association for Computational Linguistics
ER  - 

TY  - CONF
TI  - Collaborative Knowledge Distillation for Heterogeneous Information Network Embedding
AU  - Wang, Can
AU  - Zhou, Sheng
AU  - Yu, Kang
AU  - Chen, Defang
AU  - Li, Bolang
AU  - Feng, Yan
AU  - Chen, Chun
T3  - WWW '22
AB  - Learning low-dimensional representations for Heterogeneous Information Networks (HINs) has drawn increasing attention recently for its effectiveness in real-world applications. Compared with homogeneous networks, HINs are characterized by meta-paths connecting different types of nodes with semantic meanings. Existing methods mainly follow the prototype of independently learning meta-path-based embeddings and integrating them into a unified embedding. However, meta-paths in a HIN are inherently correlated since they reflect different perspectives of the same object. If each meta-path is treated as an isolated semantic data resource and the correlations among them are disregarded, sub-optimality in the both the meta-path based embedding and final embedding will be resulted. To address this issue, we make the first attempt to explicitly model the correlation among meta-paths by proposing Collaborative Knowledge Distillation for Heterogeneous Information Network Embedding (CKD). More specifically, we model the knowledge in each meta-path with two different granularities: regional knowledge and global knowledge. We learn the meta-path-based embeddings by collaboratively distill the knowledge from intra-meta-path and inter-meta-path simultaneously. Experiments conducted on six real-world HIN datasets demonstrates the effectiveness of the CKD method.
C1  - New York, NY, USA
C3  - Proceedings of the ACM Web Conference 2022
DA  - 2022///
PY  - 2022
DO  - 10.1145/3485447.3512209
SP  - 1631
EP  - 1639
PB  - Association for Computing Machinery
SN  - 978-1-4503-9096-5
UR  - https://doi.org/10.1145/3485447.3512209
KW  - Heterogeneous Information Networks
KW  - Knowledge Distillation
KW  - Network Embedding
ER  - 

TY  - CONF
TI  - Privacy Preserving Event Sequence Data Visualization Using a Sankey Diagram-like Representation
AU  - Chou, Jia-Kai
AU  - Wang, Yang
AU  - Ma, Kwan-Liu
T3  - SA '16
AB  - Given the growing rates and richness of data being collected nowadays, it is non-trivial for data owners to determine a single best publishing granularity that presents the most value of the data while preserving its privacy. There have been extensive studies on privacy preserving algorithms in the data mining community, but relatively few have been done to provide a supervised control over the anonymization process. We present the design and evaluation of a visual interface that assists users to employ commonly used data anonymization techniques for making privacy preserving visualizations of the data. We focus on event sequence data due to its vulnerability to privacy concerns. Our visual interface is designed for data owners to examine potential privacy issues, obfuscate information as suggested by the algorithm, and fine-tune the results per their requests. Case studies using multiple datasets under different scenarios demonstrate the effectiveness of our design. These studies show that using visualization as an interface can help identify potential privacy issues, reveal underlying anonymization processes, and allow users to balance between data utility and privacy.
C1  - New York, NY, USA
C3  - SIGGRAPH ASIA 2016 Symposium on Visualization
DA  - 2016///
PY  - 2016
DO  - 10.1145/3002151.3002153
PB  - Association for Computing Machinery
SN  - 978-1-4503-4547-7
UR  - https://doi.org/10.1145/3002151.3002153
KW  - event sequence data analysis
KW  - privacy preserving visualization
ER  - 

TY  - CONF
TI  - Fairness in Deceased Organ Matching
AU  - Mattei, Nicholas
AU  - Saffidine, Abdallah
AU  - Walsh, Toby
T3  - AIES '18
AB  - As algorithms are given responsibility to make decisions that impact our lives, there is increasing awareness of the need to ensure the fairness of these decisions. One of the first challenges then is to decide what fairness means in a particular context. We consider here fairness in deciding how to match organs donated by deceased donors to patients. Due to the increasing age of patients on the waiting list, and of organs being donated, the current "first come, first served” mechanism used in Australia is under review to take account of age of patients and of organs. We consider how to revise the mechanism to take account of age fairly. We identify a number of different types of fairness, such as to patients, to regions and to blood types and consider how they can be achieved.
C1  - New York, NY, USA
C3  - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society
DA  - 2018///
PY  - 2018
DO  - 10.1145/3278721.3278749
SP  - 236
EP  - 242
PB  - Association for Computing Machinery
SN  - 978-1-4503-6012-8
UR  - https://doi.org/10.1145/3278721.3278749
KW  - decision making
KW  - ethical principles
KW  - kidney allocation
KW  - preferences
ER  - 

TY  - JOUR
TI  - The Tao of Inference in Privacy-Protected Databases
AU  - Bindschaedler, Vincent
AU  - Grubbs, Paul
AU  - Cash, David
AU  - Ristenpart, Thomas
AU  - Shmatikov, Vitaly
T2  - Proc. VLDB Endow.
AB  - To protect database confidentiality even in the face of full compromise while supporting standard functionality, recent academic proposals and commercial products rely on a mix of encryption schemes. The recommendation is to apply strong, semantically secure encryption to the "sensitive" columns and protect other columns with property-revealing encryption (PRE) that supports operations such as sorting.We design, implement, and evaluate a new methodology for inferring data stored in such encrypted databases. The cornerstone is the multinomial attack, a new inference technique that is analytically optimal and empirically outperforms prior heuristic attacks against PRE-encrypted data. We also extend the multinomial attack to take advantage of correlations across multiple columns. This recovers PRE-encrypted data with sufficient accuracy to then apply machine learning and record linkage methods to infer columns protected by semantically secure encryption or redaction.We evaluate our methodology on medical, census, and union-membership datasets, showing for the first time how to infer full database records. For PRE-encrypted attributes such as demographics and ZIP codes, our attack outperforms the best prior heuristic by a factor of 16. Unlike any prior technique, we also infer attributes, such as incomes and medical diagnoses, protected by strong encryption. For example, when we infer that a patient in a hospital-discharge dataset has a mental health or substance abuse condition, this prediction is 97% accurate.
DA  - 2018/07//
PY  - 2018
DO  - 10.14778/3236187.3236217
VL  - 11
IS  - 11
SP  - 1715
EP  - 1728
SN  - 2150-8097
UR  - https://doi.org/10.14778/3236187.3236217
ER  - 

TY  - CONF
TI  - A Model Based Simulation Toolkit for Evaluating Renal Replacement Policies
AU  - Celik, Bilge
AU  - Van Gorp, Pieter M. E.
AU  - Snoeck, Andre C. J.
AU  - van Riet, Remi C.
AU  - de Winter, Peter J.
AU  - Wilbik, Anna
T3  - WSC '17
AB  - Renal failure concerns progressive loss of kidney function. Renal Replacement Therapy (RRT) is a costly, long-running process that includes several decision points in different stages. Small changes in the protocol can impact significantly the expenditures and healthcare outcomes. Unfortunately, policy makers have very little support for benchmarking improvement alternatives. The existing models are designed to fit certain applications with preset parameters and design choices which do not match with the requirements of a policy analysis. A generic approach is required to analyze the effects of different design options adjustable to finer scales. To remedy this, this paper describes a novel toolkit for evaluating renal replacement policies, containing a parametrized colored Petri-Net which can be configured for the specifics of local settings. The model is made available for open access to overcome the non-replicability issue of existing models.
C3  - Proceedings of the 2017 Winter Simulation Conference
DA  - 2017///
PY  - 2017
PB  - IEEE Press
SN  - 978-1-5386-3427-1
ER  - 

TY  - CONF
TI  - Adapting Graph Theory and Social Network Measures on Healthcare Data: A New Framework to Understand Chronic Disease Progression
AU  - Khan, Arif
AU  - Uddin, Shahadat
AU  - Srinivasan, Uma
T3  - ACSW '16
AB  - Governments all over the world are concerned about the disease burden caused by chronic conditions. A significant portion of this comes from potentially preventable hospital admissions. By adopting preventive measures, these admissions can be avoided which in turn can reduce cost and health risk, further benefitting the funders, providers and patients as well. One potential approach can be to look at healthcare information system, more specifically - hospital admission data that carries rich semantic information. In this paper we present a novel framework to apply social network and graph theoretic methods on modern healthcare data to analyse and understand chronic disease progression to enable all stakeholders to take appropriate preventive measures.
C1  - New York, NY, USA
C3  - Proceedings of the Australasian Computer Science Week Multiconference
DA  - 2016///
PY  - 2016
DO  - 10.1145/2843043.2843380
PB  - Association for Computing Machinery
SN  - 978-1-4503-4042-7
UR  - https://doi.org/10.1145/2843043.2843380
KW  - chronic disease
KW  - health informatics
KW  - knowledge management
KW  - data science
KW  - healthcare data
KW  - network theory
KW  - risk assessment
KW  - social network analysis
ER  - 

TY  - CONF
TI  - Learning Behavior of Memristor-Based Neuromorphic Circuits in the Presence of Radiation
AU  - Dahl, Sumedha Gandharava
AU  - Ivans, Robert C.
AU  - Cantley, Kurtis D.
T3  - ICONS '19
AB  - In this paper, a feed-forward spiking neural network with memristive synapses is designed to learn a spatio-temporal pattern representing the 25-pixel character 'B' by separating correlated and uncorrelated afferents. The network uses spike-timing-dependent plasticity (STDP) learning behavior, which is implemented using biphasic neuron spikes. A TiO2 memristor non-linear drift model is used to simulate synaptic behavior in the neuromorphic circuit. The network uses a many-to-one topology with 25 pre-synaptic neurons (afferent) each connected to a memristive synapse and one postsynaptic neuron. The memristor model is modified to include the experimentally observed effect of state-altering radiation. During the learning process, irradiation of the memristors alters their conductance state, and the effect on circuit learning behavior is determined. Radiation is observed to generally increase the synaptic weight of the memristive devices, making the network connections more conductive and less stable. However, the network appears to relearn the pattern when radiation ceases but does take longer to resolve the correlation and pattern. Network recovery time is proportional to flux, intensity, and duration of the radiation. Further, at lower but continuous radiation exposure, (flux 1x1010 cm-2s-1 and below), the circuit resolves the pattern successfully for up to 100 s.
C1  - New York, NY, USA
C3  - Proceedings of the International Conference on Neuromorphic Systems
DA  - 2019///
PY  - 2019
DO  - 10.1145/3354265.3354272
PB  - Association for Computing Machinery
SN  - 978-1-4503-7680-8
UR  - https://doi.org/10.1145/3354265.3354272
KW  - radiation
KW  - leaky integrate-and-fire (LIF) neuron
KW  - Neuromorphic circuits
KW  - non-linear memristor model
KW  - spatio-temporal pattern learning
KW  - spike-timing-dependent plasticity (STDP)
ER  - 

TY  - JOUR
TI  - A Local Mean Representation-Based K-Nearest Neighbor Classifier
AU  - Gou, Jianping
AU  - Qiu, Wenmo
AU  - Yi, Zhang
AU  - Xu, Yong
AU  - Mao, Qirong
AU  - Zhan, Yongzhao
T2  - ACM Trans. Intell. Syst. Technol.
AB  - K-nearest neighbor classification method (KNN), as one of the top 10 algorithms in data mining, is a very simple and yet effective nonparametric technique for pattern recognition. However, due to the selective sensitiveness of the neighborhood size k, the simple majority vote, and the conventional metric measure, the KNN-based classification performance can be easily degraded, especially in the small training sample size cases.&nbsp;In this article, to further improve the classification performance and overcome the main issues in the KNN-based classification, we propose a local mean representation-based k-nearest neighbor classifier (LMRKNN). In the LMRKNN, the categorical k-nearest neighbors of a query sample are first chosen to calculate the corresponding categorical k-local mean vectors, and then the query sample is represented by the linear combination of the categorical k-local mean vectors; finally, the class-specific representation-based distances between the query sample and the categorical k-local mean vectors are adopted to determine the class of the query sample.&nbsp;Extensive experiments on many UCI and KEEL datasets and three popular face databases are carried out by comparing LMRKNN to the state-of-art KNN-based methods. The experimental results demonstrate that the proposed LMRKNN outperforms the related competitive KNN-based methods with more robustness and effectiveness.
DA  - 2019/04//
PY  - 2019
DO  - 10.1145/3319532
VL  - 10
IS  - 3
SN  - 2157-6904
UR  - https://doi.org/10.1145/3319532
KW  - K-nearest neighbor classification
KW  - local mean vector
KW  - pattern recognition
KW  - representation
ER  - 

TY  - CONF
TI  - Flexible and Personalized Learning for Wearable Health Applications Using HyperDimensional Computing
AU  - Shahhosseini, Sina
AU  - Ni, Yang
AU  - Kasaeyan Naeini, Emad
AU  - Imani, Mohsen
AU  - Rahmani, Amir M.
AU  - Dutt, Nikil
T3  - GLSVLSI '22
AB  - Health and wellness applications increasingly rely on machine learning techniques to learn end-user physiological and behavioral patterns in everyday settings, posing two key challenges: inability to perform on-device online learning for resource-constrained wearables, and learning algorithms that support privacy-preserving personalization. We exploit a Hyperdimensional computing (HDC) solution for wearable devices that offers flexibility, high efficiency, and performance while enabling on-device personalization and privacy protection. We evaluate the efficacy of our approach using three case studies and show that our system improves performance of training by up to 35.8x compared with the state-of-the-art while offering a comparable accuracy.
C1  - New York, NY, USA
C3  - Proceedings of the Great Lakes Symposium on VLSI 2022
DA  - 2022///
PY  - 2022
DO  - 10.1145/3526241.3530373
SP  - 357
EP  - 360
PB  - Association for Computing Machinery
SN  - 978-1-4503-9322-5
UR  - https://doi.org/10.1145/3526241.3530373
KW  - hyperdimensional computing
KW  - personalization
KW  - wearable devices
ER  - 

TY  - CONF
TI  - Understanding the Comorbidity of Multiple Chronic Diseases Using a Network Approach
AU  - Hossain, Md Ekramul
AU  - khan, Arif
AU  - Uddin, Shahadat
T3  - ACSW 2019
AB  - Chronic diseases and associated conditions are the leading causes of death in most of the countries worldwide. Due to this, governments all over the world are concerned about the burden of chronic diseases. These diseases often pose severe health risks to the patients when they suffer from more than one chronic disease at the same time (also named as comorbidity of chronic disease). Several prediction approaches utilizing routinely collected administrative healthcare data have been developed for the prevention and better management of comorbidity. Most studies to date have focused on understanding the progression of single chronic disease rather than multiple chronic diseases. In this study, a research framework is proposed using social network analysis and graph theory using administrative healthcare data to understand the comorbidity of two chronic diseases (i.e., type 2 diabetes (T2D) leading to the development of cardiovascular disease). The results show that diseases related to blood (e.g., high blood pressure or high cholesterol) and kidney disease occurred frequently during the progression of cardiovascular disease for the T2D patients. The proposed framework could be useful for stakeholders including governments and health insurers to adopt appropriate prevention or management program for the patients at high risk of developing multiple chronic diseases.
C1  - New York, NY, USA
C3  - Proceedings of the Australasian Computer Science Week Multiconference
DA  - 2019///
PY  - 2019
DO  - 10.1145/3290688.3290730
PB  - Association for Computing Machinery
SN  - 978-1-4503-6603-8
UR  - https://doi.org/10.1145/3290688.3290730
KW  - Comorbidity
KW  - Administrative healthcare data
KW  - Baseline disease network
KW  - Chronic disease
KW  - Health informatics
KW  - Social network analysis
ER  - 

TY  - CONF
TI  - Modular Argumentation for Modelling Legal Doctrines of Performance Relief
AU  - Dung, Phan Minh
AU  - Thang, Phan Minh
AU  - Hung, Nguyen Duy
T3  - ICAIL '09
AB  - Legal doctrines provide principles, guidelines and rules for dispute resolution in reasoning with cases. To apply legal doctrines, the context of a contract consisting of different knowledge bases about beliefs and expertise of contract parties as well as about common social, legal domains need to be established. Judges then decide legal outcomes by reasoning from factors drawn in contract contexts following legal doctrines. In this paper, we model this decision making by modular argumentation. We focus on legal doctrines in contract law, especially the doctrines of impossibility and frustration of purpose.
C1  - New York, NY, USA
C3  - Proceedings of the 12th International Conference on Artificial Intelligence and Law
DA  - 2009///
PY  - 2009
DO  - 10.1145/1568234.1568249
SP  - 128
EP  - 136
PB  - Association for Computing Machinery
SN  - 978-1-60558-597-0
UR  - https://doi.org/10.1145/1568234.1568249
KW  - argumentation
KW  - frustration
KW  - impossibility
KW  - legal doctrines
ER  - 

TY  - CONF
TI  - Exploring the Addition of Audio Input to Wearable Punch Recognition
AU  - Ovalle, Juan Quintero
AU  - Stawarz, Katarzyna
AU  - Marzo, Asier
T3  - Interacción '19
AB  - Martial arts can promote healthy lifestyles, improve self-confidence and provide self-defence skills. Previous work has demonstrated that inertial sensors can be used to recognise movements such as punches in boxing and support self-directed training. However, many martial arts do not use gloves which means that punches can be performed with different parts of the hand, and therefore produce a different sound on impact. We investigate if it is possible to recognise different punches executed with a bare hand, and if the recognition rate improves by combining audio input with the traditional inertial sensors. We conducted a pilot study collecting a total of 600 punches, using a wearable wristband to capture inertial data and a stand-alone microphone for audio input. The results showed that it was possible to distinguish five types of punches with 94.4% accuracy when using only inertial data, and that adding audio input did not improve the accuracy. These findings can guide the design of future wearables for punch recognition.
C1  - New York, NY, USA
C3  - Proceedings of the XX International Conference on Human Computer Interaction
DA  - 2019///
PY  - 2019
DO  - 10.1145/3335595.3335641
PB  - Association for Computing Machinery
SN  - 978-1-4503-7176-6
UR  - https://doi.org/10.1145/3335595.3335641
KW  - machine learning
KW  - wearables
KW  - gesture recognition
KW  - inertial sensors
KW  - martial arts
KW  - punch recognition
ER  - 

TY  - CONF
TI  - Voice-Controlled Clinical Coding Companion (VC4) for ICD-10-AM and ACHI Code Assignment
AU  - Kaur, Rajvir
AU  - Anupama Ginige, Jeewani
T3  - ACSW '20
AB  - The use of voice assistant devices such as Siri, Alexa, Google Assistant, and Samsung Bixby in the healthcare industry is gaining a lot of attention nowadays. Many healthcare professionals believe that a clinically-oriented Voice assistant devices will ease the burden of accessing patient’s information, maintaining documents, and improve the patient experience. This paper presents a prototypic implementation of Alexa, named VC4 (voice controlled clinical coding companion), that can assist clinical coders by suggesting most appropriate ICD-10-AM and ACHI codes for a given clinical description through voice commands.
C1  - New York, NY, USA
C3  - Proceedings of the Australasian Computer Science Week Multiconference
DA  - 2020///
PY  - 2020
DO  - 10.1145/3373017.3373060
PB  - Association for Computing Machinery
SN  - 978-1-4503-7697-6
UR  - https://doi.org/10.1145/3373017.3373060
KW  - Alexa
KW  - Amazon skill
KW  - Clinical coding
KW  - Voice controlled systems
ER  - 

TY  - CHAP
TI  - Issues in the Usability of Time-Varying Multimedia
AU  - Muller, Michael J.
AU  - Farrell, Robert F.
AU  - Cebulka, Kathleen D.
AU  - Smith, John G.
T2  - Multimedia Interface Design
CY  - New York, NY, USA
DA  - 1992///
PY  - 1992
SP  - 7
EP  - 38
PB  - Association for Computing Machinery
SN  - 0-201-54981-6
UR  - https://doi.org/10.1145/146022.146029
ER  - 

TY  - JOUR
TI  - On Compile-Time Query Optimization in Deductive Databases by Means of Static Filtering
AU  - Kifer, Michael
AU  - Lozinskii, Eliezer L.
T2  - ACM Trans. Database Syst.
AB  - We extend the query optimization techniques known as algebric manipulations with relational expressions [48] to work with deductive databases. In particular, we propose a method for moving data-independent selections and projections into recursive axioms, which extends all other known techniques for performing that task [2, 3, 9, 18, 20]. We also show that, in a well-defined sense, our algorithm is optimal among the algorithms that propagate data-independent selections through recursion.
DA  - 1990/09//
PY  - 1990
DO  - 10.1145/88636.87121
VL  - 15
IS  - 3
SP  - 385
EP  - 426
SN  - 0362-5915
UR  - https://doi.org/10.1145/88636.87121
KW  - inference
KW  - dataflow
KW  - deductive databases
KW  - filtering
KW  - fixpoint
KW  - graph representation
KW  - projection
KW  - recursive rules
KW  - selection
ER  - 

TY  - CONF
TI  - Convolution Shadow Maps
AU  - Annen, Thomas
AU  - Mertens, Tom
AU  - Bekaert, Philippe
AU  - Seidel, Hans-Peter
AU  - Kautz, Jan
T3  - EGSR'07
AB  - We present Convolution Shadow Maps, a novel shadow representation that affords efficient arbitrary linear filtering of shadows. Traditional shadow mapping is inherently non-linear w.r.t. the stored depth values, due to the binary shadow test. We linearize the problem by approximating shadow test as a weighted summation of basis terms. We demonstrate the usefulness of this representation, and show that hardware-accelerated anti-aliasing techniques, such as tri-linear filtering, can be applied naturally to Convolution Shadow Maps. Our approach can be implemented very efficiently in current generation graphics hardware, and offers real-time frame rates.
C1  - Goslar, DEU
C3  - Proceedings of the 18th Eurographics Conference on Rendering Techniques
DA  - 2007///
PY  - 2007
SP  - 51
EP  - 60
PB  - Eurographics Association
SN  - 978-3-905673-52-4
ER  - 

TY  - CONF
TI  - A New Simulation Model for Kidney Transplantation in the United States
AU  - Sandikçi, Burhaneddin
AU  - Tunç, Sait
AU  - Tanriöver, Bekir
T3  - WSC '19
AB  - The United Network for Organ Sharing (UNOS) has been using simulation models for over two decades to guide the evolution of organ allocation policies in the United States. UNOS kidney simulation model (KPSAM), which played a crucial role in the 2014 U.S. kidney allocation policy update, is also available to the general public as an executable file. However, this format offers little flexibility to its users in trying out different policy proposals. We describe the development of a discrete-event simulation model as an alternative to KPSAM. It is similar to KPSAM in incorporating many clinical and operational details. On the other hand, it offers more flexibility in evaluating various policy proposals and runs significantly faster than KPSAM due to its efficient use of modern computing technologies. Simulated results closely match actual U.S. kidney transplantation outcomes, building confidence in the accuracy and validity of the model.
C3  - Proceedings of the Winter Simulation Conference
DA  - 2020///
PY  - 2020
SP  - 1079
EP  - 1090
PB  - IEEE Press
SN  - 978-1-72813-283-9
ER  - 

TY  - BOOK
TI  - ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering
CY  - New York, NY, USA
DA  - 2022///
PY  - 2022
PB  - Association for Computing Machinery
SN  - 978-1-4503-9475-8
ER  - 

TY  - CONF
TI  - Impacts of Business Intelligence on Population Health: A Systematic Literature Review
AU  - Cohen, L.
T3  - SAICSIT '17
AB  - "Business Intelligence" is an area of Information Technology (IT) that involves the collection, analysis and presentation of large amounts of data. BI has been successfully applied to promote good decision making in a variety of environments, and has high potential to make a significant impact in the domain of population health. The promotion of population health is a key concern of government authorities and various health institutions and officials making decisions about interventions that may impact on population health would benefit from the use of information on population health. BI could clearly be a facilitator in this regard, but evidence of its current application and impact in this field is not easily accessible to policy makers. This systematic literature review explored the literature and provided a synthesis of information available on the current use of BI in this area, and evidence of the impact of its use on population health. An array of applications of BI for population health were found, including data warehouses, analytics, reports, data warehouse browsers, OLAP, GIS, Dashboards and Alerts. Evidence of the impact of these applications on population health was mainly anecdotal, with only one empirical study found. Issues and challenges encountered in the development and use of BI are Privacy and Security, Data Quality and Development and Maintenance of BI infrastructure
C1  - New York, NY, USA
C3  - Proceedings of the South African Institute of Computer Scientists and Information Technologists
DA  - 2017///
PY  - 2017
DO  - 10.1145/3129416.3129441
PB  - Association for Computing Machinery
SN  - 978-1-4503-5250-5
UR  - https://doi.org/10.1145/3129416.3129441
KW  - business intelligence
KW  - population health
KW  - systematic literature review
ER  - 

TY  - CONF
TI  - A Service-Oriented Framework for Developing Personalized Patient Care Plans for COVID-19
AU  - Elahraf, Abeer
AU  - Afzal, Ayesha
AU  - Akhtar, Ahmed
AU  - Shafiq, Basit
AU  - Vaidya, Jaideep
T3  - DG.O'21
AB  - The COVID-19 pandemic has identified weaknesses and stresses in the existing healthcare and governance system, even in the most developed countries. Given the scale and scope of the pandemic, existing healthcare systems are heavily resource constrained, and home-based isolation has been considered as a potential first step for reducing both the disease spread and the stress on the healthcare system. However, the needs and requirements of home-based isolation are extremely unique for each patient, depending on their medical condition and comorbidities, family responsibilities, and environmental constraints. Therefore, it is necessary to develop personalized patient care plans to ensure that the needs of each patient are appropriately met. In this paper we propose a service oriented framework that allows dynamic composition and management of such plans assuming existence of an appropriate knowledge base and availability of web-services interfaces of the underlying systems of caregivers and service providers. We develop a prototype implementation to show the feasibility of the proposed framework and discuss the challenges/issues in deploying such a system in practice.
C1  - New York, NY, USA
C3  - DG.O2021: The 22nd Annual International Conference on Digital Government Research
DA  - 2021///
PY  - 2021
DO  - 10.1145/3463677.3463742
SP  - 234
EP  - 241
PB  - Association for Computing Machinery
SN  - 978-1-4503-8492-6
UR  - https://doi.org/10.1145/3463677.3463742
KW  - Interoperability
KW  - Knowledge-centric workflows
KW  - Personalized patient care plans
KW  - Web services
ER  - 

TY  - CONF
TI  - Using Deceased-Donor Kidneys to Initiate Chains of Living Donor Kidney Paired Donations: Algorithm and Experimentation
AU  - Cornelio, Cristina
AU  - Furian, Lucrezia
AU  - Nicolò, Antonio
AU  - Rossi, Francesca
T3  - AIES '19
AB  - We design a flexible algorithm that exploits deceased donor kidneys to initiate chains of living donor kidney paired donations, combining deceased and living donor allocation mechanisms to improve the quantity and quality of kidney transplants. The advantages of this approach have been measured using retrospective data on the pool of donor/recipient incompatible and desensitized pairs at the Padua University Hospital, the largest center for living donor kidney transplants in Italy. The experiments show a remarkable improvement on the number of patients with incompatible donor who could be transplanted, a decrease in the number of desensitization procedures, and an increase in the number of UT patients (that is, patients unlikely to be transplanted for immunological reasons) in the waiting list who could receive an organ.
C1  - New York, NY, USA
C3  - Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society
DA  - 2019///
PY  - 2019
DO  - 10.1145/3306618.3314276
SP  - 477
EP  - 483
PB  - Association for Computing Machinery
SN  - 978-1-4503-6324-2
UR  - https://doi.org/10.1145/3306618.3314276
KW  - kidneys exchange
KW  - matching
ER  - 

TY  - CONF
TI  - Recovering the Unbiased Scene Graphs from the Biased Ones
AU  - Chiou, Meng-Jiun
AU  - Ding, Henghui
AU  - Yan, Hanshu
AU  - Wang, Changhu
AU  - Zimmermann, Roger
AU  - Feng, Jiashi
T3  - MM '21
AB  - Given input images, scene graph generation (SGG) aims to produce comprehensive, graphical representations describing visual relationships among salient objects. Recently, more efforts have been paid to the long tail problem in SGG; however, the imbalance in the fraction of missing labels of different classes, or reporting bias, exacerbating the long tail is rarely considered and cannot be solved by the existing debiasing methods. In this paper we show that, due to the missing labels, SGG can be viewed as a "Learning from Positive and Unlabeled data" (PU learning) problem, where the reporting bias can be removed by recovering the unbiased probabilities from the biased ones by utilizing label frequencies, i.e., the per-class fraction of labeled, positive examples in all the positive examples. To obtain accurate label frequency estimates, we propose Dynamic Label Frequency Estimation (DLFE) to take advantage of training-time data augmentation and average over multiple training iterations to introduce more valid examples. Extensive experiments show that DLFE is more effective in estimating label frequencies than a naive variant of the traditional estimate, and DLFE significantly alleviates the long tail and achieves state-of-the-art debiasing performance on the VG dataset. We also show qualitatively that SGG models with DLFE produce prominently more balanced and unbiased scene graphs. The source code is publicly available.
C1  - New York, NY, USA
C3  - Proceedings of the 29th ACM International Conference on Multimedia
DA  - 2021///
PY  - 2021
DO  - 10.1145/3474085.3475297
SP  - 1581
EP  - 1590
PB  - Association for Computing Machinery
SN  - 978-1-4503-8651-7
UR  - https://doi.org/10.1145/3474085.3475297
KW  - long tail
KW  - missing labels
KW  - reporting bias
KW  - scene graph generation
ER  - 

TY  - CONF
TI  - Approximately Strategy-Proof Mechanisms for (Constrained) Facility Location
AU  - Sui, Xin
AU  - Boutilier, Craig
T3  - AAMAS '15
AB  - Mechanism design for facility location (or selection of alternatives in a metric space) has been studied for decades. While strategy-proof, efficient mechanisms exist for unconstrained, one-dimensional, single-facility problems, guarantees of strategy-proofness and efficiency often break when allowing: (a) multiple dimensions; (b) multiple facilities; or (c) constraints on the feasible placement of facilities. We address these more general problems, providing several possibility/impossibility results with respect to individual and group strategy-proofness in both constrained and unconstrained problems. We also bound the incentive for manipulation in median-like mechanisms in settings where strategy-proofness is not possible. We complement our results with empirical analysis of both electoral and geographic facility data, showing that the odds of successful manipulation, and more importantly, the gains and impact on social welfare, are small in practice (much less than worst-case theoretical bounds).
C1  - Richland, SC
C3  - Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems
DA  - 2015///
PY  - 2015
SP  - 605
EP  - 613
PB  - International Foundation for Autonomous Agents and Multiagent Systems
SN  - 978-1-4503-3413-6
KW  - facility location
KW  - mechanism design
KW  - single-peaked preferences
KW  - social choice
ER  - 

TY  - CONF
TI  - IoT-Based Healthcare Systems: A Survey
AU  - Yassein, Muneer Bani
AU  - Hmeidi, Ismail
AU  - Al-Harbi, Marwa
AU  - Mrayan, Lina
AU  - Mardini, Wail
AU  - Khamayseh, Yaser
T3  - DATA '19
AB  - Internet of Things (IoT) is one of the leading technologies in the IT field. All the industries are trying to employ the advantages of this technology by integrating it in different functions. Healthcare is one of the top industries that saw the benefits of incorporating this technology and gaining a great benefit from the evolution of it. This paper presents a review of services, different software, and techniques developed in the base of IoT for the Healthcare industry, which is called the Internet of Health Things (IoHT). The IoT is significantly overlapping with the Healthcare industry, converting it to a smarter Healthcare System. The result of this paper will serve as a source and a reference of information about smart Healthcare that healthcare professionals, researchers, and people interested in the topic to get know the IoHT.
C1  - New York, NY, USA
C3  - Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems
DA  - 2019///
PY  - 2019
DO  - 10.1145/3368691.3368721
PB  - Association for Computing Machinery
SN  - 978-1-4503-7284-8
UR  - https://doi.org/10.1145/3368691.3368721
KW  - mHealth
KW  - wearable
KW  - internet of health things (IoHT)
KW  - internet of things (IoT)
KW  - smart health
KW  - telehealth
ER  - 

TY  - CONF
TI  - Improving Cross-Domain Information Sharing in Care Coordination Using Semantic Web Technologies
AU  - Kotoulas, Spyros
AU  - Lopez, Vanessa
AU  - Sbodio, Marco Luca
AU  - Tommasi, Pierpaolo
AU  - Stephenson, Martin
AU  - Mac Aonghusa, Pol
T3  - IUI '14
AB  - We present an approach to access and consolidate complex information spanning multiple specialist domains and make it available to non-experts. We are using a combination of business rules and contextual exploration to reduce interface complexity and improve consumability. We present a use case and a prototype on top of a real-world enterprise solution for coordinating Social care and Health care. We evaluate our system through a user study. Our results indicate that our approach reduces the time required to obtain business results compared to a baseline graph exploration approach.
C1  - New York, NY, USA
C3  - Proceedings of the 19th International Conference on Intelligent User Interfaces
DA  - 2014///
PY  - 2014
DO  - 10.1145/2557500.2557538
SP  - 347
EP  - 352
PB  - Association for Computing Machinery
SN  - 978-1-4503-2184-6
UR  - https://doi.org/10.1145/2557500.2557538
KW  - care coordination
KW  - linked data
KW  - semantic web
ER  - 

TY  - JOUR
TI  - Inference Attacks and Controls on Genotypes and Phenotypes for Individual Genomic Data
AU  - He, Zaobo
AU  - Yu, Jiguo
AU  - Li, Ji
AU  - Han, Qilong
AU  - Luo, Guangchun
AU  - Li, Yingshu
T2  - IEEE/ACM Trans. Comput. Biol. Bioinformatics
AB  - The rapid growth of DNA-sequencing technologies motivates more personalized and predictive genetic-oriented services, which further attract individuals to increasingly release their genome information to learn about personalized medicines, disease predispositions, genetic compatibilities, etc. Individual genome information is notoriously privacy-sensitive and highly associated with relatives. In this paper, we present an inference attack algorithm to predict target genotypes and phenotypes based on belief propagation in factor graphs. With this algorithm, an attacker can effectively predict the target genotypes and phenotypes of target individuals based on genome information shared by individuals or their relatives, and genotype and phenotype association from genome-wide association study (GWAS). To address the privacy threats resulted from such inference attacks, we elaborate the metrics to evaluate data utility and privacy and then present a data sanitization method. We evaluate our inference attack algorithm and data sanitization method on real GWAS dataset: Age-related macular degeneration (AMD) case/control dataset. The evaluation results show that our work can effectively defense against genome threats while guaranteeing data utility.
DA  - 2020/06//
PY  - 2020
DO  - 10.1109/TCBB.2018.2810180
VL  - 17
IS  - 3
SP  - 930
EP  - 937
SN  - 1545-5963
UR  - https://doi.org/10.1109/TCBB.2018.2810180
ER  - 

TY  - CONF
TI  - Solving the N-Queens Problem Using Genetic Algorithms
AU  - Crawford, Kelly D.
T3  - SAC '92
C1  - New York, NY, USA
C3  - Proceedings of the 1992 ACM/SIGAPP Symposium on Applied Computing: Technological Challenges of the 1990's
DA  - 1992///
PY  - 1992
DO  - 10.1145/130069.130128
SP  - 1039
EP  - 1047
PB  - Association for Computing Machinery
SN  - 0-89791-502-X
UR  - https://doi.org/10.1145/130069.130128
ER  - 

TY  - CONF
TI  - Exploring Collective Tagging as a Mechanism to Elicit Language about Health Management
AU  - Chen, Annie T.
AU  - Carriere, Rachel
AU  - Kaplan, Samantha J.
AU  - Colht, Kelly
AU  - Morey, Ophelia T.
AU  - Flaherty, Mary Grace
AU  - Moser, Gail B.
AU  - Slager, Stacey L.
AU  - Price, Cynthia
T3  - ASIST '16
AB  - This paper describes an innovative effort to invite the public to participate in the creation of a public resource - to leverage collective wisdom in the health domain. This project involved building a website where people could contribute their experiences of body listening and how they learned/were learning to do it. Within the context of this study, body listening was described as the act of paying attention to the body's signals and cues, which can improve individuals' long-term health management. Over the course of the study, participants and moderators together authored 431 posts and contributed 818 tags. This paper presents an initial analysis of these tags.This analysis makes several intellectual contributions. First, we present a preliminary classification scheme for concepts associated with body listening and self-management that might be used in future efforts to organize knowledge in this domain. Second, we evaluate inter-annotator agreement using this classification scheme, to characterize its potential for use in future work.
C1  - USA
C3  - Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information &amp; Technology
DA  - 2016///
PY  - 2016
PB  - American Society for Information Science
KW  - body awareness
KW  - body listening
KW  - collective tagging
KW  - folksonomies
KW  - knowledge organization
ER  - 

TY  - CONF
TI  - Finding near Neighbors through Cluster Pruning
AU  - Chierichetti, Flavio
AU  - Panconesi, Alessandro
AU  - Raghavan, Prabhakar
AU  - Sozio, Mauro
AU  - Tiberi, Alessandro
AU  - Upfal, Eli
T3  - PODS '07
AB  - Finding near(est) neighbors is a classic, difficult problem in data management and retrieval, with applications in text and image search,in finding similar objects and matching patterns. Here we study cluster pruning, an extremely simple randomized technique. During preprocessing we randomly choose a subset of data points to be leaders the remaining data points are partitioned by which leader is the closest. For query processing, we find the leader(s) closest to the query point. We then seek the nearest neighbors for the query point among only the points in the clusters of the closest leader(s). Recursion may be used in both preprocessing and in search. Such schemes seek approximate nearest neighbors that are "almost as good" as the nearest neighbors. How good are these approximations and how much do they save in computation.Our contributions are: (1) we quantify metrics that allow us to study the tradeoff between processing and the quality of the approximate nearest neighbors; (2) we give rigorous theoretical analysis of our schemes, under natural generative processes (generalizing Gaussian mixtures) for the data points; (3) experiments on both synthetic data from such generative processes, as well as on from a document corpus, confirming that we save orders of magnitude in query processing cost at modest compromises in the quality of retrieved points. In particular, we show that p-spheres, a state-of-the-art solution, is outperformed by our simple scheme whether the data points are stored in main or in external memo.
C1  - New York, NY, USA
C3  - Proceedings of the Twenty-Sixth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems
DA  - 2007///
PY  - 2007
DO  - 10.1145/1265530.1265545
SP  - 103
EP  - 112
PB  - Association for Computing Machinery
SN  - 978-1-59593-685-1
UR  - https://doi.org/10.1145/1265530.1265545
KW  - clustering
KW  - generative model
KW  - nearest neighbor
ER  - 

TY  - CONF
TI  - Energy Efficient FJ/Spike LTS e-Neuron Using 55-Nm Node
AU  - Ferreira, Pietro M.
AU  - De Carvalho, Nathan
AU  - Klisnick, Geoffroy
AU  - Benlarbi-Delai, Aziz
T3  - SBCCI '19
AB  - While CMOS technology is currently reaching its limits in power consumption and circuit density, a challenger is emerging from the analogy between biology and silicon. Hardware-based neural networks may drive a new generation of bio-inspired computers by the urge of a hardware solution for real-time applications. This paper redesigns a previous proposed electronic neuron (e-Neuron) in a higher firing rate to reduce the silicon area and highlight a better energy efficiency trade-off. Besides, an innovative schematic is proposed to state an e-Neuron library based on Izhikevichs model of neural firing patterns. Both e-Neuron circuits are designed using 55 nm technology node. Physical design of transistors in weak inversion are discussed to a minimal leakage. Neural firing pattern behaviors are validated by post-layout simulations, demonstrating the spike frequency adaptation and the rebound spikes due to post-inhibitory effect in LTS e-Neuron. Presented results suggest that the time to rebound spikes is dependent of the excitation current amplitude. Both e-Neurons have presented a fF/spike energy efficiency and a smaller silicon area in comparison to Izhikevichs library propositions in the literature.
C1  - New York, NY, USA
C3  - Proceedings of the 32nd Symposium on Integrated Circuits and Systems Design
DA  - 2019///
PY  - 2019
DO  - 10.1145/3338852.3339852
PB  - Association for Computing Machinery
SN  - 978-1-4503-6844-5
UR  - https://doi.org/10.1145/3338852.3339852
KW  - neuromorphic circuits
KW  - non-linear electronics
KW  - ultra-low-power
ER  - 

TY  - CONF
TI  - Robust Irregular Tensor Factorization and Completion for Temporal Health Data Analysis
AU  - Ren, Yifei
AU  - Lou, Jian
AU  - Xiong, Li
AU  - Ho, Joyce C.
T3  - CIKM '20
AB  - Electronic health records (EHR) are often generated and collected across a large number of patients featuring distinctive medical conditions and clinical progress over a long period of time, which results in unaligned records along the time dimension. EHR is also prone to missing and erroneous data due to various practical reasons. Recently, PARAFAC2 has been re-popularized for successfully extracting meaningful medical concepts (phenotypes) from such temporal EHR by irregular tensor factorization. Despite recent advances, existing PARAFAC2 methods are unable to robustly handle erroneousness and missing data which are prevalent in clinical practice. We propose REPAIR, a Robust tEmporal PARAFAC2 method for IRregular tensor factorization and completion method, to complete an irregular tensor and extract phenotypes in the presence of missing and erroneous values. To achieve this, REPAIR designs a new effective low-rank regularization function for PARAFAC2 to handle missing and erroneous entries, which has not been explored for irregular tensors before. In addition, the optimization of REPAIR allows it to enjoy the same computational scalability and incorporate a variety of constraints as the state-of-the-art PARAFAC2 method for efficient and meaningful phenotype extraction. We evaluate REPAIR on two real temporal EHR datasets to verify its robustness in tensor factorization against various missing and outlier conditions. Furthermore, we conduct two case studies to demonstrate that REPAIR is able to extract meaningful and useful phenotypes from such corrupted temporal EHR. Our implementation is publicly available https://github.com/Emory-AIMS/Repair.
C1  - New York, NY, USA
C3  - Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management
DA  - 2020///
PY  - 2020
DO  - 10.1145/3340531.3411982
SP  - 1295
EP  - 1304
PB  - Association for Computing Machinery
SN  - 978-1-4503-6859-9
UR  - https://doi.org/10.1145/3340531.3411982
KW  - computational phenotyping
KW  - electronic health records (EHR)
KW  - irregular tensor
KW  - PARAFAC2
KW  - tensor factorization
ER  - 

TY  - CONF
TI  - Qualitative Methods for Studying Health Information Behaviors
AU  - Oh, Sanghee
AU  - Costello, Kaitlin L.
AU  - Chen, Annie T.
AU  - Wildemuth, Barbara M.
T3  - ASIST '16
AB  - Qualitative methods can be used to understand and interpret human behaviors in a variety of contexts, including people's health information behaviors. This panel will discuss current applications of qualitative methods in this context, with emphasis on three examples: visual elicitation techniques, qualitative content analysis, and grounded theory approaches. The audience will be encouraged to discuss additional qualitative approaches and mixed methods, and how they might be applied to studies of health information behaviors.
C1  - USA
C3  - Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information &amp; Technology
DA  - 2016///
PY  - 2016
PB  - American Society for Information Science
KW  - interviews
KW  - content analyses
KW  - grounded theory approach
KW  - health information behaviors
KW  - qualitative research methods
KW  - social media
KW  - visual elicitation techniques
ER  - 

TY  - JOUR
TI  - A Survey on Healthy Food Decision Influences Through Technological Innovations
AU  - Marshall, Jermaine
AU  - Jimenez-Pazmino, Priscilla
AU  - Metoyer, Ronald
AU  - Chawla, Nitesh V.
T2  - ACM Trans. Comput. Healthcare
AB  - It is well known that unhealthy food consumption plays a significant role in dietary and lifestyle-related diseases. Therefore, it is important for researchers to examine methods that may encourage the consumer to consider healthier dietary and lifestyle habits as diseases such as obesity, heart disease, and high blood pressure remain a worldwide issue. One promising approach to influencing healthy dietary and lifestyle habits is food recommendation models that recommend food to users based on various factors such as health effects, nutrition, preferences, and daily habits. Unfortunately, much of this work has focused on individual factors such as taste preferences and often neglects to understand other factors that influence our choices. Additionally, the evaluation of technological approaches often lacks user studies in the context of intended use. In this systematic review of food choice technology, we focus on the factors that may influence food choices and how technology can play a role in supporting those choices. We also describe existing work, approaches, trends, and issues in current food choice technology and give advice for future work areas in this space.
DA  - 2022/03//
PY  - 2022
DO  - 10.1145/3494580
VL  - 3
IS  - 2
SN  - 2691-1957
UR  - https://doi.org/10.1145/3494580
KW  - Food
KW  - environment
KW  - food decisions
KW  - food properties
KW  - food recommendation systems
KW  - health
KW  - health applications
KW  - personal factors
KW  - technology
ER  - 

TY  - CONF
TI  - Feasibility Analysis for Estimation of Blood Pressure and Heart Rate Using A Smart Eye Wear
AU  - Ahmed, Nasimuddin
AU  - Banerjee, Rohan
AU  - Ghose, Avik
AU  - Sinharay, Arijit
T3  - WearSys '15
AB  - The major issue with hypertension is the fact that it does not have any symptoms also measurements are highly variable in nature. However, high blood pressure has severe effects on a person's health. This poses a requirement for continuous blood pressure monitoring and analysis of people suffering from chronic condition. In this paper we propose PPG based continuous blood pressure and heart rate monitoring system in form factor of a Smart Eye wear. We do a feasibility analysis of the idea and show that it is possible to determine blood pressure and heart rate by sensing PPG from the side of head region (temple) where a spectacle frame would fit. The advantage of Smart Eye Wear over other wearable device is that it provides better contact, minimal motion artifact and maintains uniform pressure without causing any trouble to user.
C1  - New York, NY, USA
C3  - Proceedings of the 2015 Workshop on Wearable Systems and Applications
DA  - 2015///
PY  - 2015
DO  - 10.1145/2753509.2753511
SP  - 9
EP  - 14
PB  - Association for Computing Machinery
SN  - 978-1-4503-3500-3
UR  - https://doi.org/10.1145/2753509.2753511
KW  - arduino
KW  - ppg
KW  - pulse sensor
KW  - windkessel model
ER  - 

TY  - CONF
TI  - Optimizing for Reduced Code Space Using Genetic Algorithms
AU  - Cooper, Keith D.
AU  - Schielke, Philip J.
AU  - Subramanian, Devika
T3  - LCTES '99
AB  - Code space is a critical issue facing designers of software for embedded systems. Many traditional compiler optimizations are designed to reduce the execution time of compiled code, but not necessarily the size of the compiled code. Further, different results can be achieved by running some optimizations more than once and changing the order in which optimizations are applied. Register allocation only complicates matters, as the interactions between different optimizations can cause more spill code to be generated. The compiler for embedded systems, then, must take care to use the best sequence of optimizations to minimize code space.Since much of the code for embedded systems is compiled once and then burned into ROM, the software designer will often tolerate much longer compile times in the hope of reducing the size of the compiled code. We take advantage of this by using a genetic algorithm to find optimization sequences that generate small object codes. The solutions generated by this algorithm are compared to solutions found using a fixed optimization sequence and solutions found by testing random optimization sequences. Based on the results found by the genetic algorithm, a new fixed sequence is developed to reduce code size. Finally, we explore the idea of using different optimization sequences for different modules and functions of the same program.
C1  - New York, NY, USA
C3  - Proceedings of the ACM SIGPLAN 1999 Workshop on Languages, Compilers, and Tools for Embedded Systems
DA  - 1999///
PY  - 1999
DO  - 10.1145/314403.314414
SP  - 1
EP  - 9
PB  - Association for Computing Machinery
SN  - 1-58113-136-4
UR  - https://doi.org/10.1145/314403.314414
ER  - 

TY  - JOUR
TI  - Optimizing for Reduced Code Space Using Genetic Algorithms
AU  - Cooper, Keith D.
AU  - Schielke, Philip J.
AU  - Subramanian, Devika
T2  - SIGPLAN Not.
AB  - Code space is a critical issue facing designers of software for embedded systems. Many traditional compiler optimizations are designed to reduce the execution time of compiled code, but not necessarily the size of the compiled code. Further, different results can be achieved by running some optimizations more than once and changing the order in which optimizations are applied. Register allocation only complicates matters, as the interactions between different optimizations can cause more spill code to be generated. The compiler for embedded systems, then, must take care to use the best sequence of optimizations to minimize code space.Since much of the code for embedded systems is compiled once and then burned into ROM, the software designer will often tolerate much longer compile times in the hope of reducing the size of the compiled code. We take advantage of this by using a genetic algorithm to find optimization sequences that generate small object codes. The solutions generated by this algorithm are compared to solutions found using a fixed optimization sequence and solutions found by testing random optimization sequences. Based on the results found by the genetic algorithm, a new fixed sequence is developed to reduce code size. Finally, we explore the idea of using different optimization sequences for different modules and functions of the same program.
DA  - 1999/05//
PY  - 1999
DO  - 10.1145/315253.314414
VL  - 34
IS  - 7
SP  - 1
EP  - 9
SN  - 0362-1340
UR  - https://doi.org/10.1145/315253.314414
ER  - 

TY  - JOUR
TI  - Unveiling High-Speed Follow-up Consultation for Chronic Disease Treatment: A Pediatric Hospital Case in China
AU  - Fu, Jiaojiao
AU  - Zhou, Yangfan
AU  - Wang, Xin
T2  - Proc. ACM Hum.-Comput. Interact.
AB  - In developing countries suffering from a severe shortage of physicians, the follow-up clinical consultation of long-term chronic care becomes a high-speed collaborative work, which has to be conducted in a few minutes. Although much existing work studies how time factors affect physicians' workflows and requirements for information systems, high-speed chronic care is yet to be well investigated. This work bridges the gap by presenting a case of a pediatric hospital in China. We focus on the processes of follow-up consultations, the factors enabling physicians to complete consultation in several minutes, as well as the challenges faced by physicians and patients. Through observations and interviews, we find that physicians conduct multiple tasks (information acquisition, patient-provider communication, and medical data documentation) simultaneously to reduce the consultation duration. Adopting an information summary alternative is the key to fast information acquisition. Templates and references in EMR contribute to rapid documentation and prescription. However, multitasking brings physicians a heavy cognitive load. It also severely compresses the duration of patient-provider communication. As a result, some of the patients' needs, especially emotional ones, are neglected. Based on these findings, we discuss the characteristics and requirements of high-speed chronic care and accordingly propose design suggestions.
DA  - 2022/11//
PY  - 2022
DO  - 10.1145/3568491
VL  - 6
IS  - CSCW2
UR  - https://doi.org/10.1145/3568491
KW  - chronic disease
KW  - Chinese pediatric hospital
KW  - clinical consultation
KW  - follow-up treatment
KW  - high-speed collaboration
KW  - long-term collaboration
ER  - 

TY  - CONF
TI  - Towards a Heterogeneous IoT Privacy Architecture
AU  - Gupta, Sanonda Datta
AU  - Ghanavati, Sepideh
T3  - SAC '20
AB  - The increasing growth of the Internet of Things (IoT) poses a wide range of privacy and security threats to consumers. Current research have addressed some of these challenges but recent privacy breaches in Home Assistant services show the gap between the current approaches and what is required. In this paper, we propose a privacy protection framework for the IoT called HIPA (Heterogeneous IoT Privacy Architecture) which aims at resolving some of the privacy concerns in a heterogeneous environment. With the help of a hypothetical case study, we illustrate how our framework can address the inconsistencies and risks of a new IoT device deploying within a heterogeneous network. Our initial analysis shows that our proposed framework can potentially help the user to analyze the risk and make the decision prior to the installation of any new IoT device.
C1  - New York, NY, USA
C3  - Proceedings of the 35th Annual ACM Symposium on Applied Computing
DA  - 2020///
PY  - 2020
DO  - 10.1145/3341105.3374108
SP  - 770
EP  - 772
PB  - Association for Computing Machinery
SN  - 978-1-4503-6866-7
UR  - https://doi.org/10.1145/3341105.3374108
KW  - heterogeneous architecture
KW  - internet of things
KW  - privacy protection
ER  - 

TY  - JOUR
TI  - EBP: FREQUENT AND COMFORTABLE BLOOD PRESSURE MONITORING FROM INSIDE HUMAN'S EARS
AU  - Bui, Nam
AU  - Pham, Nhat
AU  - Truong, Hoang
AU  - Nguyen, Phuc
AU  - Xiao, Jianliang
AU  - Deterding, Robin
AU  - Dinh, Thang
AU  - Vu, Tam
T2  - GetMobile: Mobile Comp. and Comm.
AB  - Diagnosing hypertension or hemodialysis requires patients to carry a blood pressure (BP) monitoring device for 24 hours. Th erefore, wearing the wrist/arm-based BP monitoring device, in this case, has a signifi cant impact on users' daily activities. To address the problem, we developed eBP, an ear-worn device that measures blood pressure from inside the ear. Th rough the evaluation of 35 subjects, eBP can achieve the average error of 1.8 mmHg for systolic BP and -3.1 mmHg for diastolic BP with the standard deviation error of 7.2 mmHg and 7.9 mmHg, respectively.
DA  - 2020/05//
PY  - 2020
DO  - 10.1145/3400713.3400721
VL  - 23
IS  - 4
SP  - 34
EP  - 38
SN  - 2375-0529
UR  - https://doi.org/10.1145/3400713.3400721
ER  - 

TY  - CONF
TI  - Distributed Cooperative Bayesian Learning Strategies
AU  - Yamanishi, Kenji
T3  - COLT '97
C1  - New York, NY, USA
C3  - Proceedings of the Tenth Annual Conference on Computational Learning Theory
DA  - 1997///
PY  - 1997
DO  - 10.1145/267460.267507
SP  - 250
EP  - 262
PB  - Association for Computing Machinery
SN  - 0-89791-891-6
UR  - https://doi.org/10.1145/267460.267507
ER  - 

TY  - CONF
TI  - A Linked Data Approach to Care Coordination
AU  - Kotoulas, Spyros
AU  - Lopez, Vanessa
AU  - Sbodio, Marco Luca
AU  - Stephenson, Martin
AU  - Tommasi, Pierpaolo
AU  - Mac Aonghusa, Pol
T3  - HT '14
AB  - The success of a society is often judged by its ability to support the most vulnerable. Supporting the most vulnerable individuals is extremely challenging from an information needs perspective, since it requires data from numerous domains and systems, including Social Care, Healthcare, Public Safety and Juridical systems. Information sharing on this scale gives rise to scientific and technical challenges with regard to data representation, access, integration and retrieval granularity. This is a practice-oriented paper presenting a Linked Data-based approach that is uniquely positioned to access and surface information across domains and data sources using a combination of vulnerability indexes and contextual exploration. We apply this approach on a set of enterprise systems from IBM to develop an information sharing architecture and prototype for Care Coordination with a focus on Social Care and Healthcare. We report on expert feedback and user studies that indicate that our approach indeed reduces the time required to gain some business insight while maintaining the flexibility of a Linked Data-based integration approach.
C1  - New York, NY, USA
C3  - Proceedings of the 25th ACM Conference on Hypertext and Social Media
DA  - 2014///
PY  - 2014
DO  - 10.1145/2631775.2631807
SP  - 77
EP  - 87
PB  - Association for Computing Machinery
SN  - 978-1-4503-2954-5
UR  - https://doi.org/10.1145/2631775.2631807
KW  - care coordination
KW  - linked data
KW  - semantic web
KW  - data integration
KW  - data representation
ER  - 

TY  - CONF
TI  - Distributionally Robust Cycle and Chain Packing with Application to Organ Exchange
AU  - McElfresh, Duncan C.
AU  - Dickerson, John P.
AU  - Ren, Ke
AU  - Bidkhori, Hoda
T3  - WSC '21
AB  - We consider the cycle packing problems motivated by kidney exchange. In kidney exchange, patients with willing but incompatible donors enter into an organized market and trade donors in cyclic structures. Exchange programs attempt to match patients and donors utilizing the quality of matches. Current methods use a point estimate for the utility of a potential match that is drawn from an unknown distribution over possible true qualities. We apply the conditional value-at-risk paradigm to the size-constrained cycle and chain packing problem. We derive sample average approximation and distributionally-robust-optimization approaches to maximize the true quality of matched organs in the face of uncertainty over the quality of potential matches. We test our approach on the realistic kidney exchange data and show they outperform the state-of-the-art approaches. In the experiments, we use randomly generated exchange graphs resembling the structure of real exchanges, using anonymized data from the United Network for Organ Sharing.
C3  - Proceedings of the Winter Simulation Conference
DA  - 2022///
PY  - 2022
PB  - IEEE Press
ER  - 

TY  - CONF
TI  - Selling Multiple Items via Social Networks
AU  - Zhao, Dengji
AU  - Li, Bin
AU  - Xu, Junping
AU  - Hao, Dong
AU  - Jennings, Nicholas R.
T3  - AAMAS '18
AB  - We consider a market where a seller sells multiple units of a commodity in a social network. Each node/buyer in the social network can only directly communicate with her neighbours, i.e. the seller can only sell the commodity to her neighbours if she could not find a way to inform other buyers. In this paper, we design a novel promotion mechanism that incentivizes all buyers, who are aware of the sale, to invite all their neighbours to join the sale, even though there is no guarantee that their efforts will be paid. While traditional sale promotions such as sponsored search auctions cannot guarantee a positive return for the advertiser (the seller), our mechanism guarantees that the seller's revenue is better than not using the advertising. More importantly, the seller does not need to pay if the advertising is not beneficial to her.
C1  - Richland, SC
C3  - Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems
DA  - 2018///
PY  - 2018
SP  - 68
EP  - 76
PB  - International Foundation for Autonomous Agents and Multiagent Systems
KW  - mechanism design
KW  - information diffusion
KW  - revenue maximisation
ER  - 

TY  - CONF
TI  - Symbolic Parallel Adaptive Importance Sampling for Probabilistic Program Analysis
AU  - Luo, Yicheng
AU  - Filieri, Antonio
AU  - Zhou, Yuan
T3  - ESEC/FSE 2021
AB  - Probabilistic software analysis aims at quantifying the probability of a target event occurring during the execution of a program processing uncertain incoming data or written itself using probabilistic programming constructs. Recent techniques combine symbolic execution with model counting or solution space quantification methods to obtain accurate estimates of the occurrence probability of rare target events, such as failures in a mission-critical system. However, they face several scalability and applicability limitations when analyzing software processing with high-dimensional and correlated multivariate input distributions. In this paper, we present SYMbolic Parallel Adaptive Importance Sampling (SYMPAIS), a new inference method tailored to analyze path conditions generated from the symbolic execution of programs with high-dimensional, correlated input distributions. SYMPAIS combines results from importance sampling and constraint solving to produce accurate estimates of the satisfaction probability for a broad class of constraints that cannot be analyzed by current solution space quantification methods. We demonstrate SYMPAIS's generality and performance compared with state-of-the-art alternatives on a set of problems from different application domains.
C1  - New York, NY, USA
C3  - Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
DA  - 2021///
PY  - 2021
DO  - 10.1145/3468264.3468593
SP  - 1166
EP  - 1177
PB  - Association for Computing Machinery
SN  - 978-1-4503-8562-6
UR  - https://doi.org/10.1145/3468264.3468593
KW  - importance sampling
KW  - Markov chain Monte Carlo
KW  - probabilistic analysis
KW  - probabilistic programming
KW  - symbolic execution
ER  - 

TY  - JOUR
TI  - Unique Sharp Local Minimum in L1-Minimization Complete Dictionary Learning
AU  - Wang, Yu
AU  - Wu, Siqi
AU  - Yu, Bin
T2  - J. Mach. Learn. Res.
AB  - We study the problem of globally recovering a dictionary from a set of signals via l1- minimization. We assume that the signals are generated as i.i.d. random linear combinations of the K atoms from a complete reference dictionary D* ∈ RK×K, where the linear combination coefficients are from either a Bernoulli type model or exact sparse model. First, we obtain a necessary and sufficient norm condition for the reference dictionary D* to be a sharp local minimum of the expected l1 objective function. Our result substantially extends that of Wu and Yu (2018) and allows the combination coefficient to be non-negative. Secondly, we obtain an explicit bound on the region within which the objective value of the reference dictionary is minimal. Thirdly, we show that the reference dictionary is the unique sharp local minimum, thus establishing the first known global property of l1-minimization dictionary learning. Motivated by the theoretical results, we introduce a perturbation based test to determine whether a dictionary is a sharp local minimum of the objective function. In addition, we also propose a new dictionary learning algorithm based on Block Coordinate Descent, called DL-BCD, which is guaranteed to decrease the obective function monotonically. Simulation studies show that DL-BCD has competitive performance in terms of recovery rate compared to other state-of-the-art dictionary learning algorithms when the reference dictionary is generated from random Gaussian matrices.
DA  - 2020/01//
PY  - 2020
VL  - 21
IS  - 1
SN  - 1532-4435
KW  - dictionary learning
KW  - l1-minimization
KW  - local and global identifiability
KW  - nonconvex optimization
KW  - sharp local minimum
ER  - 

TY  - CONF
TI  - Indigenous Women Managing Pregnancy Complications in Rural Ecuador: Barriers and Opportunities to Enhance Antenatal Care
AU  - Verdezoto, Nervo
AU  - Carpio-Arias, Francisca
AU  - Carpio-Arias, Valeria
AU  - Mackintosh, Nicola
AU  - Eslambolchilar, Parisa
AU  - Delgado, Verónica
AU  - Andrade, Catherine
AU  - Vásconez, Galo
T3  - NordiCHI '20
AB  - Previous research has explored the potential use of digital health to support maternal health in the Global South highlighting the importance of understanding the socio-cultural context to inform system design. However, the experiences of indigenous women managing pregnancy complications in Latin America remain underexplored in HCI. We present a qualitative study with 25 indigenous pregnant women in an Ecuadorian rural community looking at their experiences during complications, their antenatal care visits and their access and use of technologies. Our findings highlight key barriers that hinder the use of antenatal care services and influence women's experiences managing complications. Based on the findings, we present opportunities for digital health centered on indigenous women to enhance antenatal care in rural Ecuador.
C1  - New York, NY, USA
C3  - Proceedings of the 11th Nordic Conference on Human-Computer Interaction: Shaping Experiences, Shaping Society
DA  - 2020///
PY  - 2020
DO  - 10.1145/3419249.3420141
PB  - Association for Computing Machinery
SN  - 978-1-4503-7579-5
UR  - https://doi.org/10.1145/3419249.3420141
KW  - Antenatal Care
KW  - Digital Health
KW  - Indigenous Women
KW  - Kichwas
KW  - Maternal Health
KW  - Pregnancy Complications
ER  - 

TY  - JOUR
TI  - Identifying Patterns in Medical Records through Latent Semantic Analysis
AU  - Gefen, David
AU  - Miller, Jake
AU  - Armstrong, Johnathon Kyle
AU  - Cornelius, Frances H.
AU  - Robertson, Noreen
AU  - Smith-McLallen, Aaron
AU  - Taylor, Jennifer A.
T2  - Commun. ACM
AB  - Text analysis can reveal patterns of association among medical terms and medical codes.
DA  - 2018/05//
PY  - 2018
DO  - 10.1145/3209086
VL  - 61
IS  - 6
SP  - 72
EP  - 77
SN  - 0001-0782
UR  - https://doi.org/10.1145/3209086
ER  - 

TY  - CONF
TI  - Simplifying Mobile Phone Food Diaries: Design and Evaluation of a Food Index-Based Nutrition Diary
AU  - Andrew, Adrienne H.
AU  - Borriello, Gaetano
AU  - Fogarty, James
T3  - PervasiveHealth '13
AB  - We describe the design and evaluation of POND, a Pattern-Oriented Nutrition Diary. POND is a mobile-phone food diary designed using a theory-driven approach to address a common challenge users report when using food diaries on mobile phones: the amount of effort required to create food entries in relation to the perceived self-benefit of self-monitoring food intake. The design allows users to create food entries either via a traditional database lookup or a streamlined '+1' approach. 24 people used POND to create predefined food entries. We found people preferred different approaches to creating entries, which reflected their self-reported nutrition concerns. This supports an argument for rethinking traditional approaches to designing food diaries.
C1  - Brussels, BEL
C3  - Proceedings of the 7th International Conference on Pervasive Computing Technologies for Healthcare
DA  - 2013///
PY  - 2013
DO  - 10.4108/icst.pervasivehealth.2013.252101
SP  - 260
EP  - 263
PB  - ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)
SN  - 978-1-936968-80-0
UR  - https://doi.org/10.4108/icst.pervasivehealth.2013.252101
KW  - mobile health and wellness
KW  - nutrition
KW  - self-monitoring
KW  - theory-driven design
ER  - 

TY  - JOUR
TI  - Allocation Problems in Ride-Sharing Platforms: Online Matching with Offline Reusable Resources
AU  - Dickerson, John P.
AU  - Sankararaman, Karthik A.
AU  - Srinivasan, Aravind
AU  - Xu, Pan
T2  - ACM Trans. Econ. Comput.
AB  - Bipartite-matching markets pair agents on one side of a market with agents, items, or contracts on the opposing side. Prior work addresses online bipartite-matching markets, where agents arrive over time and are dynamically matched to a known set of disposable resources. In this article, we propose a new model, Online Matching with (offline) Reusable Resources under Known Adversarial Distributions (OM-RR-KAD), in which resources on the offline side are reusable instead of disposable; that is, once matched, resources become available again at some point in the future. We show that our model is tractable by presenting an LP-based non-adaptive algorithm that achieves an online competitive ratio of ½-ϵ for any given constant ϵ &gt; 0. We also show that no adaptive algorithm can achieve a ratio of ½ + o(1) based on the same benchmark LP. Through a data-driven analysis on a massive openly available dataset, we show our model is robust enough to capture the application of taxi dispatching services and ride-sharing systems. We also present heuristics that perform well in practice.
DA  - 2021/06//
PY  - 2021
DO  - 10.1145/3456756
VL  - 9
IS  - 3
SN  - 2167-8375
UR  - https://doi.org/10.1145/3456756
KW  - matching markets
KW  - Online-matching
KW  - randomized algorithms
KW  - rideshare
ER  - 

TY  - CONF
TI  - Inferring Visual Behaviour from User Interaction Data on a Medical Dashboard
AU  - Yera, Ainhoa
AU  - Muguerza, Javier
AU  - Arbelaitz, Olatz
AU  - Perona, Iñigo
AU  - Keers, Richard
AU  - Ashcroft, Darren
AU  - Williams, Richard
AU  - Peek, Niels
AU  - Jay, Caroline
AU  - Vigo, Markel
T3  - DH '18
AB  - (its size and complexity) and its context of use. This results in user interfaces with a high-density of data that do not support optimal decision-making by clinicians. Anecdotal evidence indicates that clinicians demand the right amount of information to carry out their tasks. This suggests that adaptive user interfaces could be employed in order to cater for the information needs of the users and tackle information overload. Yet, since these information needs may vary, it is necessary first to identify and prioritise them, before implementing adaptations to the user interface. As gaze has long been known to be an indicator of interest, eye tracking allows us to unobtrusively observe where the users are looking, but it is not practical to use in a deployed system. Here, we address the question of whether we can infer visual behaviour on a medication safety dashboard through user interaction data. Our findings suggest that, there is indeed a relationship between the use of the mouse (in terms of clickstreams and mouse hovers) and visual behaviour in terms of cognitive load. We discuss the implications of this finding for the design of adaptive medical dashboards.
C1  - New York, NY, USA
C3  - Proceedings of the 2018 International Conference on Digital Health
DA  - 2018///
PY  - 2018
DO  - 10.1145/3194658.3194676
SP  - 55
EP  - 59
PB  - Association for Computing Machinery
SN  - 978-1-4503-6493-5
UR  - https://doi.org/10.1145/3194658.3194676
KW  - interaction analysis
KW  - medical dashboards
KW  - user modelling
ER  - 

TY  - CONF
TI  - Upwardly Abstracted Definition-Based Subontologies
AU  - Alghamdi, Ghadah
AU  - Schmidt, Renate A.
AU  - Del-Pinto, Warren
AU  - Gao, Yongsheng
T3  - K-CAP '21
AB  - In this paper, we present a method for extracting subontologies from $mathcalELH $ ontologies for a set of symbols. The approach is focused on the generation of upwardly abstracted definitions of concepts, which is a technique for computing definitions expressed using closest primitive ancestors. The subontologies returned by the method are evaluated for quality and compared to extracts computed with locality-based modularisation and uniform interpolation. Our subontology generation method produces promising results in terms of size and relevance to the needs of domain experts.
C1  - New York, NY, USA
C3  - Proceedings of the 11th on Knowledge Capture Conference
DA  - 2021///
PY  - 2021
DO  - 10.1145/3460210.3493564
SP  - 209
EP  - 216
PB  - Association for Computing Machinery
SN  - 978-1-4503-8457-5
UR  - https://doi.org/10.1145/3460210.3493564
KW  - ontology engineering
KW  - ontology modularisation
KW  - ontology summarisation
KW  - snomed ct
KW  - subontologies
ER  - 

TY  - CONF
TI  - Static Analysis for Probabilistic Programs: Inferring Whole Program Properties from Finitely Many Paths
AU  - Sankaranarayanan, Sriram
AU  - Chakarov, Aleksandar
AU  - Gulwani, Sumit
T3  - PLDI '13
AB  - We propose an approach for the static analysis of probabilistic programs that sense, manipulate, and control based on uncertain data. Examples include programs used in risk analysis, medical decision making and cyber-physical systems. Correctness properties of such programs take the form of queries that seek the probabilities of assertions over program variables. We present a static analysis approach that provides guaranteed interval bounds on the values (assertion probabilities) of such queries. First, we observe that for probabilistic programs, it is possible to conclude facts about the behavior of the entire program by choosing a finite, adequate set of its paths. We provide strategies for choosing such a set of paths and verifying its adequacy. The queries are evaluated over each path by a combination of symbolic execution and probabilistic volume-bound computations. Each path yields interval bounds that can be summed up with a "coverage" bound to yield an interval that encloses the probability of assertion for the program as a whole. We demonstrate promising results on a suite of benchmarks from many different sources including robotic manipulators and medical decision making programs.
C1  - New York, NY, USA
C3  - Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation
DA  - 2013///
PY  - 2013
DO  - 10.1145/2491956.2462179
SP  - 447
EP  - 458
PB  - Association for Computing Machinery
SN  - 978-1-4503-2014-6
UR  - https://doi.org/10.1145/2491956.2462179
KW  - probabilistic programming
KW  - symbolic execution
KW  - monte-carlo sampling
KW  - program verification
KW  - volume bounding
ER  - 

TY  - JOUR
TI  - Static Analysis for Probabilistic Programs: Inferring Whole Program Properties from Finitely Many Paths
AU  - Sankaranarayanan, Sriram
AU  - Chakarov, Aleksandar
AU  - Gulwani, Sumit
T2  - SIGPLAN Not.
AB  - We propose an approach for the static analysis of probabilistic programs that sense, manipulate, and control based on uncertain data. Examples include programs used in risk analysis, medical decision making and cyber-physical systems. Correctness properties of such programs take the form of queries that seek the probabilities of assertions over program variables. We present a static analysis approach that provides guaranteed interval bounds on the values (assertion probabilities) of such queries. First, we observe that for probabilistic programs, it is possible to conclude facts about the behavior of the entire program by choosing a finite, adequate set of its paths. We provide strategies for choosing such a set of paths and verifying its adequacy. The queries are evaluated over each path by a combination of symbolic execution and probabilistic volume-bound computations. Each path yields interval bounds that can be summed up with a "coverage" bound to yield an interval that encloses the probability of assertion for the program as a whole. We demonstrate promising results on a suite of benchmarks from many different sources including robotic manipulators and medical decision making programs.
DA  - 2013/06//
PY  - 2013
DO  - 10.1145/2499370.2462179
VL  - 48
IS  - 6
SP  - 447
EP  - 458
SN  - 0362-1340
UR  - https://doi.org/10.1145/2499370.2462179
KW  - probabilistic programming
KW  - symbolic execution
KW  - monte-carlo sampling
KW  - program verification
KW  - volume bounding
ER  - 

TY  - BOOK
TI  - ICIBE '22: Proceedings of the 8th International Conference on Industrial and Business Engineering
CY  - New York, NY, USA
DA  - 2022///
PY  - 2022
PB  - Association for Computing Machinery
SN  - 978-1-4503-9758-2
ER  - 

TY  - CONF
TI  - Resilience Ex Machina: Learning a Complex Medical Device for Haemodialysis Self-Treatment
AU  - Noble, Paul James
T3  - CHI '15
AB  - Resilience, the ability to bounce back or manage sufficiently despite ongoing adversity, has received considerable interest from several domains over the last two decades. A concept that is easily and widely applicable, it has evolved a variety of nuanced interpretations. Recent work in the systems theory and resilience engineering domains has moved towards some sharper definitions. This paper discusses and develops these definitions, and by contrasting with robustness, applies them as an approach for HCI evaluation. Ethnographic data from a hospital training environment is used to examine how patients learn to operate home haemodialysis devices, and how patients' safety is managed and maintained. This paper concludes that to improve recovery of adverse situations within the home, there is a necessity for design to support the acquirement of resilient reaction as a skill, in hand with temporal management. These ideas are developed in conjunction with the consideration of how technology can support, or nudge, the expression of resilient behaviours.
C1  - New York, NY, USA
C3  - Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems
DA  - 2015///
PY  - 2015
DO  - 10.1145/2702123.2702348
SP  - 4147
EP  - 4150
PB  - Association for Computing Machinery
SN  - 978-1-4503-3145-6
UR  - https://doi.org/10.1145/2702123.2702348
KW  - haemodialysis
KW  - human error
KW  - medical devices
KW  - resilience
ER  - 

TY  - CONF
TI  - Alice: Easy to Learn Interactive 3D Graphics
AU  - Pierce, Jeffrey S.
AU  - Christiansen, Kevin
AU  - Cosgrove, Dennis
AU  - Conway, Matt
AU  - Moskowitz, Dan
AU  - Stearns, Brian
AU  - Sturgill, Chris
AU  - Pausch, Randy
T3  - CHI '98
C1  - New York, NY, USA
C3  - CHI 98 Conference Summary on Human Factors in Computing Systems
DA  - 1998///
PY  - 1998
DO  - 10.1145/286498.286512
SP  - 26
EP  - 27
PB  - Association for Computing Machinery
SN  - 1-58113-028-7
UR  - https://doi.org/10.1145/286498.286512
KW  - 3D graphics
KW  - rapid prototyping
KW  - usability engineering
KW  - virtual reality
ER  - 

TY  - CONF
TI  - Finding Effective Compilation Sequences
AU  - Almagor, L.
AU  - Cooper, Keith D.
AU  - Grosul, Alexander
AU  - Harvey, Timothy J.
AU  - Reeves, Steven W.
AU  - Subramanian, Devika
AU  - Torczon, Linda
AU  - Waterman, Todd
T3  - LCTES '04
AB  - Most modern compilers operate by applying a fixed, program-independent sequence of optimizations to all programs. Compiler writers choose a single "compilation sequence", or perhaps a couple of compilation sequences. In choosing a sequence, they may consider performance of benchmarks or other important codes. These sequences are intended as general-purpose tools, accessible through command-line flags such as -O2 and -O3.Specific compilation sequences make a significant difference in the quality of the generated code, whether compiling for speed, for space, or for other metrics. A single universal compilation sequence does not produce the best results over all programs [8, 10, 29, 32]. Finding an optimal program-specific compilation sequence is difficult because the space of potential sequences is huge and the interactions between optimizations are poorly understood. Moreover, there is no systematic exploration of the costs and benefits of searching for good (i.e., within a certain percentage of optimal) program-specific compilation sequences.In this paper, we perform a large experimental study of the space of compilation sequences over a set of known benchmarks, using our prototype adaptive compiler. Our goal is to characterize these spaces and to determine if it is cost-effective to construct custom compilation sequences. We report on five exhaustive enumerations which demonstrate that 80% of the local minima in the space are within 5 to 10% of the optimal solution. We describe three algorithms tailored to search such spaces and report on experiments that use these algorithms to find good compilation sequences. These experiments suggest that properties observed in the enumerations hold for larger search spaces and larger programs. Our findings indicate that for the cost of 200 to 4,550 compilations, we can find custom sequences that are 15 to 25% better than the human-designed fixed-sequence originally used in our compiler.
C1  - New York, NY, USA
C3  - Proceedings of the 2004 ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems
DA  - 2004///
PY  - 2004
DO  - 10.1145/997163.997196
SP  - 231
EP  - 239
PB  - Association for Computing Machinery
SN  - 1-58113-806-7
UR  - https://doi.org/10.1145/997163.997196
KW  - adaptive compilers
KW  - learning models
ER  - 

TY  - JOUR
TI  - Finding Effective Compilation Sequences
AU  - Almagor, L.
AU  - Cooper, Keith D.
AU  - Grosul, Alexander
AU  - Harvey, Timothy J.
AU  - Reeves, Steven W.
AU  - Subramanian, Devika
AU  - Torczon, Linda
AU  - Waterman, Todd
T2  - SIGPLAN Not.
AB  - Most modern compilers operate by applying a fixed, program-independent sequence of optimizations to all programs. Compiler writers choose a single "compilation sequence", or perhaps a couple of compilation sequences. In choosing a sequence, they may consider performance of benchmarks or other important codes. These sequences are intended as general-purpose tools, accessible through command-line flags such as -O2 and -O3.Specific compilation sequences make a significant difference in the quality of the generated code, whether compiling for speed, for space, or for other metrics. A single universal compilation sequence does not produce the best results over all programs [8, 10, 29, 32]. Finding an optimal program-specific compilation sequence is difficult because the space of potential sequences is huge and the interactions between optimizations are poorly understood. Moreover, there is no systematic exploration of the costs and benefits of searching for good (i.e., within a certain percentage of optimal) program-specific compilation sequences.In this paper, we perform a large experimental study of the space of compilation sequences over a set of known benchmarks, using our prototype adaptive compiler. Our goal is to characterize these spaces and to determine if it is cost-effective to construct custom compilation sequences. We report on five exhaustive enumerations which demonstrate that 80% of the local minima in the space are within 5 to 10% of the optimal solution. We describe three algorithms tailored to search such spaces and report on experiments that use these algorithms to find good compilation sequences. These experiments suggest that properties observed in the enumerations hold for larger search spaces and larger programs. Our findings indicate that for the cost of 200 to 4,550 compilations, we can find custom sequences that are 15 to 25% better than the human-designed fixed-sequence originally used in our compiler.
DA  - 2004/06//
PY  - 2004
DO  - 10.1145/998300.997196
VL  - 39
IS  - 7
SP  - 231
EP  - 239
SN  - 0362-1340
UR  - https://doi.org/10.1145/998300.997196
KW  - adaptive compilers
KW  - learning models
ER  - 

TY  - CONF
TI  - Is It Feasible to Combine Non-Standard Exercise Prescriptions with Novel Smartphone Adaptive Coaching Systems to Improve Physical Activity and Health Related Outcomes in Type 2 Diabetes Mellitus?
AU  - Byrne, Hugh
AU  - Caulfield, Brian
AU  - Lowery, Madeleine
AU  - Thompson, Chris J.
AU  - Smith, Diarmuid
AU  - Griffin, Margaret
AU  - De Vito, Giuseppe
T3  - PervasiveHealth '18
AB  - High levels of physical activity are paramount in ensuring individuals maintain or improve health outcomes, function and quality of life. However, physical activity continues to be low worldwide and rates of conditions associated with sedentary lifestyles are increasing. Furthermore, adherence to exercise prescriptions in individuals whose conditions are managed more effectively with physical activity is poor. This paper examines the feasibility of a novel adaptive coaching smartphone application in yielding a high adherence to an exercise programme, also specifically designed to improve physical activity habits of previously sedentary individuals with type 2 diabetes mellitus (T2DM) by encouraging adherence. The findings of the current study suggest that combining the specific novel exercise programme used in this study with a novel adaptive coaching smartphone application in sedentary individuals with T2DM is a feasible intervention. The findings also suggest that the combined intervention can yield high adherences, increase overall physical activity levels and improve health related outcomes.
C1  - New York, NY, USA
C3  - Proceedings of the 12th EAI International Conference on Pervasive Computing Technologies for Healthcare
DA  - 2018///
PY  - 2018
DO  - 10.1145/3240925.3240977
SP  - 356
EP  - 359
PB  - Association for Computing Machinery
SN  - 978-1-4503-6450-8
UR  - https://doi.org/10.1145/3240925.3240977
KW  - Monitoring
KW  - Adaptive coaching
KW  - Exercise
KW  - Type 2 diabetes mellitus
ER  - 

TY  - CONF
TI  - A Rights Management Approach to Protection of Privacy in a Cloud of Electronic Health Records
AU  - Jafari, Mohammad
AU  - Safavi-Naini, Reihaneh
AU  - Sheppard, Nicholas Paul
T3  - DRM '11
AB  - A patient-centric DRM approach is proposed for protecting privacy of health records stored in a cloud storage based on the patient's preferences and without the need to trust the service provider. Contrary to the current server-side access control solutions, this approach protects the privacy of records from the service provider, and also controls the usage of data after it is released to an authorized user.
C1  - New York, NY, USA
C3  - Proceedings of the 11th Annual ACM Workshop on Digital Rights Management
DA  - 2011///
PY  - 2011
DO  - 10.1145/2046631.2046637
SP  - 23
EP  - 30
PB  - Association for Computing Machinery
SN  - 978-1-4503-1005-5
UR  - https://doi.org/10.1145/2046631.2046637
KW  - cloud
KW  - digital rights management
KW  - health records
ER  - 

TY  - CONF
TI  - Measurement of Specific Gravity, Urobilinogen, Blood, Protein and PH Level of Urine Samples Using Raspberry Pi Based Portable Urine Test Strip Analyzer
AU  - Paglinawan, Arnold C.
AU  - Cruz, Febus Reidj G.
AU  - Valiente, Leonardo D.
AU  - Mendoza, Jesus Paolo T.
AU  - Chanliongco, Arnold M.
AU  - Torres, Jerome B.
AU  - Tungol, Rachelle Geleen S.
T3  - ICBET 2020
AB  - Urinalysis has been a helpful tool in detecting some common diseases. It has been used to diagnose conditions such as urinary tract infections, kidney disorders, liver problems, and other metabolic conditions. Regular urinalysis may be done to detect early signs of diseases and prevent it, but it is also time consuming and costly. Urine has several components and some of the parameters can be examined through the use of a urine test strip analysis. This paper presents a portable raspberry-pi based system that is designed to analyze the key parameters of urine namely, specific gravity, urobilinogen, blood, protein, and pH level. This study is focused in determining the key parameters of urine samples aforementioned which are essential in diagnosing early signs of disorders. Image processing was used in order to properly identify the positioning of each pad in the strip and to obtain the color change evaluated in terms of HSV color space analysis. Statistical analysis was performed in order to compare the values measured by the urine test strip analyzer to the actual laboratory urinalysis test. Using T-test, the results show that there is no significant difference between the values measured by the urine test strip analyzer and the actual laboratory urinalysis.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 10th International Conference on Biomedical Engineering and Technology
DA  - 2020///
PY  - 2020
DO  - 10.1145/3397391.3397414
SP  - 58
EP  - 63
PB  - Association for Computing Machinery
SN  - 978-1-4503-7724-9
UR  - https://doi.org/10.1145/3397391.3397414
KW  - Image processing
KW  - Raspberry-pi
KW  - T-test
KW  - Urinalysis
ER  - 

TY  - CONF
TI  - Fuzzy Tsukamoto Membership Function Optimization Using PSO to Predict Diabetes Mellitus Risk Level
AU  - Pradini, Risqy S.
AU  - Previana, Cantika N.
AU  - Bachtiar, Fitra A.
T3  - SIET '20
AB  - Diabetes Mellitus (DM) is known as the silent killer because the sufferer often goes unnoticed and when it is known, complications usually already occur. The number of people with DM in Indonesia is a lot, which may increase the health costs and may cause trouble for health workers. Therefore, researchers conducted research to make predictions of the risk level of DM. By creating a prediction model the risk of DM can be identified in the early stage. The prediction of DM risk is based on the input variable consisting of age, body mass index and blood pressure (systolic) which are proven to be the basis for determining the risk ratio of DM and the output variable is the level of diabetes risk (low, high). This study uses a combination of Fuzzy Tsukamoto and PSO to predict the risk level of DM. The membership function for Fuzzy Tsukamoto will be optimized with PSO. Membership optimization is expected to increase the prediction results to be more accurate. Based on user data that has been processed using the proposed method, the prediction results are more accurate than the data processed using only Fuzzy Tsukamoto. The MSE value generated between the actual data and the proposed method is 0.012. The resulting MSE value is very small, so this proves the high level of accuracy.
C1  - New York, NY, USA
C3  - Proceedings of the 5th International Conference on Sustainable Information Engineering and Technology
DA  - 2021///
PY  - 2021
DO  - 10.1145/3427423.3427451
SP  - 101
EP  - 106
PB  - Association for Computing Machinery
SN  - 978-1-4503-7605-1
UR  - https://doi.org/10.1145/3427423.3427451
KW  - diabetes mellitus
KW  - fuzzy tsukamoto
KW  - membership function
KW  - optimization
KW  - PSO
ER  - 

TY  - JOUR
TI  - Adaptively Weighted Top-N Recommendation for Organ Matching
AU  - Shojaee, Parshin
AU  - Chen, Xiaoyu
AU  - Jin, Ran
T2  - ACM Trans. Comput. Healthcare
AB  - Reducing the shortage of organ donations to meet the demands of patients on the waiting list has being a major challenge in organ transplantation. Because of the shortage, organ matching decision is the most critical decision to assign the limited viable organs to the most “suitable” patients. Currently, organ matching decisions are only made by matching scores calculated via scoring models, which are built by the first principles. However, these models may disagree with the actual post-transplantation matching performance (e.g., patient's post-transplant quality of life (QoL) or graft failure measurements). In this paper, we formulate the organ matching decision-making as a top-N recommendation problem and propose an Adaptively Weighted Top-N Recommendation (AWTR) method. AWTR improves performance of the current scoring models by using limited actual matching performance in historical datasets as well as the collected covariates from organ donors and patients. AWTR sacrifices the overall recommendation accuracy by emphasizing the recommendation and ranking accuracy for top-N matched patients. The proposed method is validated in a simulation study, where KAS [60] is used to simulate the organ-patient recommendation response. The results show that our proposed method outperforms seven state-of-the-art top-N recommendation benchmark methods.
DA  - 2021/10//
PY  - 2021
DO  - 10.1145/3469657
VL  - 3
IS  - 1
SN  - 2691-1957
UR  - https://doi.org/10.1145/3469657
KW  - matrix completion
KW  - Learning to rank
KW  - organ matching
KW  - organ transplantation
KW  - top-N recommendation
ER  - 

TY  - CONF
TI  - A Framework for Evidence-Based Health Care Incentives Simulation
AU  - Bigus, Joseph P.
AU  - Chen-Ritzo, Ching-Hua
AU  - Sorrentino, Robert
T3  - WSC '11
AB  - We present a general simulation framework designed for modeling incentives in a health care delivery system. This first version of the framework focuses on representing provider incentives. Key framework components are described in detail, and we provide an overview of how data-driven analytic methods can be integrated with this framework to enable evidence-based simulation. The software implementation of a simple simulation model based on this framework is also presented.
C3  - Proceedings of the Winter Simulation Conference
DA  - 2011///
PY  - 2011
SP  - 1103
EP  - 1116
PB  - Winter Simulation Conference
ER  - 

TY  - CONF
TI  - Parallel Sorting Algorithms for Insertion Shell and Heapsort on the HEP (Abstract)
AU  - Crawford, Kelly D.
AU  - Wainwright, Roger L.
T3  - CSC '86
C1  - New York, NY, USA
C3  - Proceedings of the 1986 ACM Fourteenth Annual Conference on Computer Science
DA  - 1986///
PY  - 1986
DO  - 10.1145/324634.325298
SP  - 481
PB  - Association for Computing Machinery
SN  - 0-89791-177-6
UR  - https://doi.org/10.1145/324634.325298
ER  - 

TY  - CONF
TI  - Facilitating Medication Adherence and Eliminating Therapeutic Inertia Using Wireless Technology: Proof of Concept Findings with Uncontrolled Hypertensives and Kidney Transplant Recipients
AU  - McGillicuddy, John W.
AU  - Gregoski, Mathew J.
AU  - Brunner-Jackson, Brenda M.
AU  - Weiland, Ana K.
AU  - Patel, Sachin K.
AU  - Rock, Rebecca A.
AU  - Treiber, Eveline M.
AU  - Davidson, Lydia K.
AU  - Treiber, Frank A.
T3  - WH '12
AB  - Effective and efficient management of chronic illness remains a significant clinical problem. To improve chronic illness management, two obstacles that must be overcome are patient non-adherence to medication regimens and provider therapeutic inertia (failure to respond in timely manner to clinical data). Using an iterative approach, behavioral theory was used to develop a mobile health (mHealth) medication and blood pressure self-management system that was patient and provider centered. Electronic medication trays provided reminder signals and smart phone text messages reminded patients to measure blood pressures using a Bluetooth-enabled monitor. Patients received mobile phone-delivered personalized motivational and reinforcement messages based upon adherence levels to these regimens. Two 3-month proof of concept randomized control trials were conducted with 2 patient groups; 1) Hispanics with uncontrolled essential hypertension (n=6), and 2) patients with hypertension after kidney transplantation. (n=6). Hispanic patients who received the mHealth intervention all exhibited significant improvements in both medication adherence and reductions in resting and 24-hour blood pressures during the trial and at 3-month follow-up, as compared to the control group.The still ongoing kidney transplant trial has shown that recipients randomized to the mHealth intervention have demonstrated significant improvements in medication adherence and reduced blood pressure two months into the trial. Following completion of both studies, patient and provider focus groups will allow further iterative refinement of the mHealth system and a feasibility trial of larger scale and longer duration.
C1  - New York, NY, USA
C3  - Proceedings of the Conference on Wireless Health
DA  - 2012///
PY  - 2012
DO  - 10.1145/2448096.2448108
PB  - Association for Computing Machinery
SN  - 978-1-4503-1760-3
UR  - https://doi.org/10.1145/2448096.2448108
KW  - mHealth
KW  - kidney transplantation
KW  - blood pressure control
KW  - clinical inertia
KW  - essential hypertension
KW  - medication adherence
KW  - self-determination theory
ER  - 

TY  - CONF
TI  - Predicting the Behaviour of the Renal Transplant Waiting List in the País Valencià (Spain) Using Simulation Modeling
AU  - Abellán, Juan J.
AU  - Armero, Carmen
AU  - Conesa, David
AU  - Pérez-Panadés, Jordi
AU  - Martínez-Beneito, Miguel A.
AU  - Zurriaga, Oscar
AU  - García-Blasco, María J.
AU  - Vanaclocha, Herme
T3  - WSC '04
AB  - A discrete event simulation model has been set up in order to analyze the renal transplant waiting list in the País Valencià, one of the autonomous regions in which Spain is divided. The model combines the information of the arrival of the patients onto the list and the process of donations, which also depend on the number of kidneys provided by each donor. Bayesian inference has been used to take into account the uncertainty about the parameters of the input distributions (acceptance, donation and transplantation rates). After validating the model, predictions about the future behaviour of the waiting list have been done. Results indicate a decrease in the size of the waiting list in a short and middle term. Comparison with other strategies of simulation has been done in order to confirm the problem of underestimation of the variance of the expected simulation output.
C3  - Proceedings of the 36th Conference on Winter Simulation
DA  - 2004///
PY  - 2004
SP  - 1969
EP  - 1974
PB  - Winter Simulation Conference
SN  - 0-7803-8786-4
ER  - 

TY  - CONF
TI  - Automating Warm-up Length Estimation
AU  - Hoad, Kathryn
AU  - Robinson, Stewart
AU  - Davies, Ruth
T3  - WSC '08
AB  - There are two key issues in assuring the accuracy of estimates of performance obtained from a simulation model. The first is the removal of any initialisation bias, the second is ensuring that enough output data is produced to obtain an accurate estimate of performance. This paper is concerned with the first issue, and more specifically warm-up estimation. A continuing research project is described that aims to produce an automated procedure, for inclusion into commercial simulation software, for estimating the length of warm-up and hence removing initialisation bias from simulation output data.
C3  - Proceedings of the 40th Conference on Winter Simulation
DA  - 2008///
PY  - 2008
SP  - 532
EP  - 540
PB  - Winter Simulation Conference
SN  - 978-1-4244-2708-6
ER  - 

TY  - CONF
TI  - An Efficient Static Analysis Algorithm to Detect Redundant Memory Operations
AU  - Cooper, Keith D.
AU  - Xu, Li
T3  - MSP '02
AB  - As memory system performance becomes an increasingly dominant factor in overall system performance, it is important to optimize programs for memory related operations. This paper concerns static analysis to detect redundant memory operations and enable other compiler transformations to remove such redundant operations.We present an extended global value numbering algorithm to detect redundant memory operations. The key of the new algorithm is a novel SSA-based representation for memory state which allows accurate reasoning about memory state. Using this representation, the algorithm can trace values through memory operations to detect equivalence in the same way that it traces them through register-based scalar operations. Thus it discovers both redundancy involving scalar values and redundancy involving memory operations. The redundancy relation detected by the algorithm can then be used by traditional redundancy elimination transformations to remove those redundant operations.Experiments using a suite of realistic applications demonstrate the algorithm is powerful and fast. In practice, it has essentially linear time complexity.
C1  - New York, NY, USA
C3  - Proceedings of the 2002 Workshop on Memory System Performance
DA  - 2002///
PY  - 2002
DO  - 10.1145/773146.773049
SP  - 97
EP  - 107
PB  - Association for Computing Machinery
SN  - 978-1-4503-7368-5
UR  - https://doi.org/10.1145/773146.773049
ER  - 

TY  - JOUR
TI  - An Efficient Static Analysis Algorithm to Detect Redundant Memory Operations
AU  - Cooper, Keith D.
AU  - Xu, Li
T2  - SIGPLAN Not.
AB  - As memory system performance becomes an increasingly dominant factor in overall system performance, it is important to optimize programs for memory related operations. This paper concerns static analysis to detect redundant memory operations and enable other compiler transformations to remove such redundant operations.We present an extended global value numbering algorithm to detect redundant memory operations. The key of the new algorithm is a novel SSA-based representation for memory state which allows accurate reasoning about memory state. Using this representation, the algorithm can trace values through memory operations to detect equivalence in the same way that it traces them through register-based scalar operations. Thus it discovers both redundancy involving scalar values and redundancy involving memory operations. The redundancy relation detected by the algorithm can then be used by traditional redundancy elimination transformations to remove those redundant operations.Experiments using a suite of realistic applications demonstrate the algorithm is powerful and fast. In practice, it has essentially linear time complexity.
DA  - 2002/06//
PY  - 2002
DO  - 10.1145/773039.773049
VL  - 38
IS  - 2 supplement
SP  - 97
EP  - 107
SN  - 0362-1340
UR  - https://doi.org/10.1145/773039.773049
ER  - 

TY  - CONF
TI  - Re-Designing A Mobile App with Patients with Discordant Chronic Comorbidities: Usability Study
AU  - Li, Tianshuo
AU  - Haynes, Mason
AU  - Rucker, Bradley E
T3  - CHI EA '21
AB  - Patients with complex conditions and treatment plans often find it challenging to communicate with multiple providers and to prioritize various management tasks. The challenge is even greater for patients with discordant chronic comorbidities (DCCs), a situation where a patient has conditions that have unrelated and/or conflicting treatment plans. We identified possible needs and designed the Apps that addressed those needs (including: goal setting, ease of use, monitoring, and motivation). We then tested this App with patients with DCCs to see if those needs were addressed. We present results from that a usability study that highlight the design preference of patients with DCCs.
C1  - New York, NY, USA
C3  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
DA  - 2021///
PY  - 2021
DO  - 10.1145/3411763.3451518
PB  - Association for Computing Machinery
SN  - 978-1-4503-8095-9
UR  - https://doi.org/10.1145/3411763.3451518
KW  - Mobile Application
KW  - Discordant Chronic Comorbidities
KW  - Multiple Chronic Conditions
KW  - Patients Care and Support
KW  - User ability study
ER  - 

TY  - CONF
TI  - Simulating Medical Decision Trees with Random Variable Parameters
AU  - Dittus, Robert S.
AU  - Klein, Robert W.
T3  - WSC '92
C1  - New York, NY, USA
C3  - Proceedings of the 24th Conference on Winter Simulation
DA  - 1992///
PY  - 1992
DO  - 10.1145/167293.167828
SP  - 1050
EP  - 1056
PB  - Association for Computing Machinery
SN  - 0-7803-0798-4
UR  - https://doi.org/10.1145/167293.167828
ER  - 

TY  - JOUR
TI  - Tail Recursion Elimination in Deductive Databases
AU  - Ross, Kenneth A.
T2  - ACM Trans. Database Syst.
AB  - We consider an optimization technique for deductive and relational databases. The optimization technique is an extension of the magic templates rewriting, and it can improve the performance of query evaluation by not materializing the extension of intermediate views. Standard relational techniques, such as unfolding embedded view definitions, do not apply to recursively defined views, and so alternative techniques are necessary. We demonstrate the correctness of our rewriting. We define a class of “nonrepeating” view definitions, and show that for certain queries our rewriting performs at least as well as magic templates on nonrepeating views, and often much better. A syntactically recognizable property, called “weak right-linearity”, is proposed. Weak right-linearity is a sufficient condition for nonrepetition and is more general than right-linearity. Our technique gives the same benefits as right-linear evaluation of right-linear views, while applying to a significantly more general class of views.
DA  - 1996/06//
PY  - 1996
DO  - 10.1145/232616.232628
VL  - 21
IS  - 2
SP  - 208
EP  - 237
SN  - 0362-5915
UR  - https://doi.org/10.1145/232616.232628
KW  - deductive databases
KW  - magic sets
KW  - query optimization
KW  - tail recursion
ER  - 

TY  - CONF
TI  - Meeting the Challenges of Data-Intensive Science
AU  - Graves, Sara
T3  - CKD '11
AB  - Meeting the needs of Data-Intensive Science requires addressing many challenges in the entire life cycle of data including analysis, exploitation and dissemination. The broadening range of data sources often used in multi-discipline and multi-scale science studies present challenges to data providers as well as data consumers. Roles can become blurred as well as where responsibilities fall for data stewardship, curation, metadata collection and the many other necessary functions involved in enhancing Data-Intensive Science. Innovative approaches for data discovery and searching, as well as extending the use of semantics are important technologies to address these challenges. Data science is an emerging discipline that is evolving and expanding to meet some of the challenges of Data-Intensive Science, along with science collaboration portals providing social and professional networking functionality for accelerating and opening scientific exchange and interoperability in many areas.
C1  - New York, NY, USA
C3  - Proceedings of the 2011 Workshop on Climate Knowledge Discovery
DA  - 2011///
PY  - 2011
DO  - 10.1145/2110230.2110236
SP  - 4
PB  - Association for Computing Machinery
SN  - 978-1-4503-1181-6
UR  - https://doi.org/10.1145/2110230.2110236
KW  - meeting the challenges of data-intensive science
ER  - 

TY  - BOOK
TI  - CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
CY  - New York, NY, USA
DA  - 2021///
PY  - 2021
PB  - Association for Computing Machinery
SN  - 978-1-4503-8095-9
ER  - 

TY  - CHAP
TI  - Exploring Advantages in the Waiting List for Organ Donations
AU  - Harvey, Christine
AU  - Thompson, James R.
T2  - Proceedings of the 2016 Winter Simulation Conference
AB  - The waiting list for organ transplants is a complex system that affects the lives of thousands of Americans. The current policies in the United States allow patients to register at multiple Donor Service Areas (DSAs) provided they have physician approval and can cover the costs of any additional testing through insurance or personal means. This practice gives rise to ethical concerns, especially among those who believe it allows the wealthy to take unfair advantage of the system. We develop an agent-based, discrete event model that simulates the practice of multiple listings in transplant waiting list queues to explore the effects on the overall transplant system. Our analysis shows that although there are no major impacts at the national or global level, there are potential consequences at the local DSA level depending on the heterogeneity of the DSAs involved.
DA  - 2016///
PY  - 2016
SP  - 2006
EP  - 2017
PB  - IEEE Press
SN  - 978-1-5090-4484-9
ER  - 

TY  - JOUR
TI  - EBP: An Ear-Worn Device for Frequent and Comfortable Blood Pressure Monitoring
AU  - Bui, Nam
AU  - Pham, Nhat
AU  - Barnitz, Jessica Jacqueline
AU  - Zou, Zhanan
AU  - Nguyen, Phuc
AU  - Truong, Hoang
AU  - Kim, Taeho
AU  - Farrow, Nicholas
AU  - Nguyen, Anh
AU  - Xiao, Jianliang
AU  - Deterding, Robin
AU  - Dinh, Thang
AU  - Vu, Tam
T2  - Commun. ACM
AB  - Frequent blood pressure monitoring is the key to diagnosis and treatments of many severe diseases. However, the conventional ambulatory methods require patients to carry a blood pressure (BP) monitoring device for 24 h and conduct the measurement every 10–15 min. Despite their extensive usage, wearing the wrist/arm-based BP monitoring device for a long time has a significant impact on users' daily activities. To address the problem, we developed eBP to measure blood pressure (BP) from inside user's ear aiming to minimize the measurement's impact on users' normal activities although maximizing its comfort level.The key novelty of eBP includes (1) a light-based inflatable pulse sensor which goes inside the ear, (2) a digital air pump with a fine controller, and (3) BP estimation algorithms that eliminate the need of blocking the blood flow inside the ear.Through the comparative study of 35 subjects, eBP can achieve the average error of 1.8 mmHg for systolic (high-pressure value) and -3.1 mmHg for diastolic (low-pressure value) with the standard deviation error of 7.2 mmHg and 7.9 mmHg, respectively. These results satisfy the FDA's AAMI standard, which requires a mean error of less than 5 mmHg and a standard deviation of less than 8 mmHg.
DA  - 2021/07//
PY  - 2021
DO  - 10.1145/3470446
VL  - 64
IS  - 8
SP  - 118
EP  - 125
SN  - 0001-0782
UR  - https://doi.org/10.1145/3470446
ER  - 

TY  - CONF
TI  - ANNOD: A Navigator of Natural-Language Organized (Textual) Data
AU  - Williamson, Robert E.
T3  - SIGIR '85
AB  - ANNOD is the name of a system developed at the National Library of Medicine (NLM), which implements a set of linguistic and empirical techniques that permit retrieval of natural language information in response to natural language queries. The system is based on Dr. Gerard Salton's SMART [1] document retrieval system and is presently implemented on a mini-computer as part of an Interactive TExt Management System, ITEMS.[2] Actual experience with retrieval of information from NLM's Hepatitis Knowledge Base (HKB), an encyclopedic hierarchical, full-text file, is presented. The techniques used in ANNOD include: automatic stemming of words, common word deletion, thesaurus expansion, a complex empirical matching (ranking) algorithm (similarity measure), and techniques expressly designed to permit rapid response in a mini-computer environment. Preliminary testing demonstrates high efficiency in identifying portions of a text which are relevant to users.
C1  - New York, NY, USA
C3  - Proceedings of the 8th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
DA  - 1985///
PY  - 1985
DO  - 10.1145/253495.253532
SP  - 252
EP  - 266
PB  - Association for Computing Machinery
SN  - 0-89791-159-8
UR  - https://doi.org/10.1145/253495.253532
ER  - 

TY  - CONF
TI  - Privacy-Preserving and Optimal Interval Release for Disease Susceptibility
AU  - Kusano, Kosuke
AU  - Takeuchi, Ichiro
AU  - Sakuma, Jun
T3  - ASIA CCS '17
AB  - In this paper, we consider the problem of privacy-preserving release of function outputs that take private information as input. Disease susceptibilities are known to be associated with clinical features (e.g., age, sex) as well as genetic features represented by SNPs of individuals. Releasing outputs are not privacy-preserving if the private input can be uniquely identified by probabilistic inference using the outputs. To release useful outputs with preserving privacy, we present a mechanism that releases an interval as output, instead of an output value. We suppose adversaries perform probabilistic inference using released outputs to sharpen the posterior distribution of the target attributes. Then, our mechanism has two significant properties. First, when our mechanism provides the output, the increase of the adversary's posterior on any input attribute is upper-bounded by a prescribed level. Second, under this privacy constraint, the mechanism can provide the narrowest (optimal) interval that includes the true output. Building such a mechanism is often intractable. We formulate the design of the mechanism as a discrete constraint optimization problem so that it is solvable in a practical computation time. We also propose an algorithm to obtain the optimal mechanism based on dynamic programming. After applying our mechanism to release disease susceptibilities of obesity, we demonstrate that our mechanism performs better than existing methods in terms of privacy and utility.
C1  - New York, NY, USA
C3  - Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security
DA  - 2017///
PY  - 2017
DO  - 10.1145/3052973.3053021
SP  - 532
EP  - 545
PB  - Association for Computing Machinery
SN  - 978-1-4503-4944-4
UR  - https://doi.org/10.1145/3052973.3053021
KW  - disease susceptibility
KW  - genome
KW  - input inference
KW  - interval publication
KW  - privacy
ER  - 

TY  - CONF
TI  - Understanding Challenges in Prehabilitation for Patients with Multiple Chronic Conditions
AU  - Zhu, Haining
AU  - Moffa, Zachary
AU  - Wang, Xiying
AU  - Abdullah, Saeed
AU  - Julaiti, Juxihong
AU  - Carroll, John
T3  - PervasiveHealth '18
AB  - Little has been done to understand what challenges surgical patients face before surgery. Specifically, patients with multiple chronic conditions (MCC) often require prehabilitation–pre-surgery steps to improve baseline levels of health parameters required for surgery. Prehabilitation can improve health-care outcomes, reduce costs, and decrease readmissions. However, prehabilitation can be challenging for MCC patients since they need to balance chronic conditions with surgical preparation. Unsuccessful prehabilitation adherence can result in serious medical consequences including postponed surgeries and postoperative complications. In this work, we explore prehabiliation challenges faced by patients with MCC and identify opportunities for technological interventions by conducting a qualitative study of online health forum posts from 154 users. Using this data, we identified categories of patients' needs and challenges during prehabilitation.Based on our findings, we propose design recommendations to better support prehabilitation for patients with MCC.
C1  - New York, NY, USA
C3  - Proceedings of the 12th EAI International Conference on Pervasive Computing Technologies for Healthcare
DA  - 2018///
PY  - 2018
DO  - 10.1145/3240925.3240959
SP  - 138
EP  - 147
PB  - Association for Computing Machinery
SN  - 978-1-4503-6450-8
UR  - https://doi.org/10.1145/3240925.3240959
KW  - chronic condition
KW  - forum
KW  - Perioperative care
KW  - prehabilitation
KW  - rehabilitation
KW  - surgery
ER  - 

TY  - CONF
TI  - Five Decades of Healthcare Simulation
AU  - Brailsford, Sally C
AU  - Carter, Michael W
AU  - Jacobson, Sheldon H
T3  - WSC '17
AB  - In this paper we have not attempted to produce any kind of systematic review of simulation in healthcare to compete with the dozen (at least) excellent and comprehensive survey papers on this topic that already exist. We begin with a glance back at the early days of Wintersim, but then proceed, in line with the theme of this special track, to reflect on general developments in healthcare simulation over the years from our own personal perspectives. We include some memories and reflections by several pioneers in this area, both academics and healthcare practitioners, on both sides of the Atlantic. We also asked four current simulation modelers, who all specialize in healthcare applications but from very diverse perspectives, to reflect on their experiences. We endeavor to identify some common or recurring themes across the years, and end with a glimpse into the future.
C3  - Proceedings of the 2017 Winter Simulation Conference
DA  - 2017///
PY  - 2017
PB  - IEEE Press
SN  - 978-1-5386-3427-1
ER  - 

TY  - CONF
TI  - Basics and Visual Analytics of Climate Networks
AU  - Nocke, Thomas
T3  - CKD '11
AB  - This paper/talk discusses potentials of using visual analytics technology for climate network analysis. It presents basic definitions, challenges arising in this field, suitable methods &amp; tools and key references.
C1  - New York, NY, USA
C3  - Proceedings of the 2011 Workshop on Climate Knowledge Discovery
DA  - 2011///
PY  - 2011
DO  - 10.1145/2110230.2110233
SP  - 2
PB  - Association for Computing Machinery
SN  - 978-1-4503-1181-6
UR  - https://doi.org/10.1145/2110230.2110233
KW  - climate research
KW  - networks
KW  - visual analytics
KW  - visualization
ER  - 

TY  - CONF
TI  - PhD Training in Simulation: NATCOR
AU  - Pidd, Michael
AU  - Robinson, Stewart
AU  - Davies, Ruth
AU  - Hoad, Kathryn
AU  - Cheng, Russell
T3  - WSC '10
AB  - To provide a broader education for Operational Research PhD students in the UK, the Engineering and Physical Sciences Research Council funds the National Taught Course Centre for Operational Research (NATCOR). This is an initiative led by six UK universities and includes a one-week, residential simulation module taught for the first time in July 2009. We describe the background to NATCOR, summarize its content and reflect on its further development.
C3  - Proceedings of the Winter Simulation Conference
DA  - 2010///
PY  - 2010
SP  - 339
EP  - 343
PB  - Winter Simulation Conference
SN  - 978-1-4244-9864-2
ER  - 

TY  - CONF
TI  - Designing Mobile Health Technology for Bipolar Disorder: A Field Trial of the Monarca System
AU  - Bardram, Jakob E.
AU  - Frost, Mads
AU  - Szántó, Károly
AU  - Faurholt-Jepsen, Maria
AU  - Vinberg, Maj
AU  - Kessing, Lars Vedel
T3  - CHI '13
AB  - An increasing number of pervasive healthcare systems are being designed, that allow people to monitor and get feedback on their health and wellness. To address the challenges of self-management of mental illnesses, we have developed the MONARCA system - a personal monitoring system for bipolar patients. We conducted a 14 week field trial in which 12 patients used the system, and we report findings focusing on their experiences. The results were positive; compared to using paper-based forms, the adherence to self-assessment improved; the system was considered very easy to use; and the perceived usefulness of the system was high. Based on this study, the paper discusses three HCI questions related to the design of personal health technologies; how to design for disease awareness and self-treatment, how to ensure adherence to personal health technologies, and the roles of different types of technology platforms.
C1  - New York, NY, USA
C3  - Proceedings of the SIGCHI Conference on Human Factors in Computing Systems
DA  - 2013///
PY  - 2013
DO  - 10.1145/2470654.2481364
SP  - 2627
EP  - 2636
PB  - Association for Computing Machinery
SN  - 978-1-4503-1899-0
UR  - https://doi.org/10.1145/2470654.2481364
KW  - mental health
KW  - bipolar disorder
KW  - mobile application
KW  - personal health systems
ER  - 

TY  - CONF
TI  - Towards Frabjous: A Two-Level System for Functional Reactive Agent-Based Epidemic Simulation
AU  - Schneider, Oliver
AU  - Dutchyn, Christopher
AU  - Osgood, Nathaniel
T3  - IHI '12
AB  - Agent-based infection-transmission models, which simulate an infection moving through a population, are being employed more frequently by health policy-makers. However, these models present several obstacles to widespread adoption. They are complex entities and impose a high development and maintenance cost. Current tools can be opaque, requiring multidisciplinary collaboration between a modeler and an expert programmer, and another round of translation when communicating with domain experts. In this paper, we describe the use of functional reactive programming (FRP), a programming paradigm created by imbuing a functional programming language with an intrinsic sense of time, to represent agent-based models in a concise and transparent way. We document the conversion of several agent-based models developed in the popular hybrid modeling tool AnyLogic to a representation in FRP. We also introduce Frabjous, a programming framework and domain-specific language for computational modeling. Frabjous generates human-readable and modifiable FRP code from a model specification, allowing modelers to have two transparent representations in which to program: a high-level model specification, and a full functional programming language with an agent-based modeling framework.
C1  - New York, NY, USA
C3  - Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium
DA  - 2012///
PY  - 2012
DO  - 10.1145/2110363.2110458
SP  - 785
EP  - 790
PB  - Association for Computing Machinery
SN  - 978-1-4503-0781-9
UR  - https://doi.org/10.1145/2110363.2110458
KW  - agent- based modeling
KW  - agent-based simulation
KW  - domain-specific language
KW  - dynamic model
KW  - functional programming
KW  - functional reactive
KW  - simulation
ER  - 

TY  - CONF
TI  - Harnessing the Power of Two Crossmatches
AU  - Blum, Avrim
AU  - Gupta, Anupam
AU  - Procaccia, Ariel
AU  - Sharma, Ankit
T3  - EC '13
AB  - Kidney exchanges allow incompatible donor-patient pairs to swap kidneys, but each donation must pass three tests: blood, tissue, and crossmatch. In practice a matching is computed based on the first two tests, and then a single crossmatch test is performed for each matched patient. However, if two crossmatches could be performed per patient, in principle significantly more successful exchanges could take place. In this paper, we ask: If we were allowed to perform two crossmatches per patient, could we harness this additional power optimally and efficiently? Our main result is a polynomial time algorithm for this problem that almost surely computes optimal — up to lower order terms — solutions on random large kidney exchange instances.
C1  - New York, NY, USA
C3  - Proceedings of the Fourteenth ACM Conference on Electronic Commerce
DA  - 2013///
PY  - 2013
DO  - 10.1145/2492002.2482569
SP  - 123
EP  - 140
PB  - Association for Computing Machinery
SN  - 978-1-4503-1962-1
UR  - https://doi.org/10.1145/2492002.2482569
KW  - kidney exchange
KW  - random graphs
ER  - 

TY  - CONF
TI  - Harnessing the Power of Two Crossmatches
AU  - Blum, Avrim
AU  - Gupta, Anupam
AU  - Procaccia, Ariel
AU  - Sharma, Ankit
T3  - EC '13
AB  - Kidney exchanges allow incompatible donor-patient pairs to swap kidneys, but each donation must pass three tests: blood, tissue, and crossmatch. In practice a matching is computed based on the first two tests, and then a single crossmatch test is performed for each matched patient. However, if two crossmatches could be performed per patient, in principle significantly more successful exchanges could take place. In this paper, we ask: If we were allowed to perform two crossmatches per patient, could we harness this additional power optimally and efficiently? Our main result is a polynomial time algorithm for this problem that almost surely computes optimal — up to lower order terms — solutions on random large kidney exchange instances.
C1  - New York, NY, USA
C3  - Proceedings of the Fourteenth ACM Conference on Electronic Commerce
DA  - 2018///
PY  - 2018
DO  - 10.1145/2482540.2482569
SP  - 123
EP  - 140
PB  - Association for Computing Machinery
SN  - 978-1-4503-1962-1
UR  - https://doi.org/10.1145/2482540.2482569
KW  - kidney exchange
KW  - random graphs
ER  - 

TY  - CONF
TI  - Solvability by Radicals from an Algorithmic Point of View
AU  - Hanrot, G.
AU  - Morain, F.
T3  - ISSAC '01
AB  - Any textbook on Galois theory contains a proof that a polynomial equation with solvable Galois group can be solved by radicals. From a practical point of view, we need to find suitable representations of the group and the roots of the polynomial. We first reduce the problem to that of cyclic extensions of prime degree and then work out the radicals, using the work of Girstmair. We give numerical examples of Abelian and non-Abelian solvable equations and apply the general framework to the construction of Hilbert Class fields of imaginary quadratic fields.
C1  - New York, NY, USA
C3  - Proceedings of the 2001 International Symposium on Symbolic and Algebraic Computation
DA  - 2001///
PY  - 2001
DO  - 10.1145/384101.384125
SP  - 175
EP  - 182
PB  - Association for Computing Machinery
SN  - 1-58113-417-7
UR  - https://doi.org/10.1145/384101.384125
ER  - 

TY  - CONF
TI  - Algorithm and Performance Evaluation of Adaptive Multidimensional Clustering Technique
AU  - Fushimi, Shinya
AU  - Kitsuregawa, Masaru
AU  - Nakayama, Masaya
AU  - Tanaka, Hidehiko
AU  - Moto-oka, Tohru
T3  - SIGMOD '85
C1  - New York, NY, USA
C3  - Proceedings of the 1985 ACM SIGMOD International Conference on Management of Data
DA  - 1985///
PY  - 1985
DO  - 10.1145/318898.318928
SP  - 308
EP  - 318
PB  - Association for Computing Machinery
SN  - 0-89791-160-1
UR  - https://doi.org/10.1145/318898.318928
ER  - 

TY  - JOUR
TI  - Algorithm and Performance Evaluation of Adaptive Multidimensional Clustering Technique
AU  - Fushimi, Shinya
AU  - Kitsuregawa, Masaru
AU  - Nakayama, Masaya
AU  - Tanaka, Hidehiko
AU  - Moto-oka, Tohru
T2  - SIGMOD Rec.
DA  - 1985/05//
PY  - 1985
DO  - 10.1145/971699.318928
VL  - 14
IS  - 4
SP  - 308
EP  - 318
SN  - 0163-5808
UR  - https://doi.org/10.1145/971699.318928
ER  - 

TY  - CONF
TI  - Hybrid Re-Scheduling Mechanisms for Workflow Applications on Multi-Cluster Grid
AU  - Zhang, Yang
AU  - Koelbel, Charles
AU  - Cooper, Keith
T3  - CCGRID '09
AB  - Grid computing is now a viable computational paradigm for executing large scale workflow applications. However, many aspects of performance optimization remain challenging. In this paper, we focus on the workflow scheduling mechanism. While there is much work on static scheduling approaches for workflow applications in parallel environments, little work has been done on a real-world multi-cluster Grid environment. Since a typical Grid environment is dynamic, we propose a new cluster-based scheduling mechanism that dynamically executes a top-down static scheduling algorithm using the real-time feedback from the execution monitor. We also propose a novel two phase migration mechanism that mitigates the effect of a possible bad reschedule decision. Our experimental results show that this approach achieves the best performance among all the scheduling approaches we implemented on both reserved resources and those with external loads.
C1  - USA
C3  - Proceedings of the 2009 9th IEEE/ACM International Symposium on Cluster Computing and the Grid
DA  - 2009///
PY  - 2009
DO  - 10.1109/CCGRID.2009.60
SP  - 116
EP  - 123
PB  - IEEE Computer Society
SN  - 978-0-7695-3622-4
UR  - https://doi.org/10.1109/CCGRID.2009.60
KW  - Dynamic Scheduling
KW  - Grid Computing
KW  - Workflow Scheduling
ER  - 

TY  - JOUR
TI  - "I Think People Are Powerful": The Sociality of Individuals Managing Depression
AU  - Burgess, Eleanor R.
AU  - Ringland, Kathryn E.
AU  - Nicholas, Jennifer
AU  - Knapp, Ashley A.
AU  - Eschler, Jordan
AU  - Mohr, David C.
AU  - Reddy, Madhu C.
T2  - Proc. ACM Hum.-Comput. Interact.
AB  - Millions of Americans struggle with depression, a condition characterized by feelings of sadness and motivation loss. To understand how individuals managing depression conceptualize their self-management activities, we conducted visual elicitations and semi-structured interviews with 30 participants managing depression in a large city in the U.S. Midwest. Many depression support tools are focused on the individual user and do not often incorporate social features. However, our analysis showed the key importance of sociality for self-management of depression. We describe how individuals connect with specific others to achieve expected support and how these interactions are mediated through locations and communication channels. We discuss factors influencing participants' sociality including relationship roles and expectations, mood state and communication channels, location and privacy, and culture and society. We broaden our understanding of sociality in CSCW through discussing diffuse sociality (being proximate to others but not interacting directly) as an important activity to support depression self-management.
DA  - 2019/11//
PY  - 2019
DO  - 10.1145/3359143
VL  - 3
IS  - CSCW
UR  - https://doi.org/10.1145/3359143
KW  - depression
KW  - communication channels
KW  - diffuse sociality
KW  - location
KW  - mental health support technology
KW  - relationship roles
KW  - self-management
KW  - sociality
ER  - 

TY  - CONF
TI  - Consistency and Correctness of Duplicate Database Systems
AU  - Ellis, Clarence A.
T3  - SOSP '77
AB  - Solutions to the duplicate database update problem are considered, and a formal validation technique using the theory of L systems is developed and applied to the problem. The paper shows some particular solutions but is primarily concerned with general properties of the problem, convenient representational techniques, and formal proof procedures which are general enough to apply to this and to a number of other problems in parallel processing and synchronization.
C1  - New York, NY, USA
C3  - Proceedings of the Sixth ACM Symposium on Operating Systems Principles
DA  - 1977///
PY  - 1977
DO  - 10.1145/800214.806548
SP  - 67
EP  - 84
PB  - Association for Computing Machinery
SN  - 978-1-4503-7867-3
UR  - https://doi.org/10.1145/800214.806548
ER  - 

TY  - JOUR
TI  - Consistency and Correctness of Duplicate Database Systems
AU  - Ellis, Clarence A.
T2  - SIGOPS Oper. Syst. Rev.
AB  - Solutions to the duplicate database update problem are considered, and a formal validation technique using the theory of L systems is developed and applied to the problem. The paper shows some particular solutions but is primarily concerned with general properties of the problem, convenient representational techniques, and formal proof procedures which are general enough to apply to this and to a number of other problems in parallel processing and synchronization.
DA  - 1977/11//
PY  - 1977
DO  - 10.1145/1067625.806548
VL  - 11
IS  - 5
SP  - 67
EP  - 84
SN  - 0163-5980
UR  - https://doi.org/10.1145/1067625.806548
ER  - 

TY  - BOOK
TI  - SA '16: SIGGRAPH ASIA 2016 Symposium on Visualization
AB  - The SIGGRAPH Asia Symposium on Visualization is an ideal platform for attendees to explore the opportunities and challenges of cutting-edge visualization techniques which facilitates human being to understand the data sets. The program aims to cover the development, technology, and demonstration of visualization techniques and their interactive applications.
CY  - New York, NY, USA
DA  - 2016///
PY  - 2016
PB  - Association for Computing Machinery
SN  - 978-1-4503-4547-7
ER  - 

TY  - CONF
TI  - The MONARCA Self-Assessment System: A Persuasive Personal Monitoring System for Bipolar Patients
AU  - Bardram, Jakob E.
AU  - Frost, Mads
AU  - Szántó, Károly
AU  - Marcu, Gabriela
T3  - IHI '12
AB  - An increasing number of persuasive personal healthcare monitoring systems are being researched, designed and tested. However, most of these systems have targeted somatic diseases and few have targeted mental illness. This paper describes the MONARCA system; a persuasive personal monitoring system for bipolar patients based on an Android mobile phone. The paper describes the user-centered design process behind the system, the user experience, and the technical implementation. This system is one of the first examples of the use of mobile monitoring to support the treatment of mental illness, and we discuss lessons learned and how others can use our experience in the design of such systems for the treatment of this important, yet challenging, patient group.
C1  - New York, NY, USA
C3  - Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium
DA  - 2012///
PY  - 2012
DO  - 10.1145/2110363.2110370
SP  - 21
EP  - 30
PB  - Association for Computing Machinery
SN  - 978-1-4503-0781-9
UR  - https://doi.org/10.1145/2110363.2110370
KW  - bipolar disorder
KW  - android
KW  - mental illness management
KW  - personal monitoring systems
KW  - self-assessment
ER  - 

TY  - CONF
TI  - Investigating Health Management Practices of Individuals with Diabetes
AU  - Mamykina, Lena
AU  - Mynatt, Elizabeth D.
AU  - Kaufman, David R.
T3  - CHI '06
AB  - Chronic diseases, endemic in the rapidly aging population, are stretching the capacity of healthcare resources. Increasingly, individuals need to adopt proactive health attitudes and contribute to the management of their own health. We investigate existing diabetes self-management practices and ways in which reflection on prior actions impacts future lifestyle choices. The findings suggest that individuals generate and evaluate hypotheses regarding health implications of their actions. Thus, health-monitoring applications can assist individuals in making educated choices by facilitating discovery of correlations between their past actions and health states. Deployment of an early prototype of a health-monitoring application demonstrated the need for careful presentation techniques to promote more robust understanding and to avoid reinforcement of biases.
C1  - New York, NY, USA
C3  - Proceedings of the SIGCHI Conference on Human Factors in Computing Systems
DA  - 2006///
PY  - 2006
DO  - 10.1145/1124772.1124910
SP  - 927
EP  - 936
PB  - Association for Computing Machinery
SN  - 1-59593-372-7
UR  - https://doi.org/10.1145/1124772.1124910
KW  - monitoring
KW  - healthcare
KW  - elderly
KW  - home
KW  - qualitative studies
KW  - technology probes
ER  - 

TY  - CONF
TI  - The Tees Confidentiality Model: An Authorisation Model for Identities and Roles
AU  - Longstaff, Jim
AU  - Lockyer, Mike
AU  - Nicholas, John
T3  - SACMAT '03
AB  - We present a model of authorisation that is more powerful than Role Based Access Control (RBAC), and is suitable for complex web applications in addition to computer systems administration. It achieves its functionality by combining Identity Based Access Control (IBAC) and RBAC in novel ways. A particular feature of the model is a rigorous definition of override, for granting access to data and resources in exceptional circumstances. Despite its power, the model can be implemented by a single algorithm, as an extension to RBAC. The basis of the model is a new concept of permission, which we call Confidentiality Permission. There are five types of confidentiality permission, for granting access rights for identities and roles; also negative confidentiality permissions, for denying access to data and resources, exist. A single concept of Collection is used for structuring roles, identities, resource and resource type, although the RBAC general and limited role hierarchies can be used if desired. Confidentiality permissions may be defined to inherit within collections, thereby providing a mechanism for confidentiality permission assignment; however confidentiality permissions may be assigned in other ways that do not depend on collections. We use a demanding scenario from Electronic Health Records to illustrate the power of the model. We have produced several demonstrators, one of which utilises the model to control data retrieval from commercial GP and Social Services systems.
C1  - New York, NY, USA
C3  - Proceedings of the Eighth ACM Symposium on Access Control Models and Technologies
DA  - 2003///
PY  - 2003
DO  - 10.1145/775412.775428
SP  - 125
EP  - 133
PB  - Association for Computing Machinery
SN  - 1-58113-681-1
UR  - https://doi.org/10.1145/775412.775428
ER  - 

TY  - BOOK
TI  - CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
CY  - New York, NY, USA
DA  - 2021///
PY  - 2021
PB  - Association for Computing Machinery
SN  - 978-1-4503-8096-6
ER  - 

TY  - CONF
TI  - The Role of Electronic Medical Records in the Identification of Suboptimal Prescribing for Hypertension Management: An Opportunity in Unchanged Therapy
AU  - Patel, Depak
AU  - Warren, Jim
AU  - Kennelly, John
AU  - Wai, Kuinileti Chang
T3  - HIKM '13
AB  - A Participatory Action Research (PAR) approach was taken to identify electronic medical record (EMR) queries for hypertension management quality review in the context of a Pacific-led New Zealand general practice. In each PAR cycle, queries to identify patients with prescribing at variance from evidence-based practice were formulated and run, relevant patient notes were retrieved, and a quality audit of the medication decisions was carried out by a medical practitioner working in the practice. 764 enrolled and funded patients with current antihypertensive prescriptions were queried regarding adherence to national treatment guidelines. Queries based on drug classes indicated by specific comorbidities (e.g. hypertension complicated by diabetes) retrieved few cases, and with almost none having a compelling case for change in therapy upon review. A query on unchanged therapy while cardiovascular risk (CVR) and systolic blood pressure remained high, however, yielded 30 cases for review, and 10 of these were deemed as warranting further investigation. We conclude that a promising area for the use of EMR queries to improve long-term condition management is in identification of patients with persistently high risk of adverse outcomes and concurrent unchanged therapy during successive general practice visits.
C1  - AUS
C3  - Proceedings of the Sixth Australasian Workshop on Health Informatics and Knowledge Management - Volume 142
DA  - 2013///
PY  - 2013
SP  - 21
EP  - 28
PB  - Australian Computer Society, Inc.
SN  - 978-1-921770-27-2
KW  - clinical quality improvement
KW  - computer-based patient records
KW  - electronic prescribing
KW  - hypertension management
ER  - 

TY  - CONF
TI  - The Relationship between Pulse Wave Velocity and Heart-Period Variability
AU  - Peng, Rong-Chao
AU  - Yan, Wen-Rong
T3  - ICBEA '21
AB  - The beat-to-beat heart-period variability (HPV) and pulse wave velocity (PWV) provide pivotal information on cardiac autonomic function that predicts arrhythmias and morbidity. However, their relationships have not been well understood. The present study is to investigate the relationship between heart period and PWV in beat-to-beat way and to determine the contribution of cold stimulus to their relationship. Experiments were conducted on 34 subjects in natural state and 55 subjects under cold stimulus. RR intervals (RRI) and PWV were calculated from simultaneously recorded electrocardiogram (ECG) and photoplethysmography (PPG) signals. Best correlation (r=-0.485) was obtained when RRI changes were delayed about 2.2 beats with respect to PWV changes, which confirmed the time-delay phenomenon between their variations. Overall, PWV was moderately negative correlated with RRI in their lower frequency components (fc&lt;0.15Hz). Cold stimulus induced significant decreases in RRI and significant increases in PWV, and the variations of mean RRI and PWV (i.e., ΔHRV and ΔPWV) showed negative correlation (r=-0.48). Compared with natural state, cold stimulus mainly influenced their best delayed beats in the lower frequency components (fc&lt; 0.15Hz). The results of this study suggest that RRI exhibit moderate correlation with PWV, and the effects of reactivity in response to cold stimulus on the relationship between RRI and PWV were mainly reflected in the low frequency components (fc&lt; 0.15Hz), which might be helpful to understand the relationship among regulatory mechanisms of different cardiovascular risk factors.
C1  - New York, NY, USA
C3  - 4th International Conference on Biometric Engineering and Applications
DA  - 2021///
PY  - 2021
DO  - 10.1145/3476779.3476783
SP  - 19
EP  - 24
PB  - Association for Computing Machinery
SN  - 978-1-4503-8765-1
UR  - https://doi.org/10.1145/3476779.3476783
KW  - Electrocardiogram
KW  - Heart-Period Variability
KW  - Photoplethysmogram
KW  - Pulse Wave Velocity
KW  - R-R interval
ER  - 

TY  - CONF
TI  - Iterative Distribution-Aware Sampling for Probabilistic Symbolic Execution
AU  - Borges, Mateus
AU  - Filieri, Antonio
AU  - D'Amorim, Marcelo
AU  - Păsăreanu, Corina S.
T3  - ESEC/FSE 2015
AB  - Probabilistic symbolic execution aims at quantifying the probability of reaching program events of interest assuming that program inputs follow given probabilistic distributions. The technique collects constraints on the inputs that lead to the target events and analyzes them to quantify how likely it is for an input to satisfy the constraints. Current techniques either handle only linear constraints or only support continuous distributions using a “discretization” of the input domain, leading to imprecise and costly results. We propose an iterative distribution-aware sampling approach to support probabilistic symbolic execution for arbitrarily complex mathematical constraints and continuous input distributions. We follow a compositional approach, where the symbolic constraints are decomposed into sub-problems whose solution can be solved independently. At each iteration the convergence rate of the com- putation is increased by automatically refocusing the analysis on estimating the sub-problems that mostly affect the accuracy of the results, as guided by three different ranking strategies. Experiments on publicly available benchmarks show that the proposed technique improves on previous approaches in terms of scalability and accuracy of the results.
C1  - New York, NY, USA
C3  - Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering
DA  - 2015///
PY  - 2015
DO  - 10.1145/2786805.2786832
SP  - 866
EP  - 877
PB  - Association for Computing Machinery
SN  - 978-1-4503-3675-8
UR  - https://doi.org/10.1145/2786805.2786832
KW  - Monte Carlo Sampling
KW  - Probabilistic Analysis
KW  - Symbolic Execution
ER  - 

TY  - CONF
TI  - Unremarkable AI: Fitting Intelligent Decision Support into Critical, Clinical Decision-Making Processes
AU  - Yang, Qian
AU  - Steinfeld, Aaron
AU  - Zimmerman, John
T3  - CHI '19
AB  - Clinical decision support tools (DST) promise improved healthcare outcomes by offering data-driven insights. While effective in lab settings, almost all DSTs have failed in practice. Empirical research diagnosed poor contextual fit as the cause. This paper describes the design and field evaluation of a radically new form of DST. It automatically generates slides for clinicians' decision meetings with subtly embedded machine prognostics. This design took inspiration from the notion of Unremarkable Computing, that by augmenting the users' routines technology/AI can have significant importance for the users yet remain unobtrusive. Our field evaluation suggests clinicians are more likely to encounter and embrace such a DST. Drawing on their responses, we discuss the importance and intricacies of finding the right level of unremarkableness in DST design, and share lessons learned in prototyping critical AI systems as a situated experience.
C1  - New York, NY, USA
C3  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
DA  - 2019///
PY  - 2019
DO  - 10.1145/3290605.3300468
SP  - 1
EP  - 11
PB  - Association for Computing Machinery
SN  - 978-1-4503-5970-2
UR  - https://doi.org/10.1145/3290605.3300468
KW  - healthcare
KW  - decision support systems
KW  - user experience
ER  - 

TY  - CONF
TI  - Automating DES Output Analysis: How Many Replications to Run
AU  - Hoad, Kathryn
AU  - Robinson, Stewart
AU  - Davies, Ruth
T3  - WSC '07
AB  - This paper describes the selection and automation of a method for estimating how many replications should be run to achieve a required accuracy in the output. The motivation is to provide an easy to use method that can be incorporated into existing simulation software that enables practitioners to obtain results of a specified accuracy. The processes and decisions involved in selecting and setting up a method for automation are explained. The extensive test results are outlined, including results from applying the algorithm to a collection of artificial and real models.
C3  - Proceedings of the 39th Conference on Winter Simulation: 40 Years! The Best is yet to Come
DA  - 2007///
PY  - 2007
SP  - 505
EP  - 512
PB  - IEEE Press
SN  - 1-4244-1306-0
ER  - 

TY  - CONF
TI  - EMC Information Sharing: Direct Access to MVS Data from UNIX and NT
AU  - Kohler, Walt
T3  - SIGMOD '99
AB  - In this extended abstract we briefly describe EMC's information sharing technology that enables UNIX and NT systems to directly access MVS mainframe datasets and how this technology can be used to directly access an MVS DB2 database.
C1  - New York, NY, USA
C3  - Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data
DA  - 1999///
PY  - 1999
DO  - 10.1145/304182.304241
SP  - 523
EP  - 524
PB  - Association for Computing Machinery
SN  - 1-58113-084-8
UR  - https://doi.org/10.1145/304182.304241
KW  - database extractor
KW  - DB2
KW  - direct access
KW  - information sharing
KW  - MVS
KW  - NT
KW  - UNIX
ER  - 

TY  - JOUR
TI  - EMC Information Sharing: Direct Access to MVS Data from UNIX and NT
AU  - Kohler, Walt
T2  - SIGMOD Rec.
AB  - In this extended abstract we briefly describe EMC's information sharing technology that enables UNIX and NT systems to directly access MVS mainframe datasets and how this technology can be used to directly access an MVS DB2 database.
DA  - 1999/06//
PY  - 1999
DO  - 10.1145/304181.304241
VL  - 28
IS  - 2
SP  - 523
EP  - 524
SN  - 0163-5808
UR  - https://doi.org/10.1145/304181.304241
KW  - database extractor
KW  - DB2
KW  - direct access
KW  - information sharing
KW  - MVS
KW  - NT
KW  - UNIX
ER  - 

TY  - JOUR
TI  - Getting There: Barriers and Facilitators to Transportation Access in Underserved Communities
AU  - Dillahunt, Tawanna R.
AU  - Veinot, Tiffany C.
T2  - ACM Trans. Comput.-Hum. Interact.
AB  - Advances in Information and Communication Technologies (ICTs) offer new opportunities for addressing transportation needs; however, past research suggests that opportunities are not equally shared by millions of low-income Americans. We draw from four empirical studies and two case studies to contribute descriptions of the 11 everyday transportation models currently used by residents of low-income and underserved communities to enhance their access to health-enhancing resources. These models fell into personal, private, public, and interpersonal categories. We contribute insights regarding the following barriers and facilitators associated with these models: (1) affordability; (2) individual capabilities; (3) interpersonal trust, care and/or reciprocity; (4) trust in technology; (5) service availability/eligibility; (6) spatial and temporal matches; (7) match between transportation mode and physical needs; (8) service reliability and quality; and (9) infrastructure access. To address these barriers and build on these facilitators, we contribute six supportive policy and design principles. Operationalizing these principles, we propose four new ICT-enhanced models: (1) smart jitneys; (2) generalized, favor-based models; (3) expanded resource pooling; and (4) transportation clubs. The focus of these models on socio-technical integration with current capabilities and resources holds promise for enhancing access to jobs, food, and health care for residents of low-income communities.
DA  - 2018/10//
PY  - 2018
DO  - 10.1145/3233985
VL  - 25
IS  - 5
SN  - 1073-0516
UR  - https://doi.org/10.1145/3233985
KW  - low-income communities
KW  - social determinants of health (SDOH)
KW  - Transportation
ER  - 

TY  - CONF
TI  - EBP: A Wearable System For Frequent and Comfortable Blood Pressure Monitoring From User's Ear
AU  - Bui, Nam
AU  - Pham, Nhat
AU  - Barnitz, Jessica Jacqueline
AU  - Zou, Zhanan
AU  - Nguyen, Phuc
AU  - Truong, Hoang
AU  - Kim, Taeho
AU  - Farrow, Nicholas
AU  - Nguyen, Anh
AU  - Xiao, Jianliang
AU  - Deterding, Robin
AU  - Dinh, Thang
AU  - Vu, Tam
T3  - MobiCom '19
AB  - Frequent blood pressure (BP) assessment is key to the diagnosis and treatment of many severe diseases, such as heart failure, kidney failure, hypertension, and hemodialysis. Current "gold-standard” BP measurement techniques require the complete blockage of blood flow, which causes discomfort and disruption to normal activity when the assessment is done repetitively and frequently. Unfortunately, patients with hypertension or hemodialysis often have to get their BP measured every 15 minutes for a duration of 4-5 hours or more. The discomfort of wearing a cumbersome and limited mobility device affects their normal activities. In this work, we propose a device called eBP to measure BP from inside the user's ear aiming to minimize the measurement's impact on users' normal activities while maximizing its comfort level. eBP has 3 key components: (1) a light-based pulse sensor attached on an inflatable pipe that goes inside the ear, (2) a digital air pump with a fine controller, and (3) a BP estimation algorithm. In contrast to existing devices, eBP introduces a novel technique that eliminates the need to block the blood flow inside the ear, which alleviates the user's discomfort. We prototyped eBP custom hardware and software and evaluated the system through a comparative study on 35 subjects. The study shows that eBP obtains the average error of 1.8 mmHg and -3.1 mmHg and a standard deviation error of 7.2 mmHg and 7.9 mmHg for systolic (high-pressure value) and diastolic (low-pressure value), respectively. These errors are around the acceptable margins regulated by the FDA's AAMI protocol, which allows mean errors of up to 5 mmHg and a standard deviation of up to 8 mmHg.
C1  - New York, NY, USA
C3  - The 25th Annual International Conference on Mobile Computing and Networking
DA  - 2019///
PY  - 2019
DO  - 10.1145/3300061.3345454
PB  - Association for Computing Machinery
SN  - 978-1-4503-6169-9
UR  - https://doi.org/10.1145/3300061.3345454
KW  - blood pressure
KW  - wearable devices
KW  - frequent blood pressure monitoring
KW  - in-ear blood pressure monitoring
KW  - in-ear sensing
ER  - 

TY  - CONF
TI  - On the Relationship between Caching and Routing in DHTs
AU  - Sanchez-Artigas, Marc
AU  - Garcia-Lopez, Pedro
AU  - Skarmeta, Antonio G.
T3  - WI-IATW '07
AB  - Although Distributed Hash Tables (DHTs) are well suited for wide-area distributed applications, they suffer from high latencies (O(log N) in the average case). Such high latencies hinder them from being employed in many relevant wide-area applications such as DNS. To cope with this, a promising solution appears to be the caching of popular files. For effective caching, this requires that the caching protocol places the replicas in such a way that lookups paths are shortened. This implies to delve into the relationship between routing geometries and caching. In this paper, we explore the impact of routing on proactive caching using Chord as case study. To be specific, we clarify the role that path convergence plays upon caching and how this can be used to place file replicas. Also, we present a caching technique to increase the amount of path convergence for randomized topologies such as Symphony, wherein the presence of a certain degree of path convergence is not guaranteed.
C1  - USA
C3  - Proceedings of the 2007 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Workshops
DA  - 2007///
PY  - 2007
SP  - 415
EP  - 418
PB  - IEEE Computer Society
SN  - 0-7695-3028-1
ER  - 

TY  - CONF
TI  - Compositional Solution Space Quantification for Probabilistic Software Analysis
AU  - Borges, Mateus
AU  - Filieri, Antonio
AU  - d'Amorim, Marcelo
AU  - Păsăreanu, Corina S.
AU  - Visser, Willem
T3  - PLDI '14
AB  - Probabilistic software analysis aims at quantifying how likely a target event is to occur during program execution. Current approaches rely on symbolic execution to identify the conditions to reach the target event and try to quantify the fraction of the input domain satisfying these conditions. Precise quantification is usually limited to linear constraints, while only approximate solutions can be provided in general through statistical approaches. However, statistical approaches may fail to converge to an acceptable accuracy within a reasonable time.We present a compositional statistical approach for the efficient quantification of solution spaces for arbitrarily complex constraints over bounded floating-point domains. The approach leverages interval constraint propagation to improve the accuracy of the estimation by focusing the sampling on the regions of the input domain containing the sought solutions. Preliminary experiments show significant improvement on previous approaches both in results accuracy and analysis time.
C1  - New York, NY, USA
C3  - Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation
DA  - 2014///
PY  - 2014
DO  - 10.1145/2594291.2594329
SP  - 123
EP  - 132
PB  - Association for Computing Machinery
SN  - 978-1-4503-2784-8
UR  - https://doi.org/10.1145/2594291.2594329
KW  - probabilistic analysis
KW  - symbolic execution
KW  - Monte Carlo sampling
KW  - testing
ER  - 

TY  - JOUR
TI  - Compositional Solution Space Quantification for Probabilistic Software Analysis
AU  - Borges, Mateus
AU  - Filieri, Antonio
AU  - d'Amorim, Marcelo
AU  - Păsăreanu, Corina S.
AU  - Visser, Willem
T2  - SIGPLAN Not.
AB  - Probabilistic software analysis aims at quantifying how likely a target event is to occur during program execution. Current approaches rely on symbolic execution to identify the conditions to reach the target event and try to quantify the fraction of the input domain satisfying these conditions. Precise quantification is usually limited to linear constraints, while only approximate solutions can be provided in general through statistical approaches. However, statistical approaches may fail to converge to an acceptable accuracy within a reasonable time.We present a compositional statistical approach for the efficient quantification of solution spaces for arbitrarily complex constraints over bounded floating-point domains. The approach leverages interval constraint propagation to improve the accuracy of the estimation by focusing the sampling on the regions of the input domain containing the sought solutions. Preliminary experiments show significant improvement on previous approaches both in results accuracy and analysis time.
DA  - 2014/06//
PY  - 2014
DO  - 10.1145/2666356.2594329
VL  - 49
IS  - 6
SP  - 123
EP  - 132
SN  - 0362-1340
UR  - https://doi.org/10.1145/2666356.2594329
KW  - probabilistic analysis
KW  - symbolic execution
KW  - Monte Carlo sampling
KW  - testing
ER  - 

TY  - CONF
TI  - Design and Care for Discordant Chronic Comorbidities: A Comparison of Healthcare Providers' Perspectives
AU  - Ongwere, Tom
AU  - Cantor, Gabrielle S
AU  - Clawson, James
AU  - Shih, Patrick C
AU  - Connelly, Kay
T3  - PervasiveHealth '20
AB  - Care and support of discordant chronic comorbidities (DCCs) are challenges not only for patients but also for their healthcare providers. DCCs are health conditions in which patients have multiple, often unrelated, chronic illnesses that may need to be addressed concurrently but may also be associated with conflicting treatment instructions. Previous studies show that patients with DCCs reported multiple challenges. Here, we conducted interviews (N = 8) and focus groups (N = 7) with healthcare providers to obtain providers' perspectives. We compare the challenges and views reported by patients and healthcare providers. We suggest design guidelines and technology-mediated ways to address convergent and divergent issues between patients and providers. We recommend future exploration of strategies to simplify and better understand how treatment choices for one condition may impact another and how that exacerbates DCCs care costs.
C1  - New York, NY, USA
C3  - Proceedings of the 14th EAI International Conference on Pervasive Computing Technologies for Healthcare
DA  - 2021///
PY  - 2021
DO  - 10.1145/3421937.3422013
SP  - 133
EP  - 145
PB  - Association for Computing Machinery
SN  - 978-1-4503-7532-0
UR  - https://doi.org/10.1145/3421937.3422013
KW  - collaboration
KW  - decision making and prioritization
KW  - design strategy
KW  - Discordant chronic comorbidity
KW  - provider perspective patient perspective
KW  - treatment plans
ER  - 

TY  - CONF
TI  - Power Optimization of Real-Time Embedded Systems on Variable Speed Processors
AU  - Shin, Youngsoo
AU  - Choi, Kiyoung
AU  - Sakurai, Takayasu
T3  - ICCAD '00
AB  - Power efficient design of real-time embedded systems based on programmable processors becomes more important as system functionality is increasingly realized through software. This paper presents a power optimization method for real-time embedded applications on a variable speed processor. The method combines off-line and on-line components. The off-line component determines the lowest possible maximum processor speed while guaranteeing deadlines of all tasks. The on-line component dynamically varies the processor speed or bring a processor into a power-down mode according to the status of task set in order to exploit execution time variations and idle intervals. Experimental results show that the proposed method obtains a significant power reduction across several kinds of applications.
C3  - Proceedings of the 2000 IEEE/ACM International Conference on Computer-Aided Design
DA  - 2000///
PY  - 2000
SP  - 365
EP  - 368
PB  - IEEE Press
SN  - 0-7803-6448-1
ER  - 

TY  - CONF
TI  - MMHealth 2017: Workshop on Multimedia for Personal Health and Health Care
AU  - Boll, Susanne CJ
AU  - Ebrahimi, Tourdaj
AU  - Gurrin, Cathal
AU  - Jalali, Laleh
AU  - Jain, Ramesh
AU  - Meyer, Jochen
AU  - O'Connor, Noel E.
T3  - MM '17
AB  - Ever since the emergence of digitization, we've used the term multimedia to represent a combination of different kinds of media types, such as images, audio, and videos. As new sensing technologies emerge and are now becoming omnipresent in daily lives, the definition, role and significance of multimedia is changing. Multimedia now represents the means for communicating, cooperating, and also for monitoring numerous aspects of daily life, at various levels of granularity and application, ranging from personal to societal. With this shift, we have since moved from comprehending single media and its state toward comprehending media in terms of its use context.Multimedia is thus no longer confined to documentation and preservation, entertainment or personal media collections; rather, it has become an integral part of the tools and systems that are providing solutions to today's societal challenges–including challenges related to health, aging, education, societal participation, sustainable energy, and intelligent transportation. Multimedia has thus evolved into a core enabler for future interactive and cooperative applications at the heart of society. In this workshop we explore the relevance and contribution of multimedia to health care and personal media.
C1  - New York, NY, USA
C3  - Proceedings of the 25th ACM International Conference on Multimedia
DA  - 2017///
PY  - 2017
DO  - 10.1145/3123266.3132051
SP  - 1961
EP  - 1962
PB  - Association for Computing Machinery
SN  - 978-1-4503-4906-2
UR  - https://doi.org/10.1145/3123266.3132051
KW  - health care
KW  - multimedia
KW  - personal health
ER  - 

TY  - CONF
TI  - SHIRI: Buttocks Humanoid That Represents Emotions with Visual and Tactual Transformation of the Muscles
AU  - Takahashi, Nobuhiro
AU  - Matoba, Yasushi
AU  - Sato, Toshiki
AU  - Koike, Hideki
T3  - AVI '12
AB  - In this paper, we propose a novel interface design for "human-robot" communication by focusing on visual and tactual transformation of the muscles. Since recent humanoids may appear as humanoid figures using human-like body gestures and behavior, it is hard to say that they have enough elements to cover the complex composition that is a human. The muscles that constitute the human body work by not only turning joints and generating limb and body movements, but also control skin surface shape and firmness, allowing the various levels of touch response. Therefore, we attempt to approach the creation of sensitive and subtle expression by a humanoid robot using organic constructs. In this project, we produce "SHIRI", which represents emotions with organic movements of the Gluteus Maximus Actuator (GMA). In addition, we also implement user interaction for SHIRI and consider how perceptions the user can obtain by communicating with SHIRI.
C1  - New York, NY, USA
C3  - Proceedings of the International Working Conference on Advanced Visual Interfaces
DA  - 2012///
PY  - 2012
DO  - 10.1145/2254556.2254717
SP  - 792
EP  - 793
PB  - Association for Computing Machinery
SN  - 978-1-4503-1287-5
UR  - https://doi.org/10.1145/2254556.2254717
KW  - behavior recognition
KW  - emotion regulation
KW  - human-robot interaction
KW  - humanoid robotics
KW  - input device
ER  - 

TY  - JOUR
TI  - Eliminating Redundant Recursive Calls.
AU  - Cohen, Norman H.
T2  - ACM Trans. Program. Lang. Syst.
DA  - 1983/07//
PY  - 1983
DO  - 10.1145/2166.2167
VL  - 5
IS  - 3
SP  - 265
EP  - 299
SN  - 0164-0925
UR  - https://doi.org/10.1145/2166.2167
ER  - 

TY  - CONF
TI  - Algebraic Knowledge Discovery Using Haskell
AU  - Fisseler, Jens
AU  - Kern-Isberner, Gabriele
AU  - Beierle, Christoph
AU  - Koch, Andreas
AU  - Müller, Christian
T3  - PADL'07
AB  - While declarative programming languages are often considered to be applicable to “toy problems” only, we present an example of a real-world programming task realized with a functional programming language. CondorCKD is a novel algebraic knowledge discovery algorithm completely implemented in Haskell. We give an overview of CondorCKD and describe our experiences gained during its development, including the implementation of a graphical user interface and a novel approach to compute the cycles of an undirected graph.
C1  - Berlin, Heidelberg
C3  - Proceedings of the 9th International Conference on Practical Aspects of Declarative Languages
DA  - 2007///
PY  - 2007
DO  - 10.1007/978-3-540-69611-7_5
SP  - 80
EP  - 93
PB  - Springer-Verlag
SN  - 3-540-69608-3
UR  - https://doi.org/10.1007/978-3-540-69611-7_5
ER  - 

TY  - CONF
TI  - Aquacore: A Programmable Architecture for Microfluidics
AU  - Amin, Ahmed M.
AU  - Thottethodi, Mithuna
AU  - Vijaykumar, T. N.
AU  - Wereley, Steven
AU  - Jacobson, Stephen C.
T3  - ISCA '07
AB  - Advances in microfluidic research has enabled lab-on-a-chip (LoC) technology to achieve miniaturization and integration of biological and chemical analyses to a single chip comprising channels, valves, mixers, heaters, separators, and sensors. These miniature instruments appear to offer the rare combination of faster, cheaper, and higher-precision analyses in comparison to conventional bench-scale methods. LoCs have been applied to diverse domains such as proteomics, genomics, biochemistry, virology, cell biology, and chemical synthesis. However, to date LoCs have been designed as application-specific chips which incurs significant design effort, turn-around time, and cost, and degrades designer and user productivity. To address these limitations, we envision a programmable LoC (PLoC) and propose a comprehensive fluidic instruction set, called AquaCore Instruction Set (AIS), and a fluidic microarchitecture, called AquaCore, to implement AIS. We present four key design aspects in which the AIS and AquaCore differ from their computer counterparts, and our design decisions made on the basis of the implications of these differences. We demonstrate the use of the PLoC in a range of domains by hand-compiling real-world microfluidic assays in AIS, and show a detailed breakdown of the execution times for the assays and an estimate of the chip area.
C1  - New York, NY, USA
C3  - Proceedings of the 34th Annual International Symposium on Computer Architecture
DA  - 2007///
PY  - 2007
DO  - 10.1145/1250662.1250694
SP  - 254
EP  - 265
PB  - Association for Computing Machinery
SN  - 978-1-59593-706-3
UR  - https://doi.org/10.1145/1250662.1250694
KW  - fluidic
KW  - fluidic microarchitecture
KW  - instruction set
KW  - microfluidics
KW  - programmable lab on a chip
ER  - 

TY  - JOUR
TI  - Aquacore: A Programmable Architecture for Microfluidics
AU  - Amin, Ahmed M.
AU  - Thottethodi, Mithuna
AU  - Vijaykumar, T. N.
AU  - Wereley, Steven
AU  - Jacobson, Stephen C.
T2  - SIGARCH Comput. Archit. News
AB  - Advances in microfluidic research has enabled lab-on-a-chip (LoC) technology to achieve miniaturization and integration of biological and chemical analyses to a single chip comprising channels, valves, mixers, heaters, separators, and sensors. These miniature instruments appear to offer the rare combination of faster, cheaper, and higher-precision analyses in comparison to conventional bench-scale methods. LoCs have been applied to diverse domains such as proteomics, genomics, biochemistry, virology, cell biology, and chemical synthesis. However, to date LoCs have been designed as application-specific chips which incurs significant design effort, turn-around time, and cost, and degrades designer and user productivity. To address these limitations, we envision a programmable LoC (PLoC) and propose a comprehensive fluidic instruction set, called AquaCore Instruction Set (AIS), and a fluidic microarchitecture, called AquaCore, to implement AIS. We present four key design aspects in which the AIS and AquaCore differ from their computer counterparts, and our design decisions made on the basis of the implications of these differences. We demonstrate the use of the PLoC in a range of domains by hand-compiling real-world microfluidic assays in AIS, and show a detailed breakdown of the execution times for the assays and an estimate of the chip area.
DA  - 2007/06//
PY  - 2007
DO  - 10.1145/1273440.1250694
VL  - 35
IS  - 2
SP  - 254
EP  - 265
SN  - 0163-5964
UR  - https://doi.org/10.1145/1273440.1250694
KW  - fluidic
KW  - fluidic microarchitecture
KW  - instruction set
KW  - microfluidics
KW  - programmable lab on a chip
ER  - 

TY  - CONF
TI  - Position-Indexed Formulations for Kidney Exchange
AU  - Dickerson, John P.
AU  - Manlove, David F.
AU  - Plaut, Benjamin
AU  - Sandholm, Tuomas
AU  - Trimble, James
T3  - EC '16
AB  - A kidney exchange is an organized barter market where patients in need of a kidney swap willing but incompatible donors. Determining an optimal set of exchanges is theoretically and empirically hard. Traditionally, exchanges took place in cycles, with each participating patient-donor pair both giving and receiving a kidney. The recent introduction of chains, where a donor without a paired patient triggers a sequence of donations without requiring a kidney in return, increased the efficacy of fielded kidney exchanges—while also dramatically raising the empirical computational hardness of clearing the market in practice. While chains can be quite long, unbounded-length chains are not desirable: planned donations can fail before transplant for a variety of reasons, and the failure of a single donation causes the rest of that chain to fail, so parallel shorter chains are better in practice.In this paper, we address the tractable clearing of kidney exchanges with short cycles and chains that are long but bounded. This corresponds to the practice at most modern fielded kidney exchanges. We introduce three new integer programming formulations, two of which are compact. Furthermore, one of these models has a linear programming relaxation that is exactly as tight as the previous tightest formulation (which was not compact) for instances in which each donor has a paired patient. On real data from the UNOS nationwide exchange in the United States and the NLDKSS nationwide exchange in the United Kingdom, as well as on generated realistic large-scale data, we show that our new models are competitive with all existing solvers—in many cases outperforming all other solvers by orders of magnitude.Finally, we note that our position-indexed chain-edge formulation can be modified in a straightforward way to take post-match edge failure into account, under the restriction that edges have equal probabilities of failure. Post-match edge failure is a primary source of inefficiency in presently-fielded kidney exchanges. We show how to implement such failure-aware matching in our model, and also extend the state-of-the-art general branch-and-price-based non-compact formulation for the failure-aware problem to run its pricing problem in polynomial time.
C1  - New York, NY, USA
C3  - Proceedings of the 2016 ACM Conference on Economics and Computation
DA  - 2016///
PY  - 2016
DO  - 10.1145/2940716.2940759
SP  - 25
EP  - 42
PB  - Association for Computing Machinery
SN  - 978-1-4503-3936-0
UR  - https://doi.org/10.1145/2940716.2940759
KW  - kidney exchange
KW  - stochastic matching
KW  - matching markets
KW  - branch and price
KW  - integer programming
ER  - 

TY  - CONF
TI  - Applications of Computer Simulation in Health Care
AU  - England, William
AU  - Roberts, Stephen D.
T3  - WSC '78
AB  - Several hundred computer simulation models have been developed in the last 15 years to solve problems in the nation's health care delivery system. These models are categorized and reviewed according to 21 areas of application, along with discussion of general model characteristics. Charts showing trends in health care simulation modeling are given, followed by discussion of problems in model implementation and directions for future research.
C1  - Washington, DC, USA
C3  - Proceedings of the 10th Conference on Winter Simulation - Volume 2
DA  - 1978///
PY  - 1978
SP  - 665
EP  - 677
PB  - IEEE Computer Society Press
ER  - 

TY  - CONF
TI  - Isolation in Coordination: Challenges of Caregivers in the USA
AU  - Schurgin, Mark
AU  - Schlager, Mark
AU  - Vardoulakis, Laura
AU  - Pina, Laura R.
AU  - Wilcox, Lauren
T3  - CHI '21
AB  - As the global population ages and the prevalence of chronic conditions and acute infections rise, it is becoming imperative to understand the many forms of caregiving labor and create sociotechnical systems that support them. In this paper, we report results of a large survey study with 2000 informal caregivers in the USA, highlighting the fundamental challenges that different types of caregivers face when coordinating care with others. Our findings support previous findings on caregivers’ coordination challenges, while also offering insights into the situational, mediating factors that influence the extent to which care coordination challenges are felt. These mediating factors include caregivers’ relationships, access to a variety of resources including physical, social, and financial support, and physical and mental barriers. We discuss these challenges and mediating factors, and conclude with a discussion on how they can be considered in the design of future sociotechnical systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
DA  - 2021///
PY  - 2021
DO  - 10.1145/3411764.3445413
PB  - Association for Computing Machinery
SN  - 978-1-4503-8096-6
UR  - https://doi.org/10.1145/3411764.3445413
KW  - Health
KW  - Care Communication
KW  - Care Coordination
KW  - Caregiving
KW  - Well-being
ER  - 

TY  - CONF
TI  - Interactive and Guided Architectural Refactoring with Search-Based Recommendation
AU  - Lin, Yun
AU  - Peng, Xin
AU  - Cai, Yuanfang
AU  - Dig, Danny
AU  - Zheng, Diwen
AU  - Zhao, Wenyun
T3  - FSE 2016
AB  - Architectural refactorings can contain hundreds of steps and experienced developers could carry them out over several weeks. Moreover, developers need to explore a correct sequence of refactorings steps among many more incorrect alternatives. Thus, carrying out architectural refactorings is costly, risky, and challenging. In this paper, we present Refactoring Navigator: a tool-supported and interactive recommendation approach for aiding architectural refactoring. Our approach takes a given implementation as the starting point, a desired high-level design as the target, and iteratively recommends a series of refactoring steps. Moreover, our approach allows the user to accept, reject, or ignore a recommended refactoring step, and uses the user's feedback in further refactoring recommendations. We evaluated the effectiveness of our approach and tool using a controlled experiment and an industrial case study. The controlled experiment shows that the participants who used Refactoring Navigator accomplished their tasks in 77.4% less time and manually edited 98.3% fewer lines than the control group. The industrial case study suggests that Refactoring Navigator has the potential to help with architectural refactorings in practice.
C1  - New York, NY, USA
C3  - Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering
DA  - 2016///
PY  - 2016
DO  - 10.1145/2950290.2950317
SP  - 535
EP  - 546
PB  - Association for Computing Machinery
SN  - 978-1-4503-4218-6
UR  - https://doi.org/10.1145/2950290.2950317
KW  - automatic refactoring
KW  - high-level design
KW  - interactive
KW  - reflexion model
KW  - user feedback
ER  - 

TY  - JOUR
TI  - A Review and Assessment Framework for Mobile-Based Emergency Intervention Apps
AU  - Gaziel-Yablowitz, Michal
AU  - Schwartz, David G.
T2  - ACM Comput. Surv.
AB  - Smartphone applications to support healthcare are proliferating. A growing and important subset of these apps supports emergency medical intervention to address a wide range of illness-related emergencies to speed the arrival of relevant treatment. The emergency response characteristics and strategies employed by these apps are the focus of this study, resulting in an mHealth Emergency Strategy Index. While a growing body of knowledge focuses on usability, safety, and privacy aspects that characterize such apps, studies that map the various emergency intervention strategies and suggest assessment indicators to evaluate their role as emergency agents are limited. We survey an extensive range of mHealth apps designed for emergency response along with the related assessment literature and present an index for mobile-based medical emergency intervention apps that can address future assessment needs of mHealth apps.
DA  - 2018/01//
PY  - 2018
DO  - 10.1145/3145846
VL  - 51
IS  - 1
SN  - 0360-0300
UR  - https://doi.org/10.1145/3145846
KW  - mHealth Emergency Applications
ER  - 

TY  - CONF
TI  - Designing Software for Use by Humans, Not Machines
AU  - Ruston, L.
AU  - Muller, M. J.
AU  - Cebulka, K. D.
T3  - ICSE '91
C1  - Washington, DC, USA
C3  - Proceedings of the 13th International Conference on Software Engineering
DA  - 1991///
PY  - 1991
SP  - 104
EP  - 113
PB  - IEEE Computer Society Press
SN  - 0-89791-391-4
ER  - 

TY  - CONF
TI  - Analyzing Aliases of Reference Formal Parameters
AU  - Cooper, Keith D.
T3  - POPL '85
AB  - Compilers for languages with call-by-reference formal parameters must deal with aliases arising from the renaming effects at call sites. This paper presents a set of techniques for analyzing aliasing patterns. The analysis is divided into detecting the introduction of aliases and tracking their propagation. The algorithm for introduction analysis is simple enough to be performed in a structured editor or parser. A data flow analysis framework is given for the propagation problem, making it possible to solve using standard algorithms from global data flow analysis. Several optimizations are shown which can shrink the size of the problem, and extensions are given to handle ALGOL-style name scoping. Finally, this technique is compared to an alternative implementation strategy and an approximate technique.
C1  - New York, NY, USA
C3  - Proceedings of the 12th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages
DA  - 1985///
PY  - 1985
DO  - 10.1145/318593.318658
SP  - 281
EP  - 290
PB  - Association for Computing Machinery
SN  - 0-89791-147-4
UR  - https://doi.org/10.1145/318593.318658
ER  - 

TY  - JOUR
TI  - Finding Large Diverse Communities on Networks: The Edge Maximum K*-Partite Clique
AU  - Zhou, Alexander
AU  - Wang, Yue
AU  - Chen, Lei
T2  - Proc. VLDB Endow.
AB  - In this work we examine the problem of finding large, diverse communities on graphs where the users are separated into distinct groups. More specifically, this work considers diversity to be the inclusion of users from multiple groups as opposed to homogeneous communities in which the majority of users are from one group. We design and propose the k*-Partite Clique (and the edge-maximum k*-Partite Clique Problem) which modifies the k-Partite Clique structure as a means to capture these large, diverse communities in a way that does not currently exist. We then design a non-trivial baseline enumeration algorithm, which is further improved via heuristics to significantly reduce the running time whilst avoiding excessive memory requirements. Moreover, we propose a core as well as a truss structure for the k-Partite environment aimed at finding the edge-maximum k*-Partite Clique structure on the network. Comprehensive experiments on real-world datasets verify both the effectiveness of the k*-Partite Clique at finding diverse communities as well as the efficiency of the proposed heuristics to our algorithms compared to reasonable baselines.
DA  - 2020/07//
PY  - 2020
DO  - 10.14778/3407790.3407846
VL  - 13
IS  - 12
SP  - 2576
EP  - 2589
SN  - 2150-8097
UR  - https://doi.org/10.14778/3407790.3407846
ER  - 

TY  - JOUR
TI  - GreASE: A Tool for Efficient “Nonequivalence” Checking
AU  - Francesco, Nicoletta de
AU  - Lettieri, Giuseppe
AU  - Santone, Antonella
AU  - Vaglini, Gigliola
T2  - ACM Trans. Softw. Eng. Methodol.
AB  - Equivalence checking plays a crucial role in formal verification to ensure the correctness of concurrent systems. However, this method cannot be scaled as easily with the increasing complexity of systems due to the state explosion problem. This article presents an efficient procedure, based on heuristic search, for checking Milner's strong and weak equivalence; to achieve higher efficiency, we actually search for a difference between two processes to be discovered as soon as possible, thus the heuristics aims to find a counterexample, even if not the minimum one, to prove nonequivalence. The presented algorithm builds the system state graph on-the-fly, during the checking, and the heuristics promotes the construction of the more promising subgraph. The heuristic function is syntax based, but the approach can be applied to different specification languages such as CCS, LOTOS, and CSP, provided that the language semantics is based on the concept of transition. The algorithm to explore the search space of the problem is based on a greedy technique; GreASE (Greedy Algorithm for System Equivalence), the tool supporting the approach, is used to evaluate the achieved reduction of both state-space size and time with respect to other verification environments.
DA  - 2014/06//
PY  - 2014
DO  - 10.1145/2560563
VL  - 23
IS  - 3
SN  - 1049-331X
UR  - https://doi.org/10.1145/2560563
KW  - equivalence checking
KW  - Formal methods
KW  - heuristic searches
ER  - 

TY  - CONF
TI  - The Impact of Interprocedural Analysis and Optimization on the Design of a Software Development Environment
AU  - Cooper, Keith D.
AU  - Kennedy, Ken
AU  - Torczon, Linda
T3  - SLIPE '85
AB  - One of the primary goals of the IRn programming environment project is to mount a concerted attack on the problems of performing interprocedural analysis and optimization in a compiler. Few commercial optimizing compilers employ interprocedural techniques because the cost of gathering the requisite information in a traditional compiler is too great. Computing the side effects of a procedure call requires detailed knowledge of the internals of both the called procedure and any procedures invoked either directly or indirectly from it. Thus, the compiler potentially needs information about the internals of every procedure to determine the side effects of procedure calls, even separately compiled procedures. Gathering this information would require examining the source of every procedure in the program - an expensive process, particularly unfortunate since the primary goal of separate compilation is to reduce the amount of recompilation required in response to changes in an individual procedure.The existence of a software development environment like the IRn programming environment [HoKe 84] changes the compilation process enough to make computing such information palatable. Since all modules are developed and all programs are defined using tools of the environment, these tools can cooperate to record the information necessary to do a good job of interprocedural analysis and optimization. Whenever the compiler needs information about possible side effects of a particular procedure, it can simply extract this information from the environment's central database. Because the only mechanism for changing modules or programs is through the tools provided by the environment, the compiler is assured that it will be notified of any changes. Thus, it can use information derived from previous analysis with certain knowledge that the information reflects the current state of the program and its procedures.
C1  - New York, NY, USA
C3  - Proceedings of the ACM SIGPLAN 85 Symposium on Language Issues in Programming Environments
DA  - 1985///
PY  - 1985
DO  - 10.1145/800225.806832
SP  - 107
EP  - 116
PB  - Association for Computing Machinery
SN  - 0-89791-165-2
UR  - https://doi.org/10.1145/800225.806832
ER  - 

TY  - JOUR
TI  - The Impact of Interprocedural Analysis and Optimization on the Design of a Software Development Environment
AU  - Cooper, Keith D.
AU  - Kennedy, Ken
AU  - Torczon, Linda
T2  - SIGPLAN Not.
AB  - One of the primary goals of the IRn programming environment project is to mount a concerted attack on the problems of performing interprocedural analysis and optimization in a compiler. Few commercial optimizing compilers employ interprocedural techniques because the cost of gathering the requisite information in a traditional compiler is too great. Computing the side effects of a procedure call requires detailed knowledge of the internals of both the called procedure and any procedures invoked either directly or indirectly from it. Thus, the compiler potentially needs information about the internals of every procedure to determine the side effects of procedure calls, even separately compiled procedures. Gathering this information would require examining the source of every procedure in the program - an expensive process, particularly unfortunate since the primary goal of separate compilation is to reduce the amount of recompilation required in response to changes in an individual procedure.The existence of a software development environment like the IRn programming environment [HoKe 84] changes the compilation process enough to make computing such information palatable. Since all modules are developed and all programs are defined using tools of the environment, these tools can cooperate to record the information necessary to do a good job of interprocedural analysis and optimization. Whenever the compiler needs information about possible side effects of a particular procedure, it can simply extract this information from the environment's central database. Because the only mechanism for changing modules or programs is through the tools provided by the environment, the compiler is assured that it will be notified of any changes. Thus, it can use information derived from previous analysis with certain knowledge that the information reflects the current state of the program and its procedures.
DA  - 1985/06//
PY  - 1985
DO  - 10.1145/17919.806832
VL  - 20
IS  - 7
SP  - 107
EP  - 116
SN  - 0362-1340
UR  - https://doi.org/10.1145/17919.806832
ER  - 

TY  - JOUR
TI  - Time-Space Lower Bounds for Satisfiability
AU  - Fortnow, Lance
AU  - Lipton, Richard
AU  - van Melkebeek, Dieter
AU  - Viglas, Anastasios
T2  - J. ACM
AB  - We establish the first polynomial time-space lower bounds for satisfiability on general models of computation. We show that for any constant c less than the golden ratio there exists a positive constant d such that no deterministic random-access Turing machine can solve satisfiability in time nc and space nd, where d approaches 1 when c does. On conondeterministic instead of deterministic machines, we prove the same for any constant c less than √2.Our lower bounds apply to nondeterministic linear time and almost all natural NP-complete problems known. In fact, they even apply to the class of languages that can be solved on a nondeterministic machine in linear time and space n1/c.Our proofs follow the paradigm of indirect diagonalization. We also use that paradigm to prove time-space lower bounds for languages higher up in the polynomial-time hierarchy.
DA  - 2005/11//
PY  - 2005
DO  - 10.1145/1101821.1101822
VL  - 52
IS  - 6
SP  - 835
EP  - 865
SN  - 0004-5411
UR  - https://doi.org/10.1145/1101821.1101822
KW  - Complexity of satisfiability
KW  - time-space lower bounds
ER  - 

TY  - JOUR
TI  - A Model for Predicting and Evaluating Computer Resource Consumption
AU  - Ahituv, Niv
AU  - Igbaria, Magid
T2  - Commun. ACM
AB  - Evaluation and prediction of computer resource consumption can aid in the determination of computer selection and configuration planning as well as in the usefulness and maintenance of existing systems. The model presented can be used in a "What If" mode to make these determinations.
DA  - 1988/12//
PY  - 1988
DO  - 10.1145/53580.53585
VL  - 31
IS  - 12
SP  - 1467
EP  - 1473
SN  - 0001-0782
UR  - https://doi.org/10.1145/53580.53585
ER  - 

TY  - CONF
TI  - Fast Interprocedual Alias Analysis
AU  - Cooper, K. D.
AU  - Kennedy, K.
T3  - POPL '89
AB  - We present a new algorithm for computing interprocedural aliases due to passing parameters by reference. This algorithm runs in O(N2+NE) time and, when combined with algorithms for alias-free, flow-insensitive data-flow problems, yields algorithms for solution of the general flow-insensitive problems that also run in O(N2+NE) time.
C1  - New York, NY, USA
C3  - Proceedings of the 16th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages
DA  - 1989///
PY  - 1989
DO  - 10.1145/75277.75282
SP  - 49
EP  - 59
PB  - Association for Computing Machinery
SN  - 0-89791-294-2
UR  - https://doi.org/10.1145/75277.75282
ER  - 

TY  - JOUR
TI  - The Moser–Tardos Framework with Partial Resampling
AU  - Harris, David G.
AU  - Srinivasan, Aravind
T2  - J. ACM
AB  - The resampling algorithm of Moser and Tardos is a powerful approach to develop constructive versions of the Lovász Local Lemma. We generalize this to partial resampling: When a bad event holds, we resample an appropriately random subset of the variables that define this event rather than the entire set, as in Moser and Tardos. This is particularly useful when the bad events are determined by sums of random variables. This leads to several improved algorithmic applications in scheduling, graph transversals, packet routing, and so on. For instance, we settle a conjecture of Szabó and Tardos (2006) on graph transversals asymptotically and obtain improved approximation ratios for a packet routing problem of Leighton, Maggs, and Rao (1994).
DA  - 2019/08//
PY  - 2019
DO  - 10.1145/3342222
VL  - 66
IS  - 5
SN  - 0004-5411
UR  - https://doi.org/10.1145/3342222
KW  - discrepancy
KW  - independent transversal
KW  - Lovász local lemma
KW  - packet routing
KW  - resampling algorithm
ER  - 

TY  - CONF
TI  - A Line Selection Method Based on Transient Phase Current Variation Comparison
AU  - Qin, Zeyu
AU  - Ye, Liuqing
AU  - Wang, Zhe
AU  - Qin, Yue
AU  - Huang, Jiyuan
AU  - Xie, Xiong
AU  - Hu, Xiangwei
T3  - ICITEE2021
AB  - A new line selection method based on waveform comparison of sudden Phase Current Variation (PCV) is proposed to solve the problem of discriminant margin lack and prone to misjudgment in traditional faulty line selection method. By analyzing the characteristics of the faulty phase current, it shows that each phase current waveform of faulty line is different, and the Current Variation (CV) amplitude of the faulty phase is much larger than the normal phase, while the amplitude and waveform of each non-faulty line PCV are almost the same. This method defines the relative difference parameter to represent the waveform variation of each phase current. By calculating and comparing the value of the relative difference parameters, the maximum is selected as the faulty parameter of the faulty line. The simulation result shows that the method is not affected by line structure, fault type, operation mode and other factors, and it has significant advantages of applicability, economy and reliability when selecting the faulty lines.
C1  - New York, NY, USA
C3  - The 4th International Conference on Information Technologies and Electrical Engineering
DA  - 2022///
PY  - 2022
DO  - 10.1145/3513142.3513169
PB  - Association for Computing Machinery
SN  - 978-1-4503-8649-4
UR  - https://doi.org/10.1145/3513142.3513169
KW  - Discrimination margin
KW  - Faulty line selection
KW  - Faulty parameter
KW  - Phase current variation
KW  - Relative difference parameter
ER  - 

TY  - CONF
TI  - Interactive Surface That Have Dynamic Softness Control
AU  - Sato, Toshiki
AU  - Takahashi, Nobuhiro
AU  - Matoba, Yasushi
AU  - Koike, Hideki
T3  - AVI '12
AB  - In the field of interface surface research, the idea of the 'softness' of a surface medium is one significant factor in determining a suitable means of interaction with the user. With direct touch input, for example, the degree of surface softness allows for the generation various touch sensations and tactile feedback. Additionally, the softness also affects the shape of the surface: a soft surface will allow the user to deform the surface at will while a hard surface will maintain its shape easier. In many traditional flexible surfaces to date, this element has been considered static and thus unchangeable. This project, in contrast, considers the softness of a surface to be dynamic and thus further explores the interaction possibilities with this type of surface. We demonstrate the possibilities of dynamically changing surfaces and their derived user interaction.
C1  - New York, NY, USA
C3  - Proceedings of the International Working Conference on Advanced Visual Interfaces
DA  - 2012///
PY  - 2012
DO  - 10.1145/2254556.2254719
SP  - 796
EP  - 797
PB  - Association for Computing Machinery
SN  - 978-1-4503-1287-5
UR  - https://doi.org/10.1145/2254556.2254719
KW  - dynamic softness control
KW  - input devices and strategies
KW  - interactive surface
KW  - tabletop
ER  - 

TY  - JOUR
TI  - Making Effective Use of Multicore Systems A Software Perspective: The Multicore Transformation (Ubiquity Symposium)
AU  - Cooper, Keith D.
T2  - Ubiquity
AB  - Multicore processors dominate the commercial marketplace, with the consequence that almost all computers are now parallel computers. To take maximum advantage of multicore chips, applications and systems should take advantage of that parallelism. As of today, a small fraction of applications do. To improve that situation and to capitalize fully on the power of multicore systems, we need to adopt programming models, parallel algorithms, and programming languages that are appropriate for the multicore world, and to integrate these ideas and tools into the courses that educate the next generation of computer scientists.
DA  - 2014/09//
PY  - 2014
DO  - 10.1145/2618407
VL  - 2014
IS  - September
UR  - https://doi.org/10.1145/2618407
ER  - 

TY  - CONF
TI  - XSearch: A Domain-Specific Cross-Language Relevant Question Retrieval Tool
AU  - Xu, Bowen
AU  - Xing, Zhenchang
AU  - Xia, Xin
AU  - Lo, David
AU  - Le, Xuan-Bach D.
T3  - ESEC/FSE 2017
AB  - During software development process, Chinese developers often seek solutions to the technical problems they encounter by searching relevant questions on Q&amp;A sites. When developers fail to find solutions on Q&amp;A sites in Chinese, they could translate their query and search on the English Q&amp;A sites. However, Chinese developers who are non-native English speakers often are not comfortable to ask or search questions in English, as they do not know the proper translation of the Chinese technical words into the English technical words. Furthermore, the process of manually formulating cross-language queries and determining the importance of query words is a tedious and time-consuming process. For the purpose of helping Chinese developers take advantages of the rich knowledge base of the English version of Stack Overflow and simplify the retrieval process, we propose an automated cross-language relevant question retrieval tool (XSearch) to retrieve relevant English questions on Stack Overflow for a given Chinese question. This tool can address the increasing need for developer to solve technical problems by retrieving cross-language relevant Q&amp;A resources. Demo Tool Website: http://172.93.36.10:8080/XSearch Demo Video: https://goo.gl/h57sed
C1  - New York, NY, USA
C3  - Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering
DA  - 2017///
PY  - 2017
DO  - 10.1145/3106237.3122820
SP  - 1009
EP  - 1013
PB  - Association for Computing Machinery
SN  - 978-1-4503-5105-8
UR  - https://doi.org/10.1145/3106237.3122820
KW  - Cross-Language Question Retrieval
KW  - Domain-Specific Translation
ER  - 

TY  - CONF
TI  - Rematerialization
AU  - Briggs, Preston
AU  - Cooper, Keith D.
AU  - Torczon, Linda
T3  - PLDI '92
AB  - This paper examines a problem that arises during global register allocation – rematerialization. If a value cannot be kept in a register, the allocator should recognize when it is cheaper to recompute the value (rematerialize it) than to store and reload it. Chaitin's original graph-coloring allocator handled simple instance of this problem correctly. This paper details a general solution to the problem and presents experimental evidence that shows its importance.Our approach is to tag individual values in the procedure's SSA graph with information specifying how it should be spilled. We use a variant of Wegman and Zadeck's sparse simple constant algorithm to propagate tags throughout the graph. The allocator then splits live ranges into values with different tags. This isolates those values that can be easily rematerialized from values that require general spilling. We modify the base allocator to use this information when estimating spill costs and introducing spill code.Our presentation focuses on rematerialization in the context of Chaitin's allocator; however, the problem arises in any global allocator. We believe that our approach will work in other allocators–while the details of implementation will vary, the key insights should carry over directly.
C1  - New York, NY, USA
C3  - Proceedings of the ACM SIGPLAN 1992 Conference on Programming Language Design and Implementation
DA  - 1992///
PY  - 1992
DO  - 10.1145/143095.143143
SP  - 311
EP  - 321
PB  - Association for Computing Machinery
SN  - 0-89791-475-9
UR  - https://doi.org/10.1145/143095.143143
ER  - 

TY  - JOUR
TI  - Rematerialization
AU  - Briggs, Preston
AU  - Cooper, Keith D.
AU  - Torczon, Linda
T2  - SIGPLAN Not.
AB  - This paper examines a problem that arises during global register allocation – rematerialization. If a value cannot be kept in a register, the allocator should recognize when it is cheaper to recompute the value (rematerialize it) than to store and reload it. Chaitin's original graph-coloring allocator handled simple instance of this problem correctly. This paper details a general solution to the problem and presents experimental evidence that shows its importance.Our approach is to tag individual values in the procedure's SSA graph with information specifying how it should be spilled. We use a variant of Wegman and Zadeck's sparse simple constant algorithm to propagate tags throughout the graph. The allocator then splits live ranges into values with different tags. This isolates those values that can be easily rematerialized from values that require general spilling. We modify the base allocator to use this information when estimating spill costs and introducing spill code.Our presentation focuses on rematerialization in the context of Chaitin's allocator; however, the problem arises in any global allocator. We believe that our approach will work in other allocators–while the details of implementation will vary, the key insights should carry over directly.
DA  - 1992/07//
PY  - 1992
DO  - 10.1145/143103.143143
VL  - 27
IS  - 7
SP  - 311
EP  - 321
SN  - 0362-1340
UR  - https://doi.org/10.1145/143103.143143
ER  - 

TY  - JOUR
TI  - A Reconfigurable Hardware Approach to Network Simulation
AU  - Stiliadis, Dimitrios
AU  - Varma, Anujan
T2  - ACM Trans. Model. Comput. Simul.
DA  - 1997/01//
PY  - 1997
DO  - 10.1145/244804.244809
VL  - 7
IS  - 1
SP  - 131
EP  - 156
SN  - 1049-3301
UR  - https://doi.org/10.1145/244804.244809
KW  - ATM switch scheduling
KW  - field-programmable gate array
KW  - hardware simulation
ER  - 

TY  - CONF
TI  - Tailoring Graph-Coloring Register Allocation For Runtime Compilation
AU  - Cooper, Keith D.
AU  - Dasgupta, Anshuman
T3  - CGO '06
AB  - Just-in-time compilers are invoked during application execution and therefore need to ensure fast compilation times. Consequently, runtime compiler designers are averse to implementing compile-time intensive optimization algorithms. Instead, they tend to select faster but less effective transformations. In this paper, we explore this trade-off for an important optimization - global register allocation. We present a graph-coloring register allocator that has been redesigned for runtime compilation. Compared to Chaitin- Briggs [7], a standard graph-coloring technique, the reformulated algorithm requires considerably less allocation time and produces allocations that are only marginally worse than those of Chaitin-Briggs. Our experimental results indicate that the allocator performs better than the linear-scan and Chaitin-Briggs allocators on most benchmarks in a runtime compilation environment. By increasing allocation efficiency and preserving optimization quality, the presented algorithm increases the suitability and profitability of a graph-coloring register allocation strategy for a runtime compiler.
C1  - USA
C3  - Proceedings of the International Symposium on Code Generation and Optimization
DA  - 2006///
PY  - 2006
DO  - 10.1109/CGO.2006.35
SP  - 39
EP  - 49
PB  - IEEE Computer Society
SN  - 0-7695-2499-0
UR  - https://doi.org/10.1109/CGO.2006.35
ER  - 

TY  - JOUR
TI  - Understanding the Microtask Crowdsourcing Experience for Workers with Disabilities: A Comparative View
AU  - Rechkemmer, Amy
AU  - Yin, Ming
T2  - Proc. ACM Hum.-Comput. Interact.
AB  - Microtask crowdsourcing holds great potential as an employment opportunity with the flexibility and anonymity that individuals with disability may require. Though prior research has explored the accessibility of crowd work, the lived crowd work experiences of the broader community of workers with disability are still largely under-explored, especially when it comes to how their experiences are similar to or different from the experiences of workers without disability. In this work, we aim to obtain a deeper understanding of the microtask crowdsourcing experience for people with disabilities, especially regarding their financial and social experiences of participating in crowd work, along with the benefits and challenges that they encounter through this work. Specifically, we first surveyed 1,200 crowd workers both with and without disability about their experiences using the Amazon Mechanical Turk platform, and the differences we found inspired the design of a follow-up survey to gain greater understanding of the crowd work experience for workers with disability. Our findings reveal that workers with disability receive unique benefits from performing crowd work, such as a greater sense of purpose, but also encounter many challenges, such as completing tasks on time and earning a livable wage, causing them to turn to online communities for assistance. Although many of the challenges they face are not unique to crowd workers with disability, workers with disability may be disproportionately impacted by these challenges. From our findings, we provide implications for crowd platforms, as well as the gig economy as a whole, that seek to promote greater consideration of workers with a diverse range of conditions to create a more valuable work experience for them.
DA  - 2022/11//
PY  - 2022
DO  - 10.1145/3555137
VL  - 6
IS  - CSCW2
UR  - https://doi.org/10.1145/3555137
KW  - survey
KW  - accessibility
KW  - crowdsourcing
KW  - mechanical turk
ER  - 

TY  - CONF
TI  - Scalable Skyline Computation Using Object-Based Space Partitioning
AU  - Zhang, Shiming
AU  - Mamoulis, Nikos
AU  - Cheung, David W.
T3  - SIGMOD '09
AB  - The skyline operator returns from a set of multi-dimensional objects a subset of superior objects that are not dominated by others. This operation is considered very important in multi-objective analysis of large datasets. Although a large number of skyline methods have been proposed, the majority of them focuses on minimizing the I/O cost. However, in high dimensional spaces, the problem can easily become CPU-bound due to the large number of computations required for comparing objects with current skyline points while scanning the database. Based on this observation, we propose a dynamic indexing technique for skyline points that can be integrated into state-of-the-art sort-based skyline algorithms to boost their computational performance. The new indexing and dominance checking approach is supported by a theoretical analysis, while our experiments show that it scales well with the input size and dimensionality not only because unnecessary dominance checks are avoided but also because it allows efficient dominance checking with the help of bitwise operations.
C1  - New York, NY, USA
C3  - Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data
DA  - 2009///
PY  - 2009
DO  - 10.1145/1559845.1559897
SP  - 483
EP  - 494
PB  - Association for Computing Machinery
SN  - 978-1-60558-551-2
UR  - https://doi.org/10.1145/1559845.1559897
KW  - preference
KW  - skyline
KW  - space partitioning
ER  - 

TY  - CONF
TI  - Efficient Computation of Flow Insensitive Interprocedural Summary Information
AU  - Cooper, Keith D.
AU  - Kennedy, Ken
T3  - SIGPLAN '84
C1  - New York, NY, USA
C3  - Proceedings of the 1984 SIGPLAN Symposium on Compiler Construction
DA  - 1984///
PY  - 1984
DO  - 10.1145/502874.502898
SP  - 247
EP  - 258
PB  - Association for Computing Machinery
SN  - 0-89791-139-3
UR  - https://doi.org/10.1145/502874.502898
ER  - 

TY  - JOUR
TI  - Efficient Computation of Flow Insensitive Interprocedural Summary Information
AU  - Cooper, Keith D.
AU  - Kennedy, Ken
T2  - SIGPLAN Not.
DA  - 1984/06//
PY  - 1984
DO  - 10.1145/502949.502898
VL  - 19
IS  - 6
SP  - 247
EP  - 258
SN  - 0362-1340
UR  - https://doi.org/10.1145/502949.502898
ER  - 

TY  - CONF
TI  - Interprocedural Side-Effect Analysis in Linear Time
AU  - Cooper, K. D.
AU  - Kennedy, K.
T3  - PLDI '88
AB  - We present a new method for solving Banning's alias-free flow-insensitive side-effect analysis problem. The algorithm employs a new data structure, called the binding multi-graph, along with depth-first search to achieve a running time that is linear in the size of the call multi-graph of the program. This method can be extended to produce fast algorithms for data-flow problems with more complex lattice structures.
C1  - New York, NY, USA
C3  - Proceedings of the ACM SIGPLAN 1988 Conference on Programming Language Design and Implementation
DA  - 1988///
PY  - 1988
DO  - 10.1145/53990.53996
SP  - 57
EP  - 66
PB  - Association for Computing Machinery
SN  - 0-89791-269-1
UR  - https://doi.org/10.1145/53990.53996
ER  - 

TY  - JOUR
TI  - Interprocedural Side-Effect Analysis in Linear Time
AU  - Cooper, K. D.
AU  - Kennedy, K.
T2  - SIGPLAN Not.
AB  - We present a new method for solving Banning's alias-free flow-insensitive side-effect analysis problem. The algorithm employs a new data structure, called the binding multi-graph, along with depth-first search to achieve a running time that is linear in the size of the call multi-graph of the program. This method can be extended to produce fast algorithms for data-flow problems with more complex lattice structures.
DA  - 1988/06//
PY  - 1988
DO  - 10.1145/960116.53996
VL  - 23
IS  - 7
SP  - 57
EP  - 66
SN  - 0362-1340
UR  - https://doi.org/10.1145/960116.53996
ER  - 

TY  - JOUR
TI  - Interprocedural Side-Effect Analysis in Linear Time
AU  - Cooper, Keith D.
AU  - Kennedy, Ken
T2  - SIGPLAN Not.
AB  - We present a new method for solving Banning's alias-free flow-insensitive side-effect analysis problem. The algorithm employs a new data structure, called the binding multi-graph, along with depth-first search to achieve a running time that is linear in the size of the call multi-graph of the program. This method can be extended to produce fast algorithms for data-flow problems with more complex lattice structures.
DA  - 2004/04//
PY  - 2004
DO  - 10.1145/989393.989418
VL  - 39
IS  - 4
SP  - 217
EP  - 228
SN  - 0362-1340
UR  - https://doi.org/10.1145/989393.989418
ER  - 

TY  - CONF
TI  - Redundancy Elimination Revisited
AU  - Cooper, Keith
AU  - Eckhardt, Jason
AU  - Kennedy, Ken
T3  - PACT '08
AB  - This work proposes and evaluates improvements to previously known algorithms for redundancy elimination.Enhanced Scalar Replacement combines two classic techniques, scalar replacement and hash-based value numbering. The former detects redundant array references within and across loop iterations, while the latter detects a large class of redundancies, but only within a single loop iteration. By integrating the two techniques, ESR detects and eliminates a wider range of expression redundancies across loop iterations.We also extend hash-based value numbering to perform reassociation. Classic redundancy elimination techniques operate on an intermediate representation of the program in which operand association and order is of fixed shape. This rigidity in code shape may sometimes obscure redundancies. Our optimizer attempts to shape the code by changing associativity, exposing more redundancies. Opportunities for ESR, in particular, are increased with reassociation.
C1  - New York, NY, USA
C3  - Proceedings of the 17th International Conference on Parallel Architectures and Compilation Techniques
DA  - 2008///
PY  - 2008
DO  - 10.1145/1454115.1454120
SP  - 12
EP  - 21
PB  - Association for Computing Machinery
SN  - 978-1-60558-282-5
UR  - https://doi.org/10.1145/1454115.1454120
KW  - expression optimization
KW  - loop optimization
KW  - reassociation
KW  - redundancy elimination
KW  - scalar replacement
ER  - 

TY  - JOUR
TI  - Operator Strength Reduction
AU  - Cooper, Keith D.
AU  - Simpson, L. Taylor
AU  - Vick, Christopher A.
T2  - ACM Trans. Program. Lang. Syst.
AB  - Operator strength reduction is a technique that improves compiler-generated code by reformulating certain costly computations in terms of less expensive ones. A common case arises in array addressing expressions used in loops. The compiler can replace the sequence of multiplies generated by a direct translation of the address expression with an equivalent sequence of additions. When combined with linear function test replacement, strength reduction can speed up the execution of loops containing array references. The improvement comes from two sources: a reduction in the number of operations needed to implement the loop and the use of less costly operations.This paper presents a new algorithm for operator strength reduction, called OSR. OSR improves upon an earlier algorithm of Allen, Cocke, and Kennedy [Allen et al. 1981]. OSR operates on the static single assignment (SSA) form of a procedure [Cytron et al. 1991]. By taking advantage of the properties of SSA form, we have derived an algorithm that is simple to understand, quick to implement, and, in practice, fast to run. Its asymptotic complexity is, in the worst case, the same as the Allen, Cocke,and Kennedy algorithm (ACK). OSR achieves optimization results that are equivalent to those obtained with the ACK algorithm. OSR has been implemented in several research and production compilers.
DA  - 2001/09//
PY  - 2001
DO  - 10.1145/504709.504710
VL  - 23
IS  - 5
SP  - 603
EP  - 625
SN  - 0164-0925
UR  - https://doi.org/10.1145/504709.504710
KW  - loops
KW  - static single assignment form
KW  - strength reduction
ER  - 

TY  - CONF
TI  - Time-Space Trade-Offs for Undirected St-Connectivity on a JAG
AU  - Edmonds, Jeff
T3  - STOC '93
C1  - New York, NY, USA
C3  - Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing
DA  - 1993///
PY  - 1993
DO  - 10.1145/167088.167272
SP  - 718
EP  - 727
PB  - Association for Computing Machinery
SN  - 0-89791-591-7
UR  - https://doi.org/10.1145/167088.167272
ER  - 

TY  - JOUR
TI  - Explicit OR-Dispersers with Polylogarithmic Degree
AU  - Saks, Michael
AU  - Srinivasan, Aravind
AU  - Zhou, Shiyu
T2  - J. ACM
AB  - An (N, M, T)-OR-disperser is a bipartite multigraph G=(V, W, E) with |V| = N, and |W| = M, having the following expansion property: any subset of V having at least T vertices has a neighbor set of size at least M/2. For any pair of constants ξ, λ, 1 ≥ ξ &gt; λ ≥ 0, any sufficiently large N, and for any T ≥ 2(logN) M ≤ 2(log N)λ, we give an explicit elementary construction of an (N, M, T)-OR-disperser such that the out-degree of any vertex in V is at most polylogarithmic in N. Using this with known applications of OR-dispersers yields several results. First, our construction implies that the complexity class Strong-RP defined by Sipser, equals RP. Second, for any fixed η &gt; 0, we give the first polynomial-time simulation of RP algorithms using the output of any “η-minimally random” source. For any integral R &gt; 0, such a source accepts a single request for an R-bit string and generates the string according to a distribution that assigns probability at most 2−Rη to any string. It is minimally random in the sense that any weaker source is insufficient to do a black-box polynomial-time simulation of RP algorithms.
DA  - 1998/01//
PY  - 1998
DO  - 10.1145/273865.273915
VL  - 45
IS  - 1
SP  - 123
EP  - 154
SN  - 0004-5411
UR  - https://doi.org/10.1145/273865.273915
KW  - derandomization
KW  - expander graphs
KW  - explicit constructions
KW  - hardness of approximation
KW  - hashing lemmas
KW  - imperfect sources of randomness
KW  - measures of information
KW  - pseudo-random generators
KW  - randomized computation
KW  - time-space tradeoffs
ER  - 

TY  - JOUR
TI  - The Impact of Interprocedural Analysis and Optimization in the Rn Programming Environment
AU  - Cooper, Keith D.
AU  - Kennedy, Ken
AU  - Torczon, Linda
T2  - ACM Trans. Program. Lang. Syst.
AB  - In spite of substantial progress in the theory of interprocedural data flow analysis, few practical compiling systems can afford to apply it to produce more efficient object programs. To perform interprocedural analysis, a compiler needs not only the source code of the module being compiled, but also information about the side effects of every procedure in the program containing that module, even separately compiled procedures. In a conventional batch compiler system, the increase in compilation time required to gather this information would make the whole process impractical. In an integrated programming environment, however, other tools can cooperate with the compiler to compute the necessary interprocedural information incrementally. as the program is being developed, decreasing both the overall cost of the analysis and the cost of individual compilations.A central goal of the Rn project at Rice University is to construct a prototype software development environment that is designed to build whole programs, rather than just individual modules. It employs interprocedural analysis and optimization to produce high-quality machine code for whole programs. This paper presents an overview of the methods used by the environment to accomplish this task and discusses the impact of these methods on the various environment components. The responsibilities of each component of the environment for the preparation and use of interprocedural information are presented in detail.
DA  - 1986/08//
PY  - 1986
DO  - 10.1145/6465.6489
VL  - 8
IS  - 4
SP  - 491
EP  - 523
SN  - 0164-0925
UR  - https://doi.org/10.1145/6465.6489
ER  - 

TY  - JOUR
TI  - Implementation of Dynamic Trees with In-Subtree Operations
AU  - Radzik, Tomasz
T2  - ACM J. Exp. Algorithmics
AB  - We describe an implementation of dynamic trees with "in-subtree" operations. Our implementation follows Sleator and Tarjan's framework of dynamic-tree implementations based on splay trees. We consider the following two examples of "in-subtree" operations. (a) For a given node v, find a node with the minimum key in the subtree rooted at v. (b) For a given node v, find a random node with key X in the subtree rooted at v (value X is fixed throughout the whole computation). The first operation may provide support for edge deletions in the dynamic minimum spanning tree problem. The second one may be useful in local search methods for degree-constrained minimum spanning tree problems. We conducted experiments with our dynamic-tree implementation within these two contexts, and the results suggest that this implementation may lead to considerably faster codes than straightforward approaches do.
DA  - 1998/09//
PY  - 1998
DO  - 10.1145/297096.297144
VL  - 3
SP  - 9
EP  - es
SN  - 1084-6654
UR  - https://doi.org/10.1145/297096.297144
KW  - algorithms
KW  - experimentation
KW  - design
KW  - dynamic minimum spanning tree
KW  - dynamic trees
KW  - performance
KW  - splay trees
ER  - 

TY  - CONF
TI  - Practical Methods for Shape Fitting and Kinetic Data Structures Using Core Sets
AU  - Yu, Hai
AU  - Agarwal, Pankaj K.
AU  - Poreddy, Raghunath
AU  - Varadarajan, Kasturi R.
T3  - SCG '04
AB  - The notion of ε-kernel was introduced by Agarwal et al. to set up a unified framework for computing various extent measures of a point set p approximately. Roughly speaking, a subset Q ⊆ P is an ε-kernel of P if for every slab W containing Q, the expanded slab (1+ε)W contains P. They illustrated the significance of an ε-kernel by showing that it yields approximation algorithms for a wide range of problems.We present a simpler and more practical algorithm for computing the ε-kernel of a set P of points in ℝ3. We demonstrate the practicality of our algorithm by showing its empirical performance on various inputs. We then describe an incremental algorithm for fitting various shapes and use the ideas of our algorithm for computing ε-kernels to analyze the performance of this algorithm. We illustrate the versatility and practicality of this technique by implementing approximation algorithms for minimum enclosing cylinder, minimum-volume bounding box, and minimum-width annulus. Finally, we show that ε-kernels can be effectively used to expedite the algorithms for maintaining extents of moving points.
C1  - New York, NY, USA
C3  - Proceedings of the Twentieth Annual Symposium on Computational Geometry
DA  - 2004///
PY  - 2004
DO  - 10.1145/997817.997858
SP  - 263
EP  - 272
PB  - Association for Computing Machinery
SN  - 1-58113-885-7
UR  - https://doi.org/10.1145/997817.997858
KW  - core sets
KW  - extent measures
KW  - kinetic data structures
KW  - shape fitting
ER  - 

TY  - CONF
TI  - Effective Partial Redundancy Elimination
AU  - Briggs, Preston
AU  - Cooper, Keith D.
T3  - PLDI '94
AB  - Partial redundancy elimination is a code optimization with a long history of literature and implementation. In practice, its effectiveness depends on issues of naming and code shape. This paper shows that a combination of global reassociation and global value numbering can increase the effectiveness of partial redundancy elimination. By imposing a discipline on the choice of names and the shape of expressions, we are able to expose more redundancies.As part of the work, we introduce a new algorithm for global reassociation of expressions. It uses global information to reorder expressions, creating opportunities for other optimizations. The new algorithm generalizes earlier work that ordered FORTRAN array address expressions to improve otpimization [25].
C1  - New York, NY, USA
C3  - Proceedings of the ACM SIGPLAN 1994 Conference on Programming Language Design and Implementation
DA  - 1994///
PY  - 1994
DO  - 10.1145/178243.178257
SP  - 159
EP  - 170
PB  - Association for Computing Machinery
SN  - 0-89791-662-X
UR  - https://doi.org/10.1145/178243.178257
ER  - 

TY  - JOUR
TI  - Effective Partial Redundancy Elimination
AU  - Briggs, Preston
AU  - Cooper, Keith D.
T2  - SIGPLAN Not.
AB  - Partial redundancy elimination is a code optimization with a long history of literature and implementation. In practice, its effectiveness depends on issues of naming and code shape. This paper shows that a combination of global reassociation and global value numbering can increase the effectiveness of partial redundancy elimination. By imposing a discipline on the choice of names and the shape of expressions, we are able to expose more redundancies.As part of the work, we introduce a new algorithm for global reassociation of expressions. It uses global information to reorder expressions, creating opportunities for other optimizations. The new algorithm generalizes earlier work that ordered FORTRAN array address expressions to improve otpimization [25].
DA  - 1994/06//
PY  - 1994
DO  - 10.1145/773473.178257
VL  - 29
IS  - 6
SP  - 159
EP  - 170
SN  - 0362-1340
UR  - https://doi.org/10.1145/773473.178257
ER  - 

TY  - JOUR
TI  - ShareJIT: JIT Code Cache Sharing across Processes and Its Practical Implementation
AU  - Xu, Xiaoran
AU  - Cooper, Keith
AU  - Brock, Jacob
AU  - Zhang, Yan
AU  - Ye, Handong
T2  - Proc. ACM Program. Lang.
AB  - Just-in-time (JIT) compilation coupled with code caching are widely used to improve performance in dynamic programming language implementations. These code caches, along with the associated profiling data for the hot code, however, consume significant amounts of memory. Furthermore, they incur extra JIT compilation time for their creation. On Android, the current standard JIT compiler and its code caches are not shared among processes—that is, the runtime system maintains a private code cache, and its associated data, for each runtime process. However, applications running on the same platform tend to share multiple libraries in common. Sharing cached code across multiple applications and multiple processes can lead to a reduction in memory use. It can directly reduce compile time. It can also reduce the cumulative amount of time spent interpreting code. All three of these effects can improve actual runtime performance. In this paper, we describe ShareJIT, a global code cache for JITs that can share code across multiple applications and multiple processes. We implemented ShareJIT in the context of the Android Runtime (ART), a widely used, state-of-the-art system. To increase sharing, our implementation constrains the amount of context that the JIT compiler can use to optimize the code. This exposes a fundamental tradeoff: increased specialization to a single process’ context decreases the extent to which the compiled code can be shared. In ShareJIT, we limit some optimization to increase shareability. To evaluate the ShareJIT, we tested 8 popular Android apps in a total of 30 experiments. ShareJIT improved overall performance by 9% on average, while decreasing memory consumption by 16% on average and JIT compilation time by 37% on average.
DA  - 2018/10//
PY  - 2018
DO  - 10.1145/3276494
VL  - 2
IS  - OOPSLA
UR  - https://doi.org/10.1145/3276494
KW  - Android Runtime System
KW  - Code Cache Sharing
KW  - JIT Compilation
ER  - 

TY  - CONF
TI  - COP-E-CAT: Cleaning and Organization Pipeline for EHR Computational and Analytic Tasks
AU  - Mandyam, Aishwarya
AU  - Yoo, Elizabeth C.
AU  - Soules, Jeff
AU  - Laudanski, Krzysztof
AU  - Engelhardt, Barbara E.
T3  - BCB '21
AB  - In order to ensure that analyses of complex electronic healthcare record (EHR) data are reproducible and generalizable, it is crucial for researchers to use comparable preprocessing, filtering, and imputation strategies. We introduce COP-E-CAT: Cleaning and Organization Pipeline for EHR Computational and Analytic Tasks, an open-source processing and analysis software for MIMIC-IV, a ubiquitous benchmark EHR dataset. COP-E-CAT allows users to select filtering characteristics and preprocess covariates to generate data structures for use in downstream analysis tasks. This user-friendly approach shows promise in facilitating reproducibility and comparability among studies that leverage the MIMIC-IV data, and enhances EHR accessibility to a wider spectrum of researchers than current data processing methods. We demonstrate the versatility of our workflow by describing three use cases: ensemble prediction, reinforcement learning, and dimension reduction. The software is available at: https://github.com/eyeshoe/cop-e-cat.
C1  - New York, NY, USA
C3  - Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics
DA  - 2021///
PY  - 2021
DO  - 10.1145/3459930.3469536
PB  - Association for Computing Machinery
SN  - 978-1-4503-8450-6
UR  - https://doi.org/10.1145/3459930.3469536
KW  - electronic health records
KW  - health informatics
KW  - reinforcement learning
ER  - 

TY  - CONF
TI  - Efficient Performance Estimation for General Real-Time Task Systems
AU  - Liu, Hongchao Stephanie
AU  - Hu, Xiaobo Sharon
T3  - ICCAD '01
AB  - The paper presents a novel approach to compute tight upper bounds on the processor utilization independent of the implementation for general real-time systems where tasks are composed of subtasks and precedence constraints may exist among subtasks of the same task. We formulate the problem as a set of linear programming (LP) problems. Observations are made to reduce the number of LP problem instances required to be solved, which greatly improves the computation time of the utilization bounds. Furthermore, additional constraints are allowed to be included under certain circumstances to improve the quality of the bounds.
C3  - Proceedings of the 2001 IEEE/ACM International Conference on Computer-Aided Design
DA  - 2001///
PY  - 2001
SP  - 464
EP  - 470
PB  - IEEE Press
SN  - 0-7803-7249-2
ER  - 

TY  - JOUR
TI  - Type-Based Detection of XML Query-Update Independence
AU  - Bidoit-Tollu, Nicole
AU  - Colazzo, Dario
AU  - Ulliana, Federico
T2  - Proc. VLDB Endow.
AB  - This paper presents a novel static analysis technique to detect XML query-update independence, in the presence of a schema. Rather than types, our system infers chains of types. Each chain represents a path that can be traversed on a valid document during query/update evaluation. The resulting independence analysis is precise, although it raises a challenging issue: recursive schemas may lead to inference of infinitely many chains.A sound and complete approximation technique ensuring a finite analysis in any case is presented, together with an efficient implementation performing the chain-based analysis in polynomial space and time.
DA  - 2012/05//
PY  - 2012
DO  - 10.14778/2311906.2311914
VL  - 5
IS  - 9
SP  - 872
EP  - 883
SN  - 2150-8097
UR  - https://doi.org/10.14778/2311906.2311914
ER  - 

TY  - CONF
TI  - Enhanced Code Compression for Embedded RISC Processors
AU  - Cooper, Keith D.
AU  - McIntosh, Nathaniel
T3  - PLDI '99
AB  - This paper explores compiler techniques for reducing the memory needed to load and run program executables. In embedded systems, where economic incentives to reduce both RAM and ROM are strong, the size of compiled code is increasingly important. Similarly, in mobile and network computing, the need to transmit an executable before running it places a premium on code size. Our work focuses on reducing the size of a program's code segment, using pattern-matching techniques to identify and coalesce together repeated instruction sequences. In contrast to other methods, our framework preserves the ability to run program executables directly, without an intervening decompression stage. Our compression framework is integrated into an industrial-strength optimizing compiler, which allows us to explore the interaction between code compression and classical code optimization techniques, and requires that we contend with the difficulties of compressing previously optimized code. The specific contributions in this paper include a comprehensive experimental evaluation of code compression for a RISC-like architecture, a more powerful pattern-matching scheme for improved identification of repeated code fragments, and a new form of profile-driven code compression that reduces the speed penalty arising from compression.
C1  - New York, NY, USA
C3  - Proceedings of the ACM SIGPLAN 1999 Conference on Programming Language Design and Implementation
DA  - 1999///
PY  - 1999
DO  - 10.1145/301618.301655
SP  - 139
EP  - 149
PB  - Association for Computing Machinery
SN  - 1-58113-094-5
UR  - https://doi.org/10.1145/301618.301655
ER  - 

TY  - JOUR
TI  - Enhanced Code Compression for Embedded RISC Processors
AU  - Cooper, Keith D.
AU  - McIntosh, Nathaniel
T2  - SIGPLAN Not.
AB  - This paper explores compiler techniques for reducing the memory needed to load and run program executables. In embedded systems, where economic incentives to reduce both RAM and ROM are strong, the size of compiled code is increasingly important. Similarly, in mobile and network computing, the need to transmit an executable before running it places a premium on code size. Our work focuses on reducing the size of a program's code segment, using pattern-matching techniques to identify and coalesce together repeated instruction sequences. In contrast to other methods, our framework preserves the ability to run program executables directly, without an intervening decompression stage. Our compression framework is integrated into an industrial-strength optimizing compiler, which allows us to explore the interaction between code compression and classical code optimization techniques, and requires that we contend with the difficulties of compressing previously optimized code. The specific contributions in this paper include a comprehensive experimental evaluation of code compression for a RISC-like architecture, a more powerful pattern-matching scheme for improved identification of repeated code fragments, and a new form of profile-driven code compression that reduces the speed penalty arising from compression.
DA  - 1999/05//
PY  - 1999
DO  - 10.1145/301631.301655
VL  - 34
IS  - 5
SP  - 139
EP  - 149
SN  - 0362-1340
UR  - https://doi.org/10.1145/301631.301655
ER  - 

TY  - CONF
TI  - On the Effect of Randomness on Planted 3-Coloring Models
AU  - David, Roee
AU  - Feige, Uriel
T3  - STOC '16
AB  - We present the hosted coloring framework for studying al- gorithmic and hardness results for the k-coloring problem. There is a class H of host graphs. One selects a graph H ∈ H and plants in it a balanced k-coloring (by partitioning the vertex set into k roughly equal parts, and removing all edges within each part). The resulting graph G is given as input to a polynomial time algorithm that needs to k-color G (any legal k-coloring would do – the algorithm is not required to recover the planted k-coloring). Earlier planted models correspond to the case that H is the class of all n-vertex d-regular graphs, a member H ∈ H is chosen at random, and then a balanced k-coloring is planted at random. Blum and Spencer [1995] designed algorithms for this model when d = n δ (for 0 &lt; δ ≤ 1), and Alon and Kahale [1997] managed to do so even when d is a sufficiently large constant. The new aspect in our framework is that it need not in- volve randomness. In one model within the framework (with k = 3) H is a d regular spectral expander (meaning that ex- cept for the largest eigenvalue of its adjacency matrix, every other eigenvalue has absolute value much smaller than d) chosen by an adversary, and the planted 3-coloring is ran- dom. We show that the 3-coloring algorithm of Alon and Kahale [1997] can be modified to apply to this case. In an- other model H is a random d-regular graph but the planted balanced 3-coloring is chosen by an adversary, after seeing H. We show that for a certain range of average degrees somewhat below √ n, finding a 3-coloring is NP-hard. To- gether these results (and other results that we have) help clarify which aspects of randomness in the planted coloring model are the key to successful 3-coloring algorithms.
C1  - New York, NY, USA
C3  - Proceedings of the Forty-Eighth Annual ACM Symposium on Theory of Computing
DA  - 2016///
PY  - 2016
DO  - 10.1145/2897518.2897561
SP  - 77
EP  - 90
PB  - Association for Computing Machinery
SN  - 978-1-4503-4132-5
UR  - https://doi.org/10.1145/2897518.2897561
KW  - Adversary
KW  - Exact recovery
KW  - Expanders
KW  - NP-hardness
ER  - 

TY  - CONF
TI  - Compiler-Controlled Memory
AU  - Cooper, Keith D.
AU  - Harvey, Timothy J.
T3  - ASPLOS VIII
AB  - Optimizations aimed at reducing the impact of memory operations on execution speed have long concentrated on improving cache performance. These efforts achieve a. reasonable level of success. The primary limit on the compiler's ability to improve memory behavior is its imperfect knowledge about the run-time behavior of the program. The compiler cannot completely predict runtime access patterns.There is an exception to this rule. During the register allocation phase, the compiler often must insert substantial amounts of spill code; that is, instructions that move values from registers to memory and back again. Because the compiler itself inserts these memory instructions, it has more knowledge about them than other memory operations in the program.Spill-code operations are disjoint from the memory manipulations required by the semantics of the program being compiled, and, indeed, the two can interfere in the cache. This paper proposes a hardware solution to the problem of increased spill costs—a small compiler-controlled memory (CCM) to hold spilled values. This small random-access memory can (and should) be placed in a distinct address space from the main memory hierarchy. The compiler can target spill instructions to use the CCM, moving most compiler-inserted memory traffic out of the pathway to main memory and eliminating any impact that those spill instructions would have on the state of the main memory hierarchy. Such memories already exist on some DSP microprocessors. Our techniques can be applied directly on those chips.This paper presents two compiler-based methods to exploit such a memory, along with experimental results showing that speedups from using CCM may be sizable. It shows that using the register allocation's coloring paradigm to assign spilled values to memory can greatly reduce the amount of memory required by a program.
C1  - New York, NY, USA
C3  - Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems
DA  - 1998///
PY  - 1998
DO  - 10.1145/291069.291010
SP  - 2
EP  - 11
PB  - Association for Computing Machinery
SN  - 1-58113-107-0
UR  - https://doi.org/10.1145/291069.291010
ER  - 

TY  - JOUR
TI  - Compiler-Controlled Memory
AU  - Cooper, Keith D.
AU  - Harvey, Timothy J.
T2  - SIGPLAN Not.
AB  - Optimizations aimed at reducing the impact of memory operations on execution speed have long concentrated on improving cache performance. These efforts achieve a. reasonable level of success. The primary limit on the compiler's ability to improve memory behavior is its imperfect knowledge about the run-time behavior of the program. The compiler cannot completely predict runtime access patterns.There is an exception to this rule. During the register allocation phase, the compiler often must insert substantial amounts of spill code; that is, instructions that move values from registers to memory and back again. Because the compiler itself inserts these memory instructions, it has more knowledge about them than other memory operations in the program.Spill-code operations are disjoint from the memory manipulations required by the semantics of the program being compiled, and, indeed, the two can interfere in the cache. This paper proposes a hardware solution to the problem of increased spill costs—a small compiler-controlled memory (CCM) to hold spilled values. This small random-access memory can (and should) be placed in a distinct address space from the main memory hierarchy. The compiler can target spill instructions to use the CCM, moving most compiler-inserted memory traffic out of the pathway to main memory and eliminating any impact that those spill instructions would have on the state of the main memory hierarchy. Such memories already exist on some DSP microprocessors. Our techniques can be applied directly on those chips.This paper presents two compiler-based methods to exploit such a memory, along with experimental results showing that speedups from using CCM may be sizable. It shows that using the register allocation's coloring paradigm to assign spilled values to memory can greatly reduce the amount of memory required by a program.
DA  - 1998/10//
PY  - 1998
DO  - 10.1145/291006.291010
VL  - 33
IS  - 11
SP  - 2
EP  - 11
SN  - 0362-1340
UR  - https://doi.org/10.1145/291006.291010
ER  - 

TY  - JOUR
TI  - Compiler-Controlled Memory
AU  - Cooper, Keith D.
AU  - Harvey, Timothy J.
T2  - SIGOPS Oper. Syst. Rev.
AB  - Optimizations aimed at reducing the impact of memory operations on execution speed have long concentrated on improving cache performance. These efforts achieve a. reasonable level of success. The primary limit on the compiler's ability to improve memory behavior is its imperfect knowledge about the run-time behavior of the program. The compiler cannot completely predict runtime access patterns.There is an exception to this rule. During the register allocation phase, the compiler often must insert substantial amounts of spill code; that is, instructions that move values from registers to memory and back again. Because the compiler itself inserts these memory instructions, it has more knowledge about them than other memory operations in the program.Spill-code operations are disjoint from the memory manipulations required by the semantics of the program being compiled, and, indeed, the two can interfere in the cache. This paper proposes a hardware solution to the problem of increased spill costs—a small compiler-controlled memory (CCM) to hold spilled values. This small random-access memory can (and should) be placed in a distinct address space from the main memory hierarchy. The compiler can target spill instructions to use the CCM, moving most compiler-inserted memory traffic out of the pathway to main memory and eliminating any impact that those spill instructions would have on the state of the main memory hierarchy. Such memories already exist on some DSP microprocessors. Our techniques can be applied directly on those chips.This paper presents two compiler-based methods to exploit such a memory, along with experimental results showing that speedups from using CCM may be sizable. It shows that using the register allocation's coloring paradigm to assign spilled values to memory can greatly reduce the amount of memory required by a program.
DA  - 1998/10//
PY  - 1998
DO  - 10.1145/384265.291010
VL  - 32
IS  - 5
SP  - 2
EP  - 11
SN  - 0163-5980
UR  - https://doi.org/10.1145/384265.291010
ER  - 

TY  - CONF
TI  - Coloring Heuristics for Register Allocation
AU  - Briggs, P.
AU  - Cooper, K. D.
AU  - Kennedy, K.
AU  - Torczon, L.
T3  - PLDI '89
AB  - We describe an improvement to a heuristic introduced by Chaitin for use in graph coloring register allocation. Our modified heuristic produces better colorings, with less spill code. It has similar compile-time and implementation requirements. We present experimental data to compare the two methods.
C1  - New York, NY, USA
C3  - Proceedings of the ACM SIGPLAN 1989 Conference on Programming Language Design and Implementation
DA  - 1989///
PY  - 1989
DO  - 10.1145/73141.74843
SP  - 275
EP  - 284
PB  - Association for Computing Machinery
SN  - 0-89791-306-X
UR  - https://doi.org/10.1145/73141.74843
ER  - 

TY  - JOUR
TI  - Coloring Heuristics for Register Allocation
AU  - Briggs, P.
AU  - Cooper, K. D.
AU  - Kennedy, K.
AU  - Torczon, L.
T2  - SIGPLAN Not.
AB  - We describe an improvement to a heuristic introduced by Chaitin for use in graph coloring register allocation. Our modified heuristic produces better colorings, with less spill code. It has similar compile-time and implementation requirements. We present experimental data to compare the two methods.
DA  - 1989/06//
PY  - 1989
DO  - 10.1145/74818.74843
VL  - 24
IS  - 7
SP  - 275
EP  - 284
SN  - 0362-1340
UR  - https://doi.org/10.1145/74818.74843
ER  - 

TY  - JOUR
TI  - Coloring Register Pairs
AU  - Briggs, Preston
AU  - Cooper, Keith D.
AU  - Torczon, Linda
T2  - ACM Lett. Program. Lang. Syst.
AB  - Many architectures require that a program use pairs of adjacent registers to hold double-precision floating-point values. Register allocators based on Chaitin's graph-coloring technique have trouble with programs that contain both single-register values and values that require adjacent pairs of registers. In particular, Chaitin's algorithm often produces excessive spilling on such programs. This results in underuse of the register set; the extra loads and stores inserted into the program for spilling also slow execution.An allocator based on an optimistic coloring scheme naturally avoids this problem. Such allocators delay the decision to spill a value until late in the allocation process. This eliminates the over-spilling provoked by adjacent register pairs in Chaitin's scheme.This paper discusses the representation of register pairs in a graph coloring allocator. It explains the problems that arise with Chaitin's allocator and shows how the optimistic allocator avoids them. It provides a rationale for determining how to add larger aggregates to the interference graph.
DA  - 1992/03//
PY  - 1992
DO  - 10.1145/130616.130617
VL  - 1
IS  - 1
SP  - 3
EP  - 13
SN  - 1057-4514
UR  - https://doi.org/10.1145/130616.130617
KW  - graph coloring
KW  - register allocation
ER  - 

TY  - JOUR
TI  - Efficient Computation of Flow-Insensitive Interprocedural Summary Information—a Correction
AU  - Cooper, Keith D.
AU  - Kennedy, Ken
T2  - SIGPLAN Not.
DA  - 1988/04//
PY  - 1988
DO  - 10.1145/44326.44329
VL  - 23
IS  - 4
SP  - 35
EP  - 42
SN  - 0362-1340
UR  - https://doi.org/10.1145/44326.44329
ER  - 

TY  - CONF
TI  - Combined Fault Tolerance and Scheduling Techniques for Workflow Applications on Computational Grids
AU  - Zhang, Yang
AU  - Mandal, Anirban
AU  - Koelbel, Charles
AU  - Cooper, Keith
T3  - CCGRID '09
AB  - Complex scientific workflows are now Increasingly executed on computational grids. In addition to the challenges of managing and scheduling these workflows, reliability challenges arise because of the unreliable nature of large-scale grid infrastructure. Fault tolerance mechanisms like over-provisioning and checkpoint-recovery are used in current grid application management systems to address these reliability challenges. In this work, we propose new approaches that combine these fault tolerance techniques with existing workflow scheduling algorithms. We present a study on the effectiveness of the combined approaches by analyzing their impact on the reliability of workflow execution, workflow performance and resource usage under different reliability models, failure prediction accuracies and workflow application types.
C1  - USA
C3  - Proceedings of the 2009 9th IEEE/ACM International Symposium on Cluster Computing and the Grid
DA  - 2009///
PY  - 2009
DO  - 10.1109/CCGRID.2009.59
SP  - 244
EP  - 251
PB  - IEEE Computer Society
SN  - 978-0-7695-3622-4
UR  - https://doi.org/10.1109/CCGRID.2009.59
KW  - Grid Computing
KW  - Workflow Scheduling
KW  - Fault Tolerant
KW  - Performance Modeling
ER  - 

TY  - JOUR
TI  - Unexpected Side Effects of Inline Substitution: A Case Study
AU  - Cooper, Keith D.
AU  - Hall, Mary W.
AU  - Torczon, Linda
T2  - ACM Lett. Program. Lang. Syst.
AB  - The structure of a program can encode implicit information that changes both the shape and speed of the generated code. Interprocedural transformations like inlining often discard such information; using interprocedural data-flow information as a basis for optimization can have the same effect.In the course of a study on inline substitution with commercial FORTRAN compilers, we encountered unexpected performance problems in one of the programs. This paper describes the specific problem that we encountered, explores its origins, and examines the ability of several analytical techniques to help the compiler avoid similar problems.
DA  - 1992/03//
PY  - 1992
DO  - 10.1145/130616.130619
VL  - 1
IS  - 1
SP  - 22
EP  - 32
SN  - 1057-4514
UR  - https://doi.org/10.1145/130616.130619
KW  - inline substitution
KW  - interprocedural analysis
KW  - interprocedural optimization
ER  - 

TY  - JOUR
TI  - A Deformation Transformer for Real-Time Cloth Animation
AU  - Feng, Wei-Wen
AU  - Yu, Yizhou
AU  - Kim, Byung-Uck
T2  - ACM Trans. Graph.
AB  - Achieving interactive performance in cloth animation has significant implications in computer games and other interactive graphics applications. Although much progress has been made, it is still much desired to have real-time high-quality results that well preserve dynamic folds and wrinkles. In this paper, we introduce a hybrid method for real-time cloth animation. It relies on data-driven models to capture the relationship between cloth deformations at two resolutions. Such data-driven models are responsible for transforming low-quality simulated deformations at the low resolution into high-resolution cloth deformations with dynamically introduced fine details. Our data-driven transformation is trained using rotation invariant quantities extracted from the cloth models, and is independent of the simulation technique chosen for the lower resolution model. We have also developed a fast collision detection and handling scheme based on dynamically transformed bounding volumes. All the components in our algorithm can be efficiently implemented on programmable graphics hardware to achieve an overall real-time performance on high-resolution cloth models.
DA  - 2010/07//
PY  - 2010
DO  - 10.1145/1778765.1778845
VL  - 29
IS  - 4
SN  - 0730-0301
UR  - https://doi.org/10.1145/1778765.1778845
KW  - regression
KW  - collision
KW  - deformation transform
KW  - skinning
ER  - 

TY  - CONF
TI  - A Deformation Transformer for Real-Time Cloth Animation
AU  - Feng, Wei-Wen
AU  - Yu, Yizhou
AU  - Kim, Byung-Uck
T3  - SIGGRAPH '10
AB  - Achieving interactive performance in cloth animation has significant implications in computer games and other interactive graphics applications. Although much progress has been made, it is still much desired to have real-time high-quality results that well preserve dynamic folds and wrinkles. In this paper, we introduce a hybrid method for real-time cloth animation. It relies on data-driven models to capture the relationship between cloth deformations at two resolutions. Such data-driven models are responsible for transforming low-quality simulated deformations at the low resolution into high-resolution cloth deformations with dynamically introduced fine details. Our data-driven transformation is trained using rotation invariant quantities extracted from the cloth models, and is independent of the simulation technique chosen for the lower resolution model. We have also developed a fast collision detection and handling scheme based on dynamically transformed bounding volumes. All the components in our algorithm can be efficiently implemented on programmable graphics hardware to achieve an overall real-time performance on high-resolution cloth models.
C1  - New York, NY, USA
C3  - ACM SIGGRAPH 2010 Papers
DA  - 2010///
PY  - 2010
DO  - 10.1145/1833349.1778845
PB  - Association for Computing Machinery
SN  - 978-1-4503-0210-4
UR  - https://doi.org/10.1145/1833349.1778845
KW  - regression
KW  - collision
KW  - deformation transform
KW  - skinning
ER  - 

TY  - JOUR
TI  - Technical Reports
AU  - Staff, SIGACT News
T2  - SIGACT News
DA  - 1980/01//
PY  - 1980
DO  - 10.1145/1008630.1008631
VL  - 12
IS  - 1
SP  - 2
EP  - 135
SN  - 0163-5700
UR  - https://doi.org/10.1145/1008630.1008631
ER  - 

TY  - JOUR
TI  - Anonymous Blockchain-Based System for Consortium
AU  - Wang, Qin
AU  - Chen, Shiping
AU  - Xiang, Yang
T2  - ACM Trans. Manage. Inf. Syst.
AB  - Blockchain records transactions with various protection techniques against tampering. To meet the requirements on cooperation and anonymity of companies and organizations, researchers have developed a few solutions. Ring signature-based schemes allow multiple participants cooperatively to manage while preserving their individuals’ privacy. However, the solutions cannot work properly due to the increased computing complexity along with the expanded group size. In this article, we propose a Multi-center Anonymous Blockchain-based (MAB) system, with joint management for the consortium and privacy protection for the participants. To achieve that, we formalize the syntax used by the MAB system and present a general construction based on a modular design. By applying cryptographic primitives to each module, we instantiate our scheme with anonymity and decentralization. Furthermore, we carry out a comprehensive formal analysis of our exemplified scheme. A proof of concept simulation is provided to show the feasibility. The results demonstrate security and efficiency from both theoretical perspectives and practical perspectives.
DA  - 2021/06//
PY  - 2021
DO  - 10.1145/3459087
VL  - 12
IS  - 3
SN  - 2158-656X
UR  - https://doi.org/10.1145/3459087
KW  - anonymity
KW  - Blockchain
KW  - consortium
KW  - multi-center
ER  - 

TY  - JOUR
TI  - Interprocedural Constant Propagation
AU  - Callahan, David
AU  - Cooper, Keith D.
AU  - Kennedy, Ken
AU  - Torczon, Linda
T2  - SIGPLAN Not.
AB  - In a compiling system that attempts to improve code for a whole program by optimizing across procedures, the compiler can generate better code for a specific procedure if it knows which variables will have constant values, and what those values will be, when the procedure is invoked. This paper presents a general algorithm for determining for each procedure in a given program the set of inputs that will have known constant values at run time. The precision of the answers provided by this method are dependent on the precision of the local analysis of individual procedures in the program. Since the algorithm is intended for use in a sophisticated software development environment in which local analysis would be provided by the source editor, the quality of the answers will depend on the amount of work the editor performs. Several reasonable strategies for local analysis with different levels of complexity and precision are suggested and the results of a prototype implementation in a vectorizing Fortran compiler are presented.
DA  - 2004/04//
PY  - 2004
DO  - 10.1145/989393.989412
VL  - 39
IS  - 4
SP  - 155
EP  - 166
SN  - 0362-1340
UR  - https://doi.org/10.1145/989393.989412
ER  - 

TY  - CONF
TI  - Interprocedural Constant Propagation
AU  - Callahan, David
AU  - Cooper, Keith D.
AU  - Kennedy, Ken
AU  - Torczon, Linda
T3  - SIGPLAN '86
AB  - In a compiling system that attempts to improve code for a whole program by optimizing across procedures, the compiler can generate better code for a specific procedure if it knows which variables will have constant values, and what those values will be, when the procedure is invoked. This paper presents a general algorithm for determining for each procedure in a given program the set of inputs that will have known constant values at run time. The precision of the answers provided by this method are dependent on the precision of the local analysis of individual procedures in the program. Since the algorithm is intended for use in a sophisticated software development environment in which local analysis would be provided by the source editor, the quality of the answers will depend on the amount of work the editor performs. Several reasonable strategies for local analysis with different levels of complexity and precision are suggested and the results of a prototype implementation in a vectorizing Fortran compiler are presented.
C1  - New York, NY, USA
C3  - Proceedings of the 1986 SIGPLAN Symposium on Compiler Construction
DA  - 1986///
PY  - 1986
DO  - 10.1145/12276.13327
SP  - 152
EP  - 161
PB  - Association for Computing Machinery
SN  - 0-89791-197-0
UR  - https://doi.org/10.1145/12276.13327
ER  - 

TY  - JOUR
TI  - Interprocedural Constant Propagation
AU  - Callahan, David
AU  - Cooper, Keith D.
AU  - Kennedy, Ken
AU  - Torczon, Linda
T2  - SIGPLAN Not.
AB  - In a compiling system that attempts to improve code for a whole program by optimizing across procedures, the compiler can generate better code for a specific procedure if it knows which variables will have constant values, and what those values will be, when the procedure is invoked. This paper presents a general algorithm for determining for each procedure in a given program the set of inputs that will have known constant values at run time. The precision of the answers provided by this method are dependent on the precision of the local analysis of individual procedures in the program. Since the algorithm is intended for use in a sophisticated software development environment in which local analysis would be provided by the source editor, the quality of the answers will depend on the amount of work the editor performs. Several reasonable strategies for local analysis with different levels of complexity and precision are suggested and the results of a prototype implementation in a vectorizing Fortran compiler are presented.
DA  - 1986/07//
PY  - 1986
DO  - 10.1145/13310.13327
VL  - 21
IS  - 7
SP  - 152
EP  - 161
SN  - 0362-1340
UR  - https://doi.org/10.1145/13310.13327
ER  - 

TY  - CONF
TI  - Scibase: An Object-Oriented Scientific Database for Cell Physiology Research
AU  - Guu, Yi-Wen
AU  - Belford, Geneva G.
T3  - SAC '93
C1  - New York, NY, USA
C3  - Proceedings of the 1993 ACM/SIGAPP Symposium on Applied Computing: States of the Art and Practice
DA  - 1993///
PY  - 1993
DO  - 10.1145/162754.162907
SP  - 309
EP  - 317
PB  - Association for Computing Machinery
SN  - 0-89791-567-4
UR  - https://doi.org/10.1145/162754.162907
ER  - 

TY  - CONF
TI  - Register Promotion in C Programs
AU  - Lu, John
AU  - Cooper, Keith D.
T3  - PLDI '97
AB  - The combination of pointers and pointer arithmetic in C makes the task of improving C programs somewhat more difficult than improving programs written in simpler languages like Fortran. While much work has been published that focuses on the analysis of pointers, little has appeared that uses the results of such analysis to improve the code compiled for C. This paper examines the problem of register promotion in C and presents experimental results showing that it can have dramatic effects on memory traffic.
C1  - New York, NY, USA
C3  - Proceedings of the ACM SIGPLAN 1997 Conference on Programming Language Design and Implementation
DA  - 1997///
PY  - 1997
DO  - 10.1145/258915.258943
SP  - 308
EP  - 319
PB  - Association for Computing Machinery
SN  - 0-89791-907-6
UR  - https://doi.org/10.1145/258915.258943
ER  - 

TY  - JOUR
TI  - Register Promotion in C Programs
AU  - Lu, John
AU  - Cooper, Keith D.
T2  - SIGPLAN Not.
AB  - The combination of pointers and pointer arithmetic in C makes the task of improving C programs somewhat more difficult than improving programs written in simpler languages like Fortran. While much work has been published that focuses on the analysis of pointers, little has appeared that uses the results of such analysis to improve the code compiled for C. This paper examines the problem of register promotion in C and presents experimental results showing that it can have dramatic effects on memory traffic.
DA  - 1997/05//
PY  - 1997
DO  - 10.1145/258916.258943
VL  - 32
IS  - 5
SP  - 308
EP  - 319
SN  - 0362-1340
UR  - https://doi.org/10.1145/258916.258943
ER  - 

TY  - JOUR
TI  - Combining Analyses, Combining Optimizations
AU  - Click, Cliff
AU  - Cooper, Keith D.
T2  - ACM Trans. Program. Lang. Syst.
AB  - Modern optimizing compilers use several passes over a program's intermediate representation to generate good code. Many of these optimizations exhibit a phase-ordering problem. Getting the best code may require iterating optimizations until a fixed point is reached. Combining these phases can lead to the discovery of more facts about the program, exposing more opportunities for optimization. This article presents a framework for describing optimizations. It shows how to combine two such frameworks and how to reason about the properties of the resulting framework. The structure of the frame work provides insight into when a combination yields better results. To make the ideas more concrete, this article presents a framework for combining constant propagation, value numbering, and unreachable-code elimination. It is an open question as to what other frameworks can be combined in this way.
DA  - 1995/03//
PY  - 1995
DO  - 10.1145/201059.201061
VL  - 17
IS  - 2
SP  - 181
EP  - 196
SN  - 0164-0925
UR  - https://doi.org/10.1145/201059.201061
KW  - constant propagation
KW  - data-flow analysis
KW  - optimizing compilers
KW  - value numbering
ER  - 

TY  - CONF
TI  - Service Creation and Management in Active Telecom Networks
AU  - Kasiviswanathan, Shiva Prasad
AU  - Rudelson, Mark
AU  - Smith, Adam
AU  - Ullman, Jonathan
AU  - Brunner, Marcus
AU  - Plattner, Bernhard
AU  - Stadler, Rolf
AB  - Marginal (contingency) tables are the method of choice for government agencies releasing statistical summaries of categorical data. In this paper, we derive lower bounds on how much distortion (noise) is necessary in these tables to ensure the privacy of sensitive data. We extend a line of recent work on impossibility results for private data analysis [9, 12, 13, 15] to a natural and important class of functionalities. Consider a database consisting of n rows (one per individual), each row comprising d binary attributes. For any subset of T attributes of size |T|=k, the marginal table for T has 2k entries; each entry counts how many times in the database a particular setting of these attributes occurs. We provide lower bounds for releasing all d k k-attribute marginal tables under several different notions of privacy. (1) We give efficient polynomial time attacks which allow an adversary to reconstruct sensitive information given insufficiently perturbed marginal table releases. In particular, for a constant k, we obtain a tight bound of Ω(min √n, √dk-1) on the average distortion per entry for any mechanism that releases all k-attribute marginals while providing "attribute" privacy (a weak notion implied by most privacy definitions). (2) Our reconstruction attacks require a new lower bound on the least singular value of a random matrix with correlated rows. Let M(k) be a matrix with d k rows formed by taking all possible k-way entry-wise products of an underlying set of d random vectors from 0,1n. For constant k, we show that the least singular value of M(k) is Ω(√dk) with high probability (the same asymptotic bound as for independent rows). (3) We obtain stronger lower bounds for marginal tables satisfying differential privacy. We give a lower bound of Ω(min √n, √ dk), which is tight for n Ω (dk). We extend our analysis to obtain stronger results for mechanisms that add instance-independent noise and weaker results when k is super-constant., booktitle = Proceedings of the Forty-Second ACM Symposium on Theory of Computing, pages = 775–784, numpages = 10, keywords = reconstruction attacks, differential privacy, random matrices, data privacy, location = Cambridge, Massachusetts, USA, series = STOC '10
C1  - New York, NY, USA
C3  - Commun. ACM
DA  - 2001/04//
PY  - 2001
DO  - 10.1145/367211.367257
VL  - 44
SP  - 55
EP  - 61
PB  - Association for Computing Machinery
SN  - 978-1-4503-0050-6
UR  - https://doi.org/10.1145/367211.367257
ER  - 

TY  - CONF
TI  - Unconditionally Secure Ring Authentication
AU  - Safavi-Naini, Reihaneh
AU  - Wang, Shuhong
AU  - Desmedt, Yvo
T3  - ASIACCS '07
AB  - We propose ring authentication in unconditionally secure setting. In a ring authentication system a sender can choose a set of users and construct an authenticated message for a receiver such that the receiver can verify authenticity of the message with respect to the user group chosen by the real sender. The sender will be unconditionally secure even if the receiver has corrupted up to c users and has access to up to ℓ past messages in the system. This functionality is similar to the one provided by ring signature systems with the difference that protection is against an adversary with unlimited power. (This also implies that the verification is not public and is by group members.) In ring signatures an adversary with unlimited computational power can always forge signed messages attributing them to groups of his choice. In our proposed systems the success chance of the adversary can be reduced to the required security of the system. We define model, propose a generic construction whose security is reduced to the security of its building blocks, and give concrete examples of this construction. The construction can also be used in computational setting resulting in ring authentication systems without public key cryptography.
C1  - New York, NY, USA
C3  - Proceedings of the 2nd ACM Symposium on Information, Computer and Communications Security
DA  - 2007///
PY  - 2007
DO  - 10.1145/1229285.1229310
SP  - 173
EP  - 181
PB  - Association for Computing Machinery
SN  - 1-59593-574-6
UR  - https://doi.org/10.1145/1229285.1229310
KW  - authentication codes
KW  - ring signature
KW  - unconditional security
ER  - 

TY  - JOUR
TI  - On the Performance of Group Key Agreement Protocols
AU  - Amir, Yair
AU  - Kim, Yongdae
AU  - Nita-Rotaru, Cristina
AU  - Tsudik, Gene
T2  - ACM Trans. Inf. Syst. Secur.
AB  - Group key agreement is a fundamental building block for secure peer group communication systems. Several group key management techniques were proposed in the last decade, all assuming the existence of an underlying group communication infrastructure to provide reliable and ordered message delivery as well as group membership information. Despite analysis, implementation, and deployment of some of these techniques, the actual costs associated with group key management have been poorly understood so far. This resulted in an undesirable tendency: on the one hand, adopting suboptimal security for reliable group communication, while, on the other hand, constructing excessively costly group key management protocols.This paper presents a thorough performance evaluation of five notable distributed key management techniques (for collaborative peer groups) integrated with a reliable group communication system. An in-depth comparison and analysis of the five techniques is presented based on experimental results obtained in actual local- and wide-area networks. The extensive performance measurement experiments conducted for all methods offer insights into their scalability and practicality. Furthermore, our analysis of the experimental results highlights several observations that are not obvious from the theoretical analysis.
DA  - 2004/08//
PY  - 2004
DO  - 10.1145/1015040.1015045
VL  - 7
IS  - 3
SP  - 457
EP  - 488
SN  - 1094-9224
UR  - https://doi.org/10.1145/1015040.1015045
KW  - Group Communication
KW  - Group Key Management
KW  - Peer Groups
KW  - Secure Communication
ER  - 

TY  - JOUR
TI  - URSA: Ubiquitous and Robust Access Control for Mobile Ad Hoc Networks
AU  - Luo, Haiyun
AU  - Kong, Jiejun
AU  - Zerfos, Petros
AU  - Lu, Songwu
AU  - Zhang, Lixia
T2  - IEEE/ACM Trans. Netw.
AB  - Restricting network access of routing and packet forwarding to well-behaving nodes and denying access from misbehaving nodes are critical for the proper functioning of a mobile ad-hoc network where cooperation among all networking nodes is usually assumed. However, the lack of a network infrastructure, the dynamics of the network topology and node membership, and the potential attacks from inside the network by malicious and/or noncooperative selfish nodes make the conventional network access control mechanisms not applicable. We present URSA, a ubiquitous and robust access control solution for mobile ad hoc networks. URSA implements ticket certification services through multiple-node consensus and fully localized instantiation. It uses tickets to identify and grant network access to well-behaving nodes. In URSA, no single node monopolizes the access decision or is completely trusted. Instead, multiple nodes jointly monitor a local node and certify/revoke its ticket. Furthermore, URSA ticket certification services are fully localized into each node's neighborhood to ensure service ubiquity and resilience. Through analysis, simulations, and experiments, we show that our design effectively enforces access control in the highly dynamic, mobile ad hoc network.
DA  - 2004/12//
PY  - 2004
DO  - 10.1109/TNET.2004.838598
VL  - 12
IS  - 6
SP  - 1049
EP  - 1063
SN  - 1063-6692
UR  - https://doi.org/10.1109/TNET.2004.838598
KW  - mobile ad hoc networks
KW  - self-organized access control
ER  - 

TY  - CONF
TI  - Dynamic Compilation for Component-Based High Performance Computing
AU  - Sandoval, Jeffrey A.
AU  - Cooper, Keith D.
T3  - CBHPC '09
AB  - Component-based frameworks for high-performance computing (HPC) are being developed in response to the increasing complexity of HPC software. These frameworks promote the development of clear programming interfaces that allow easy code reuse, distributed development, and inter-language operability. Unfortunately, the decoupled and dynamic nature of component-based applications prevent static compilers from applying optimization across components. Consequently, if components are used at too fine of a granularity they can impose a significant performance overhead – a serious consequence in the HPC community. Therefore, this paper proposes dynamic compilation as a method for improving performance of component-based HPC applications, which would allow programmers to express programs at the most natural granularity. We introduce a technique called IR annotation in which a high-level intermediate representation (IR) of a program is embedded inside an optimized native executable. This approach leverages the benefits of multiple program representations at the cost of increased code size. The optimized code enables immediate execution at native speed while the IR allows selective dynamic re-compilation if runtime monitoring exposes performance problems. Our experimental results for using IR annotation on the Common Component Architecture (CCA), a component-based HPC framework, show that the program size increase is only about 2.5x. Finally, we propose several ideas that can potentially further reduce the code growth due to IR annotation.
C1  - New York, NY, USA
C3  - Proceedings of the 2009 Workshop on Component-Based High Performance Computing
DA  - 2009///
PY  - 2009
DO  - 10.1145/1687774.1687778
PB  - Association for Computing Machinery
SN  - 978-1-60558-718-9
UR  - https://doi.org/10.1145/1687774.1687778
ER  - 

TY  - CONF
TI  - Better Keep Cash in Your Boots - Hardware Wallets Are the New Single Point of Failure
AU  - Dabrowski, Adrian
AU  - Pfeffer, Katharina
AU  - Reichel, Markus
AU  - Mai, Alexandra
AU  - Weippl, Edgar R.
AU  - Franz, Michael
T3  - DeFi '21
AB  - Hardware wallets are currently considered the most secure way to manage cryptocurrency keys and sign transactions. However, previous publications show that such tokens can be replaced or manipulated in a number of hard-to-detect ways pre- or post-delivery to the user and that implemented (remote) attestation and authenticity checks fail their purpose for multiple reasons.We analyzed the architecture of current products by examining their initialization procedure and attestation methods. Unlike previous publications, we found that tightened attestation and communications encryption will not solve the fundamental architectural flaws sustainably. We conclude that the architecture of current-generation cryptocurrency hardware wallets missed the opportunity for a resilient design by copying the PC's wallet architecture and thus merely shifting the single point of trust from the PC to the hardware wallet.We advocate a mutually verified architecture through changes to BIP32/BIP44 wallet architectures to incorporate collaborative signatures and key generation. This way, neither a compromised wallet nor a compromised PC can meaningfully manipulate keys or transactions.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM CCS Workshop on Decentralized Finance and Security
DA  - 2021///
PY  - 2021
DO  - 10.1145/3464967.3488588
SP  - 1
EP  - 8
PB  - Association for Computing Machinery
SN  - 978-1-4503-8540-4
UR  - https://doi.org/10.1145/3464967.3488588
KW  - collaborative protocols
KW  - cryptocurrencies
KW  - decentralized finance
KW  - hardware wallets
ER  - 

TY  - CONF
TI  - Home Automation in the Wild: Challenges and Opportunities
AU  - Brush, A.J. Bernheim
AU  - Lee, Bongshin
AU  - Mahajan, Ratul
AU  - Agarwal, Sharad
AU  - Saroiu, Stefan
AU  - Dixon, Colin
T3  - CHI '11
AB  - Visions of smart homes have long caught the attention of researchers and considerable effort has been put toward enabling home automation. However, these technologies have not been widely adopted despite being available for over three decades. To gain insight into this state of affairs, we conducted semi-structured home visits to 14 households with home automation. The long term experience, both positive and negative, of the households we interviewed illustrates four barriers that need to be addressed before home automation becomes amenable to broader adoption. These barriers are high cost of ownership, inflexibility, poor manageability, and difficulty achieving security. Our findings also provide several directions for further research, which include eliminating the need for structural changes for installing home automation, providing users with simple security primitives that they can confidently configure, and enabling composition of home devices.
C1  - New York, NY, USA
C3  - Proceedings of the SIGCHI Conference on Human Factors in Computing Systems
DA  - 2011///
PY  - 2011
DO  - 10.1145/1978942.1979249
SP  - 2115
EP  - 2124
PB  - Association for Computing Machinery
SN  - 978-1-4503-0228-9
UR  - https://doi.org/10.1145/1978942.1979249
KW  - domestic technology
KW  - home automation
KW  - smart home
ER  - 

TY  - CONF
TI  - Fast Copy Coalescing and Live-Range Identification
AU  - Budimlic, Zoran
AU  - Cooper, Keith D.
AU  - Harvey, Timothy J.
AU  - Kennedy, Ken
AU  - Oberg, Timothy S.
AU  - Reeves, Steven W.
T3  - PLDI '02
AB  - This paper presents a fast new algorithm for modeling and reasoning about interferences for variables in a program without constructing an interference graph. It then describes how to use this information to minimize copy insertion for ϕ-node instantiation during the conversion of the static single assignment (SSA) form into the control-flow graph (CFG), effectively yielding a new, very fast copy coalescing and live-range identification algorithm.This paper proves some properties of the SSA form that enable construction of data structures to compute interference information for variables that are considered for folding. The asymptotic complexity of our SSA-to-CFG conversion algorithm is where-is the number of instructions in the program.Performing copy folding during the SSA-to-CFG conversion eliminates the need for a separate coalescing phase while simplifying the intermediate code. This may make graph-coloring register allocation more practical in just in time (JIT) and other time-critical compilers For example, Sun's Hotspot Server Compiler already employs a graph-coloring register allocator[10].This paper also presents an improvement to the classical interference-graph based coalescing optimization that shows adecrease in memory usage of up to three orders of magnitude and a decrease of a factor of two in compilation time, while providing the exact same results.We present experimental results that demonstrate that our algorithm is almost as precise (within one percent on average) as the improved interference-graph-based coalescing algorithm, while requiring three times less compilation time.
C1  - New York, NY, USA
C3  - Proceedings of the ACM SIGPLAN 2002 Conference on Programming Language Design and Implementation
DA  - 2002///
PY  - 2002
DO  - 10.1145/512529.512534
SP  - 25
EP  - 32
PB  - Association for Computing Machinery
SN  - 1-58113-463-0
UR  - https://doi.org/10.1145/512529.512534
KW  - register allocation
KW  - code generation
KW  - copy coalescing
KW  - interference graph
KW  - live-range identification
ER  - 

TY  - JOUR
TI  - Fast Copy Coalescing and Live-Range Identification
AU  - Budimlic, Zoran
AU  - Cooper, Keith D.
AU  - Harvey, Timothy J.
AU  - Kennedy, Ken
AU  - Oberg, Timothy S.
AU  - Reeves, Steven W.
T2  - SIGPLAN Not.
AB  - This paper presents a fast new algorithm for modeling and reasoning about interferences for variables in a program without constructing an interference graph. It then describes how to use this information to minimize copy insertion for ϕ-node instantiation during the conversion of the static single assignment (SSA) form into the control-flow graph (CFG), effectively yielding a new, very fast copy coalescing and live-range identification algorithm.This paper proves some properties of the SSA form that enable construction of data structures to compute interference information for variables that are considered for folding. The asymptotic complexity of our SSA-to-CFG conversion algorithm is where-is the number of instructions in the program.Performing copy folding during the SSA-to-CFG conversion eliminates the need for a separate coalescing phase while simplifying the intermediate code. This may make graph-coloring register allocation more practical in just in time (JIT) and other time-critical compilers For example, Sun's Hotspot Server Compiler already employs a graph-coloring register allocator[10].This paper also presents an improvement to the classical interference-graph based coalescing optimization that shows adecrease in memory usage of up to three orders of magnitude and a decrease of a factor of two in compilation time, while providing the exact same results.We present experimental results that demonstrate that our algorithm is almost as precise (within one percent on average) as the improved interference-graph-based coalescing algorithm, while requiring three times less compilation time.
DA  - 2002/05//
PY  - 2002
DO  - 10.1145/543552.512534
VL  - 37
IS  - 5
SP  - 25
EP  - 32
SN  - 0362-1340
UR  - https://doi.org/10.1145/543552.512534
KW  - register allocation
KW  - code generation
KW  - copy coalescing
KW  - interference graph
KW  - live-range identification
ER  - 

TY  - CONF
TI  - Anvil - System Architecture and Experiences from Deployment and Early User Operations
AU  - Song, X. Carol
AU  - Smith, Preston
AU  - Kalyanam, Rajesh
AU  - Zhu, Xiao
AU  - Adams, Eric
AU  - Colby, Kevin
AU  - Finnegan, Patrick
AU  - Gough, Erik
AU  - Hillery, Elizabett
AU  - Irvine, Rick
AU  - Maji, Amiya
AU  - St. John, Jason
T3  - PEARC '22
AB  - Anvil is a new XSEDE advanced capacity computational resource funded by NSF. Designed with a systematic strategy to meet the ever increasing and diversifying research needs for advanced computational capacity, Anvil integrates a large capacity high-performance computing (HPC) system with a comprehensive ecosystem of software, access interfaces, programming environments, and composable services in a seamless environment to support a broad range of current and future science and engineering applications of the nation’s research community. Anchored by a 1000-node CPU cluster featuring the latest AMD EPYC 3rd generation (Milan) processors, along with a set of 1TB large memory and NVIDIA A100 GPU nodes, Anvil integrates a multi-tier storage system, a Kubernetes composable subsystem, and a pathway to Azure commercial cloud to support a variety of workflows and storage needs. Anvil was successfully deployed and integrated with XSEDE during the world-wide COVID-19 pandemic. Entering production operation in February 2022, Anvil will serve the nation’s science and engineering research community for five years. This paper describes the Anvil system and services, including its various components and subsystems, user facing features, and shares the Anvil team’s experience through its early user access program from November 2021 through January 2022.
C1  - New York, NY, USA
C3  - Practice and Experience in Advanced Research Computing
DA  - 2022///
PY  - 2022
DO  - 10.1145/3491418.3530766
PB  - Association for Computing Machinery
SN  - 978-1-4503-9161-0
UR  - https://doi.org/10.1145/3491418.3530766
ER  - 

TY  - JOUR
TI  - A Copy Network with Shared Buffers for Large-Scale Multicast ATM Switching
AU  - De Zhong, Wen
AU  - Kaniyil, Jaidev
AU  - Onozato, Y.
T2  - IEEE/ACM Trans. Netw.
DA  - 1993/04//
PY  - 1993
DO  - 10.1109/90.222923
VL  - 1
IS  - 2
SP  - 157
EP  - 165
SN  - 1063-6692
UR  - https://doi.org/10.1109/90.222923
ER  - 

TY  - JOUR
TI  - Ultracomputers: A Teraflop before Its Time
AU  - Bell, Gordon
T2  - Commun. ACM
DA  - 1992/08//
PY  - 1992
DO  - 10.1145/135226.135227
VL  - 35
IS  - 8
SP  - 26
EP  - 47
SN  - 0001-0782
UR  - https://doi.org/10.1145/135226.135227
KW  - government policy
KW  - parallel processing
KW  - scientific programming
ER  - 

TY  - CONF
TI  - Automatic Generation of Behavioral Models from Switch-Level Descriptions
AU  - Blaauw, D. T.
AU  - Saab, D. G.
AU  - Mueller-Thuns, R. B.
AU  - Abraham, J. A.
AU  - Rahmeh, J. T.
T3  - DAC '89
AB  - This paper discusses the automatic generation of high-level software models from switch-level circuit descriptions. The proposed algorithms operate directly on the hierarchical description, and incorporate information about the design such as the structure, regularity, functionality, and control signals in the generation process. New algorithms are proposed and have been implemented for combinational modules and bus structures. A significant speedup has been obtained for these modules of a commercially available chip.
C1  - New York, NY, USA
C3  - Proceedings of the 26th ACM/IEEE Design Automation Conference
DA  - 1989///
PY  - 1989
DO  - 10.1145/74382.74413
SP  - 179
EP  - 184
PB  - Association for Computing Machinery
SN  - 0-89791-310-8
UR  - https://doi.org/10.1145/74382.74413
ER  - 

TY  - JOUR
TI  - Minimizing Wasted Space in Partitioned Segmentation
AU  - Gelenbe, Erol
AU  - Boekhorst, J. C. A.
AU  - Kessels, J. L. W.
T2  - Commun. ACM
AB  - A paged virtual memory system using a finite number of page sizes is considered. Two algorithms for assigning pages to segments are discussed. Both of these algorithms are simple to implement. The problem of choosing the page sizes to minimize the expected value of total wasted space in internal fragmentation and in a page table, per segment, is then solved for a probability density function of segment size which may be expressed as a convex combination of Erlang densities.
DA  - 1973/06//
PY  - 1973
DO  - 10.1145/362248.362253
VL  - 16
IS  - 6
SP  - 343
EP  - 349
SN  - 0001-0782
UR  - https://doi.org/10.1145/362248.362253
KW  - dynamic storage allocation
KW  - fragmentation
KW  - multiple page sizes
KW  - paging
KW  - segmentation
KW  - virtual memory
ER  - 

TY  - JOUR
TI  - Improvements to Graph Coloring Register Allocation
AU  - Briggs, Preston
AU  - Cooper, Keith D.
AU  - Torczon, Linda
T2  - ACM Trans. Program. Lang. Syst.
AB  - We describe two improvements to Chaitin-style graph coloring register allocators. The first, optimistic coloring, uses a stronger heuristic to find a k-coloring for the interference graph. The second extends Chaitin's treatment of rematerialization to handle a larger class of values. These techniques are complementary. Optimistic coloring decreases the number of procedures that require spill code and reduces the amount of spill code when spilling is unavoidable. Rematerialization lowers the cost of spilling some values. This paper describes both of the techniques and our experience building and using register allocators that incorporate them. It provides a detailed description of optimistic coloring and rematerialization. It presents experimental data to show the performance of several versions of the register allocator on a suite of FORTRAN programs. It discusses several insights that we discovered only after repeated implementation of these allocators.
DA  - 1994/05//
PY  - 1994
DO  - 10.1145/177492.177575
VL  - 16
IS  - 3
SP  - 428
EP  - 455
SN  - 0164-0925
UR  - https://doi.org/10.1145/177492.177575
KW  - graph coloring
KW  - register allocation
KW  - code generation
ER  - 

TY  - CONF
TI  - ACME: Adaptive Compilation Made Efficient
AU  - Cooper, Keith D.
AU  - Grosul, Alexander
AU  - Harvey, Timothy J.
AU  - Reeves, Steven
AU  - Subramanian, Devika
AU  - Torczon, Linda
AU  - Waterman, Todd
T3  - LCTES '05
AB  - Research over the past five years has shown significant performance improvements using a technique called adaptive compilation. An adaptive compiler uses a compile-execute-analyze feedback loop to find the combination of optimizations and parameters that minimizes some performance goal, such as code size or execution time.Despite its ability to improve performance, adaptive compilation has not seen widespread use because of two obstacles: the large amounts of time that such systems have used to perform the many compilations and executions prohibits most users from adopting these systems, and the complexity inherent in a feedback-driven adaptive system has made it difficult to build and hard to use.A significant portion of the adaptive compilation process is devoted to multiple executions of the code being compiled. We have developed a technique called virtual execution to address this problem. Virtual execution runs the program a single time and preserves information that allows us to accurately predict the performance of different optimization sequences without running the code again. Our prototype implementation of this technique significantly reduces the time required by our adaptive compiler.In conjunction with this performance boost, we have developed a graphical-user interface (GUI) that provides a controlled view of the compilation process. By providing appropriate defaults, the interface limits the amount of information that the user must provide to get started. At the same time, it lets the experienced user exert fine-grained control over the parameters that control the system.
C1  - New York, NY, USA
C3  - Proceedings of the 2005 ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems
DA  - 2005///
PY  - 2005
DO  - 10.1145/1065910.1065921
SP  - 69
EP  - 77
PB  - Association for Computing Machinery
SN  - 1-59593-018-3
UR  - https://doi.org/10.1145/1065910.1065921
KW  - adaptive compilation
ER  - 

TY  - JOUR
TI  - ACME: Adaptive Compilation Made Efficient
AU  - Cooper, Keith D.
AU  - Grosul, Alexander
AU  - Harvey, Timothy J.
AU  - Reeves, Steven
AU  - Subramanian, Devika
AU  - Torczon, Linda
AU  - Waterman, Todd
T2  - SIGPLAN Not.
AB  - Research over the past five years has shown significant performance improvements using a technique called adaptive compilation. An adaptive compiler uses a compile-execute-analyze feedback loop to find the combination of optimizations and parameters that minimizes some performance goal, such as code size or execution time.Despite its ability to improve performance, adaptive compilation has not seen widespread use because of two obstacles: the large amounts of time that such systems have used to perform the many compilations and executions prohibits most users from adopting these systems, and the complexity inherent in a feedback-driven adaptive system has made it difficult to build and hard to use.A significant portion of the adaptive compilation process is devoted to multiple executions of the code being compiled. We have developed a technique called virtual execution to address this problem. Virtual execution runs the program a single time and preserves information that allows us to accurately predict the performance of different optimization sequences without running the code again. Our prototype implementation of this technique significantly reduces the time required by our adaptive compiler.In conjunction with this performance boost, we have developed a graphical-user interface (GUI) that provides a controlled view of the compilation process. By providing appropriate defaults, the interface limits the amount of information that the user must provide to get started. At the same time, it lets the experienced user exert fine-grained control over the parameters that control the system.
DA  - 2005/06//
PY  - 2005
DO  - 10.1145/1070891.1065921
VL  - 40
IS  - 7
SP  - 69
EP  - 77
SN  - 0362-1340
UR  - https://doi.org/10.1145/1070891.1065921
KW  - adaptive compilation
ER  - 

TY  - CONF
TI  - Almost Optimum Placement Legalization by Minimum Cost Flow and Dynamic Programming
AU  - Brenner, Ulrich
AU  - Pauli, Anna
AU  - Vygen, Jens
T3  - ISPD '04
AB  - VLSI placement tools usually work in two steps: First, the cells that have to be placed are roughly spread out over the chip area ignoring disjointness (global placement). Then, in a second step, the cells are moved to their final position such that all overlaps are removed and all additional constraints are met (detailed placement or legalization).We consider algorithms for legalization. In particular, we analyze a generic legalization algorithm based on minimum cost flows and dynamic programming. Specializations are being used in industry for many years, and an improved version was proposed very recently in [2]. The objective of all these algorithms is to minimize the weighted sum of (squared) movements, i.e. they assume the placement to be already optimized except for not being legal.To evaluate results, we propose two different lower bounds for the legalization problem, one based on linear assignment, and the other one based on an integer linear programming relaxation. We prove that the second lower bound is always at least as good as the first one. We also show how to compute the bounds efficiently. We then give an extensive experimental analysis of the algorithms and the lower bounds by testing them on a set of recent industrial ASICs with up to 2.4 million cells. In particular, we show that the gap between the new algorithm and the better lower bound is usually less than 10 percent. This proves that the legalization problem is solved almost optimally.Besides (weighted) total (squared) movement, we also consider various other objectives like wirelength, timing, and routability. Our experiments demonstrate that minimizing total (weighted, squared) movement has almost no negative effect on the timing properties, routability and netlength. Therefore the new algorithm will help in overall design closure.
C1  - New York, NY, USA
C3  - Proceedings of the 2004 International Symposium on Physical Design
DA  - 2004///
PY  - 2004
DO  - 10.1145/981066.981069
SP  - 2
EP  - 9
PB  - Association for Computing Machinery
SN  - 1-58113-817-2
UR  - https://doi.org/10.1145/981066.981069
KW  - detailed placement
KW  - legalization
KW  - minimum-cost flow
KW  - placement
ER  - 

TY  - CONF
TI  - Efficiently Exploring Compiler Optimization Sequences with Pairwise Pruning
AU  - Chabbi, Milind M.
AU  - Mellor-Crummey, John M.
AU  - Cooper, Keith D.
T3  - EXADAPT '11
AB  - Most compilers apply optimizations in a fixed order regardless of input programs. However, it is well known that optimizations can have enabling, and disabling interactions or equivalent effects. The effects of interference are program specific and hence no single sequence is universally appropriate for all input programs. In this paper we explore the problem of searching for optimal sequences of compiler optimizations to apply for a given program and describe novel strategies that bring us a step closer to searching this problem space efficiently. We also construct models for accurately predicting the runtime performance of a program when a sequence of optimizations is applied to it. The early results of the models on a small set of input programs are encouraging and suggest that the approaches we describe are worthy of further consideration.
C1  - New York, NY, USA
C3  - Proceedings of the 1st International Workshop on Adaptive Self-Tuning Computing Systems for the Exaflop Era
DA  - 2011///
PY  - 2011
DO  - 10.1145/2000417.2000421
SP  - 34
EP  - 45
PB  - Association for Computing Machinery
SN  - 978-1-4503-0708-6
UR  - https://doi.org/10.1145/2000417.2000421
KW  - compiler optimization
KW  - optimization sequence
KW  - phase ordering
ER  - 

TY  - CONF
TI  - On the Second Eigenvalue of Random Regular Graphs
AU  - Friedman, J.
AU  - Kahn, J.
AU  - Szemerédi, E.
T3  - STOC '89
C1  - New York, NY, USA
C3  - Proceedings of the Twenty-First Annual ACM Symposium on Theory of Computing
DA  - 1989///
PY  - 1989
DO  - 10.1145/73007.73063
SP  - 587
EP  - 598
PB  - Association for Computing Machinery
SN  - 0-89791-307-8
UR  - https://doi.org/10.1145/73007.73063
ER  - 

TY  - CONF
TI  - A Time-Geographic Approach to Quantify the Duration of Interaction in Movement Data
AU  - Su, Rongxiang
AU  - Dodge, Somayeh
AU  - Goulias, Konstadinos
T3  - HANIMOB '21
AB  - Interaction between moving individuals is a critical factor in shaping social dynamics and human networks. Recent advancements in trajectory analytics have resulted in promising methods to identify and extract spatio-temporal patterns of interaction using movement tracking data. However, methodologies to quantify the duration of interaction remain limited. In the present work, we advance the existing time-geographic based approach that mainly relies on potential path area computation and polygon intersection to quantify the duration of potential concurrent interactions (i.e. synchronous interaction in space and time) between mobile individuals. Two case studies using real human GPS tracking data in California reveal that in general, the proposed time-geographic based approach outperforms the proximity-based approach which is commonly used in digital contact tracing technologies. Our method is more effective in the identification of potential continuous interactions, especially when individuals do not move together. In addition, the results show that the proposed method can estimate the duration of contacts more accurately and can identify more complete interactions over a continuous time period, while the proximity-based approach underestimates contacts which may result in more intermittent interactions with shorter durations.
C1  - New York, NY, USA
C3  - Proceedings of the 1st ACM SIGSPATIAL International Workshop on Animal Movement Ecology and Human Mobility
DA  - 2021///
PY  - 2021
DO  - 10.1145/3486637.3489490
SP  - 18
EP  - 26
PB  - Association for Computing Machinery
SN  - 978-1-4503-9122-1
UR  - https://doi.org/10.1145/3486637.3489490
KW  - interaction analysis
KW  - human mobility
KW  - interaction duration
KW  - time geography
ER  - 

TY  - CONF
TI  - PAST: Scalable Ethernet for Data Centers
AU  - Stephens, Brent
AU  - Cox, Alan
AU  - Felter, Wes
AU  - Dixon, Colin
AU  - Carter, John
T3  - CoNEXT '12
AB  - We present PAST, a novel network architecture for data center Ethernet networks that implements a Per-Address Spanning Tree routing algorithm. PAST preserves Ethernet's self-configuration and mobility support while increasing its scalability and usable bandwidth. PAST is explicitly designed to accommodate unmodified commodity hosts and Ethernet switch chips. Surprisingly, we find that PAST can achieve performance comparable to or greater than Equal-Cost Multipath (ECMP) forwarding, which is currently limited to layer-3 IP networks, without any multipath hardware support. In other words, the hardware and firmware changes proposed by emerging standards like TRILL are not required for high-performance, scalable Ethernet networks. We evaluate PAST on Fat Tree, HyperX, and Jellyfish topologies, and show that it is able to capitalize on the advantages each offers. We also describe an OpenFlow-based implementation of PAST in detail.
C1  - New York, NY, USA
C3  - Proceedings of the 8th International Conference on Emerging Networking Experiments and Technologies
DA  - 2012///
PY  - 2012
DO  - 10.1145/2413176.2413183
SP  - 49
EP  - 60
PB  - Association for Computing Machinery
SN  - 978-1-4503-1775-7
UR  - https://doi.org/10.1145/2413176.2413183
KW  - data center
KW  - openflow
KW  - software defined networking
ER  - 

TY  - CONF
TI  - SecHOG: Privacy-Preserving Outsourcing Computation of Histogram of Oriented Gradients in the Cloud
AU  - Wang, Qian
AU  - Wang, Jingjun
AU  - Hu, Shengshan
AU  - Zou, Qin
AU  - Ren, Kui
T3  - ASIA CCS '16
AB  - Abundant multimedia data generated in our daily life has intrigued a variety of very important and useful real-world applications such as object detection and recognition etc. Accompany with these applications, many popular feature descriptors have been developed, e.g., SIFT, SURF and HOG. Manipulating massive multimedia data locally, however, is a storage and computation intensive task, especially for resource-constrained clients. In this work, we focus on exploring how to securely outsource the famous feature extraction algorithm–Histogram of Oriented Gradients (HOG) to untrusted cloud servers, without revealing the data owner's private information. For the first time, we investigate this secure outsourcing computation problem under two different models and accordingly propose two novel privacy-preserving HOG outsourcing protocols, by efficiently encrypting image data by somewhat homomorphic encryption (SHE) integrated with single-instruction multiple-data (SIMD), designing a new batched secure comparison protocol, and carefully redesigning every step of HOG to adapt it to the ciphertext domain. Explicit Security and effectiveness analysis are presented to show that our protocols are practically-secure and can approximate well the performance of the original HOG executed in the plaintext domain. Our extensive experimental evaluations further demonstrate that our solutions achieve high efficiency and perform comparably to the original HOG when being applied to human detection.
C1  - New York, NY, USA
C3  - Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security
DA  - 2016///
PY  - 2016
DO  - 10.1145/2897845.2897861
SP  - 257
EP  - 268
PB  - Association for Computing Machinery
SN  - 978-1-4503-4233-9
UR  - https://doi.org/10.1145/2897845.2897861
KW  - cloud
KW  - HOG
KW  - outsourcing computation
KW  - privacy preservation
ER  - 

TY  - CONF
TI  - Shared Research Group Storage Solution with Integrated Access Management
AU  - Dietz, Daniel T.
AU  - Gorenstein, Lev A.
AU  - Veldman, Gregory S.
AU  - Colby, Kevin D.
T3  - PEARC17
AB  - Management of research data storage areas for projects and research groups has become a time consuming task for research and data support staff. Purdue Research Computing staff have developed a web-based system to allow quick provisioning of group storage areas on the Research Data Depot shared filesystem operated by Purdue's Research Computing group. The system allows technical and non-technical staff to provision Research Data Depot storage allocations and configure data access management for research groups quickly, consistently, and reliably via an intuitive web interface. This system also allows for tracking allocation usage of these storage spaces and customizable alerting of the users of these spaces when allocations are nearly consumed.
C1  - New York, NY, USA
C3  - Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact
DA  - 2017///
PY  - 2017
DO  - 10.1145/3093338.3093354
PB  - Association for Computing Machinery
SN  - 978-1-4503-5272-7
UR  - https://doi.org/10.1145/3093338.3093354
KW  - access management
KW  - Data Depot
KW  - data storage
KW  - research data
ER  - 

TY  - JOUR
TI  - Coloring Heuristics for Register Allocation
AU  - Briggs, Preston
AU  - Cooper, Keith D.
AU  - Kennedy, Ken
AU  - Torczon, Linda
T2  - SIGPLAN Not.
AB  - We describe an improvement to a heuristic introduced by Chaitin for use in graph coloring register allocation. Our modified heuristic produces better colorings, with less spill code. It has similar compile-time and implementation requirements. We present experimental data to compare the two methods.
DA  - 2004/04//
PY  - 2004
DO  - 10.1145/989393.989424
VL  - 39
IS  - 4
SP  - 283
EP  - 294
SN  - 0362-1340
UR  - https://doi.org/10.1145/989393.989424
ER  - 

TY  - CONF
TI  - Alice: Lessons Learned from Building a 3D System for Novices
AU  - Conway, Matthew
AU  - Audia, Steve
AU  - Burnette, Tommy
AU  - Cosgrove, Dennis
AU  - Christiansen, Kevin
T3  - CHI '00
AB  - We present lessons learned from developing Alice, a 3D graphics programming environment designed for undergraduates with no 3D graphics or programming experience. Alice is a Windows 95/NT tool for describing the time-based and interactive behavior of 3D objects, not a CAD tool for creating object geometry. Our observations and conclusions come from formal and informal observations of hundreds of users. Primary results include the use of LOGO-style egocentric coordinate systems, the use of arbitrary objects as lightweight coordinate systems, the launching of implicit threads of execution, extensive function overloading for a small set of commands, the careful choice of command names, and the ubiquitous use of animation and undo.
C1  - New York, NY, USA
C3  - Proceedings of the SIGCHI Conference on Human Factors in Computing Systems
DA  - 2000///
PY  - 2000
DO  - 10.1145/332040.332481
SP  - 486
EP  - 493
PB  - Association for Computing Machinery
SN  - 1-58113-216-6
UR  - https://doi.org/10.1145/332040.332481
KW  - animation authoring tools
KW  - interactive 3D graphics
ER  - 

TY  - CONF
TI  - Garbage Collection for Multicore NUMA Machines
AU  - Auhagen, Sven
AU  - Bergstrom, Lars
AU  - Fluet, Matthew
AU  - Reppy, John
T3  - MSPC '11
AB  - Modern high-end machines feature multiple processor packages, each of which contains multiple independent cores and integrated memory controllers connected directly to dedicated physical RAM. These packages are connected via a shared bus, creating a system with a heterogeneous memory hierarchy. Since this shared bus has less bandwidth than the sum of the links to memory, aggregate memory bandwidth is higher when parallel threads all access memory local to their processor package than when they access memory attached to a remote package. This bandwidth limitation has traditionally limited the scalability of modern functional language implementations, which seldom scale well past 8 cores, even on small benchmarks.This work presents a garbage collector integrated with our strict, parallel functional language implementation, Manticore, and shows that it scales effectively on both a 48-core AMD Opteron machine and a 32-core Intel Xeon machine.
C1  - New York, NY, USA
C3  - Proceedings of the 2011 ACM SIGPLAN Workshop on Memory Systems Performance and Correctness
DA  - 2011///
PY  - 2011
DO  - 10.1145/1988915.1988929
SP  - 51
EP  - 57
PB  - Association for Computing Machinery
SN  - 978-1-4503-0794-9
UR  - https://doi.org/10.1145/1988915.1988929
KW  - garbage collection
KW  - NUMA
KW  - parallelism
ER  - 

TY  - CONF
TI  - Guaranteed Bounds for Posterior Inference in Universal Probabilistic Programming
AU  - Beutner, Raven
AU  - Ong, C.-H. Luke
AU  - Zaiser, Fabian
T3  - PLDI 2022
AB  - We propose a new method to approximate the posterior distribution of probabilistic programs by means of computing guaranteed bounds. The starting point of our work is an interval-based trace semantics for a recursive, higher-order probabilistic programming language with continuous distributions. Taking the form of (super-/subadditive) measures, these lower/upper bounds are non-stochastic and provably correct: using the semantics, we prove that the actual posterior of a given program is sandwiched between the lower and upper bounds (soundness); moreover, the bounds converge to the posterior (completeness). As a practical and sound approximation, we introduce a weight-aware interval type system, which automatically infers interval bounds on not just the return value but also the weight of program executions, simultaneously. We have built a tool implementation, called GuBPI, which automatically computes these posterior lower/upper bounds. Our evaluation on examples from the literature shows that the bounds are useful, and can even be used to recognise wrong outputs from stochastic posterior inference procedures.
C1  - New York, NY, USA
C3  - Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation
DA  - 2022///
PY  - 2022
DO  - 10.1145/3519939.3523721
SP  - 536
EP  - 551
PB  - Association for Computing Machinery
SN  - 978-1-4503-9265-5
UR  - https://doi.org/10.1145/3519939.3523721
KW  - probabilistic programming
KW  - symbolic execution
KW  - abstract interpretation
KW  - Bayesian inference
KW  - interval arithmetic
KW  - operational semantics
KW  - type system
KW  - verification
ER  - 

TY  - CONF
TI  - An Overview of Side Channel Analysis Attacks
AU  - Le, Thanh-Ha
AU  - Canovas, Cécile
AU  - Clédière, Jessy
T3  - ASIACCS '08
AB  - During the last ten years, power analysis attacks have been widely developed under many forms. They analyze the relation between the power consumption or electromagnetic radiation of a cryptographic device and the handled data during cryptographic operations. The goal of this paper is to give a global view of statistical attacks based on side channel analysis. These techniques are classified into two classes: attacks without reference device (e.g. Differential Power Analysis, Correlation Power Analysis) and attacks using a reference device (e.g. Template Attack, Stochastic Model Attack). In this paper, we present the attacks with an easy comprehensible way and focus on their implementation aspect. The pros and cons of each attack is highlighted in details with concrete electromagnetic signals. At least, our paper proposes also some solutions to enhance the existing attacks.
C1  - New York, NY, USA
C3  - Proceedings of the 2008 ACM Symposium on Information, Computer and Communications Security
DA  - 2008///
PY  - 2008
DO  - 10.1145/1368310.1368319
SP  - 33
EP  - 43
PB  - Association for Computing Machinery
SN  - 978-1-59593-979-1
UR  - https://doi.org/10.1145/1368310.1368319
KW  - CPA
KW  - DEMA
KW  - DPA
KW  - side channel attacks
KW  - stochastic model
KW  - template attack
ER  - 

TY  - JOUR
TI  - Pteromys: Interactive Design and Optimization of Free-Formed Free-Flight Model Airplanes
AU  - Umetani, Nobuyuki
AU  - Koyama, Yuki
AU  - Schmidt, Ryan
AU  - Igarashi, Takeo
T2  - ACM Trans. Graph.
AB  - This paper introduces novel interactive techniques for designing original hand-launched free-flight glider airplanes which can actually fly. The aerodynamic properties of a glider aircraft depend on their shape, imposing significant design constraints. We present a compact and efficient representation of glider aerodynamics that can be fit to real-world conditions using a data-driven method. To do so, we acquire a sample set of glider flight trajectories using a video camera and the system learns a nonlinear relationship between forces on the wing and wing shape. Our acquisition system is much simpler to construct than a wind tunnel, but using it we can efficiently discover a wing model for simple gliding aircraft. Our resulting model can handle general free-form wing shapes and yet agrees sufficiently well with the acquired airplane flight trajectories. Based on this compact aerodynamics model, we present a design tool in which the wing configuration created by a user is interactively optimized to maximize flight-ability. To demonstrate the effectiveness of our tool for glider design by novice users, we compare it with a traditional design workflow.
DA  - 2014/07//
PY  - 2014
DO  - 10.1145/2601097.2601129
VL  - 33
IS  - 4
SN  - 0730-0301
UR  - https://doi.org/10.1145/2601097.2601129
KW  - optimization
KW  - aerodynamics
KW  - fabrication
ER  - 

TY  - CONF
TI  - Key Distribution and Update for Secure Inter-Group Multicast Communication
AU  - Wang, Weichao
AU  - Bhargava, Bharat
T3  - SASN '05
AB  - Group communication has become an important component in wireless networks. In this paper, we focus on the environments in which multiple groups coexist in the system, and both intra and inter group multicast traffic must be protected by secret keys. We propose a mechanism that integrates polynomials with flat tables to achieve personal key share distribution and efficient key refreshment during group changes. The proposed mechanism distributes keys via true broadcast. The contributions of the research include: (1) By switching from asymmetric algorithms to symmetric encryption methods, the proposed mechanism avoids heavy computation, and improves the processing efficiency of multicast traffic and the power usage at the wireless nodes. The group managers do not have to generate public-private key pairs when the group member changes. (2) It becomes more difficult for an attacker to impersonate another node since personal key shares are adopted. The additional storage overhead at the wireless nodes and the increased broadcast traffic during key refreshment are justified. In addition, we describe techniques to improve the robustness of the proposed mechanism under the complicated scenarios such as collusive attacks and batch group member changes.
C1  - New York, NY, USA
C3  - Proceedings of the 3rd ACM Workshop on Security of Ad Hoc and Sensor Networks
DA  - 2005///
PY  - 2005
DO  - 10.1145/1102219.1102227
SP  - 43
EP  - 52
PB  - Association for Computing Machinery
SN  - 1-59593-227-5
UR  - https://doi.org/10.1145/1102219.1102227
KW  - inter-group communication
KW  - key distribution and update
KW  - security
ER  - 

TY  - JOUR
TI  - JET: Dynamic Join-Exit-Tree Amortization and Scheduling for Contributory Key Management
AU  - Mao, Yinian
AU  - Sun, Yan
AU  - Wu, Min
AU  - Liu, K. J. R.
T2  - IEEE/ACM Trans. Netw.
AB  - In secure group communications, the time cost associated with key updates in the events of member join and departure is an important aspect of quality of service, especially in large groups with highly dynamic membership. To achieve better time efficiency, we propose a join-exit-tree (JET) key management framework. First, a special key tree topology with join and exit subtrees is introduced to handle key updates for dynamic membership. Then, optimization techniques are employed to determine the capacities of join and exit subtrees for achieving the best time efficiency, and algorithms are designed to dynamically update the join and exit trees. We show that, on average, the asymptotic time cost for each member join/departure event is reduced to O(log (log n)) from the previous cost of O(log n), where n is the group size. Our experimental results based on simulated user activities as well as the real MBone data demonstrate that the proposed JET scheme can significantly improve the time efficiency, while maintaining low communication and computation cost, of tree-based contributory key management.
DA  - 2006/10//
PY  - 2006
DO  - 10.1109/TNET.2006.882851
VL  - 14
IS  - 5
SP  - 1128
EP  - 1140
SN  - 1063-6692
UR  - https://doi.org/10.1109/TNET.2006.882851
KW  - contributory key management
KW  - dynamic tree topology
KW  - secure group communications
KW  - time efficiency
ER  - 

TY  - CONF
TI  - High Performance and Low Power FIR Filter Design Based on Sharing Multiplication
AU  - Park, Jongsun
AU  - Jeong, Woopyo
AU  - Choo, Hunsoo
AU  - Mahmoodi-Meimand, Hamid
AU  - Wang, Yongtao
AU  - Roy, Kaushik
T3  - ISLPED '02
AB  - We present a high performance and low power FIR filter design, which is based on computation sharing multiplier (CSHM). CSHM specifically targets computation re-use in vector-scalar products and is effectively used in our FIR filter design. Efficient circuit level techniques: a new carry select adder and conditional capture flip-flop (CCFF), are also used to further improve power and performance. The proposed FIR filter architecture was implemented in 0.25 μm technology. Experimental results on a 10 tap low pass CSHM FIR filter show speed and power improvement of 19% and 17%, respectively, with respect to an FIR filter based on Wallace tree multiplier.
C1  - New York, NY, USA
C3  - Proceedings of the 2002 International Symposium on Low Power Electronics and Design
DA  - 2002///
PY  - 2002
DO  - 10.1145/566408.566485
SP  - 295
EP  - 300
PB  - Association for Computing Machinery
SN  - 1-58113-475-4
UR  - https://doi.org/10.1145/566408.566485
KW  - computation sharing
KW  - conditional capture flip-flop
KW  - FIR filter design
KW  - high performance and low power carry select adder
ER  - 

TY  - CONF
TI  - Adaptive Content Networking
AU  - Buchholz, Sven
AU  - Buchholz, Thomas
T3  - ISICT '03
AB  - Content adaptation is an essential concept to meet the heterogeneous requirements of clients in Pervasive Computing environments. However, content adaptation interferes with replication applied in content networks to improve the performance of information access. Leveraging the advantages of replication in the world of Pervasive Computing is the subject of this paper.We introduce the notion of Cost-Quality Optimized Content Networking and formalize the inherent optimization problem. Additionally, web-caching and Content Distribution Networks (CDN) are compared with respect to their potential to accommodate cost-quality optimized content adaptation showing web-caching to be inappropriate due to intolerable response delays. The feasibility in the CDN scenario is illustrated by outlining a CDN system architecture.
C3  - Proceedings of the 1st International Symposium on Information and Communication Technologies
DA  - 2003///
PY  - 2003
SP  - 213
EP  - 219
PB  - Trinity College Dublin
ER  - 

TY  - CONF
TI  - Circular Scheduling: A New Technique to Perform Software Pipelining
AU  - Jain, Suneel
T3  - PLDI '91
C1  - New York, NY, USA
C3  - Proceedings of the ACM SIGPLAN 1991 Conference on Programming Language Design and Implementation
DA  - 1991///
PY  - 1991
DO  - 10.1145/113445.113464
SP  - 219
EP  - 228
PB  - Association for Computing Machinery
SN  - 0-89791-428-7
UR  - https://doi.org/10.1145/113445.113464
ER  - 

TY  - JOUR
TI  - Circular Scheduling: A New Technique to Perform Software Pipelining
AU  - Jain, Suneel
T2  - SIGPLAN Not.
DA  - 1991/05//
PY  - 1991
DO  - 10.1145/113446.113464
VL  - 26
IS  - 6
SP  - 219
EP  - 228
SN  - 0362-1340
UR  - https://doi.org/10.1145/113446.113464
ER  - 

TY  - CONF
TI  - Non-Keyboard QWERTY Touch Typing: A Portable Input Interface for the Mobile User
AU  - Goldstein, Mikael
AU  - Book, Robert
AU  - Alsiö, Gunilla
AU  - Tessa, Silvia
T3  - CHI '99
AB  - Using traditional mobile input devices results in decreased effectiveness and efficiency. To improve usability issues a portable Non-Keyboard QWERTY touch-typing paradigm that supports the mobile touch-typing user is presented and investigated. It requires negligible training time. Pressure sensors strapped to the fingertips of gloves detect which finger is depressed. A language model based on lexical and syntactic knowledge transforms the depressed finger stroke sequence into real words and sentences. Different mobile input QWERTY paradigms (miniaturised, floating and Non-Keyboard) have been compared with full-size QWERTY. Among the mobile input paradigms, the Non-Keyboard fared significantly better, both regarding character error rate and subjective ratings.
C1  - New York, NY, USA
C3  - Proceedings of the SIGCHI Conference on Human Factors in Computing Systems
DA  - 1999///
PY  - 1999
DO  - 10.1145/302979.302984
SP  - 32
EP  - 39
PB  - Association for Computing Machinery
SN  - 0-201-48559-1
UR  - https://doi.org/10.1145/302979.302984
KW  - keyboard
KW  - language model
KW  - lexical knowledge
KW  - mobile user
KW  - PDA
KW  - portability
KW  - QWERTY
KW  - stylus input
KW  - syntactic knowledge
KW  - text input
KW  - touch-typing
KW  - Wizard-of-Oz
ER  - 

TY  - JOUR
TI  - Do Disk Arms Move?
AU  - Lynch, W. C.
T2  - SIGMETRICS Perform. Eval. Rev.
AB  - Measurement of the lengths of disk arm movements in a 2314 disk storage facility of an IBM 360/67 operating under the Michigan Terminal System yielded the unexpected data that the arms need not move in 63% of the accesses and need move for an average of only 30ms. in the remaining 37% of the cases. A description and analysis of a possible mechanism of action is presented. The predictions of this model do not disagree with the measured data.
DA  - 1972/12//
PY  - 1972
DO  - 10.1145/1041603.1041604
VL  - 1
IS  - 4
SP  - 3
EP  - 16
SN  - 0163-5999
UR  - https://doi.org/10.1145/1041603.1041604
ER  - 

TY  - CONF
TI  - The Role of the Intelligent Peripheral Interface in Systems Architecture
AU  - Allan, I. Dal
T3  - AFIPS '83
AB  - The American National Standards Institute (ANSI) work committees on computer interfaces are actively developing proposed standards that will affect almost all vendors of computers and peripherals. This paper reviews one project, the Intelligent Peripheral Interface (IPI), in particular, and references the application area of the Small Computer Systems Interface (SCSI).The architecture of future computer systems is going to depend in large part on the types of interfaces used, because of the ever-increasing role that peripherals play in systems performance, cost, availability, and reliability. The capability of peripherals dictates system performance—more than the central processor does, in most configurations. The cost of peripherals represents more than half of what the user pays for a system.The interface chosen to interconnect peripherals has a major influence on the cost and performance tradeoffs that a vendor can make. This paper addresses these issues with regard to the ANSI efforts on intelligent interfaces.
C1  - New York, NY, USA
C3  - Proceedings of the May 16-19, 1983, National Computer Conference
DA  - 1983///
PY  - 1983
DO  - 10.1145/1500676.1500751
SP  - 623
EP  - 630
PB  - Association for Computing Machinery
SN  - 0-88283-039-2
UR  - https://doi.org/10.1145/1500676.1500751
ER  - 

TY  - JOUR
TI  - Type Inference for C: Applications to the Static Analysis of Incomplete Programs
AU  - Melo, Leandro T. C.
AU  - Ribeiro, Rodrigo G.
AU  - Guimarães, Breno C. F.
AU  - Pereira, Fernando Magno Quintão
T2  - ACM Trans. Program. Lang. Syst.
AB  - Type inference is a feature that is common to a variety of programming languages. While, in the past, it has been prominently present in functional ones (e.g., ML and Haskell), today, many object-oriented/multi-paradigm languages such as C# and C++ offer, to a certain extent, such a feature. Nevertheless, type inference still is an unexplored subject in the realm of C. In particular, it remains open whether it is possible to devise a technique that encompasses the idiosyncrasies of this language. The first difficulty encountered when tackling this problem is that parsing C requires, not only syntactic, but also semantic information. Yet, greater challenges emerge due to C’s intricate type system. In this work, we present a unification-based framework that lets us infer the missing struct, union, enum, and typedef declarations in a program.As an application of our technique, we investigate the reconstruction of partial programs. Incomplete source code naturally appears in software development: during design and while evolving, testing, and analyzing programs; therefore, understanding it is a valuable asset. With a reconstructed well-typed program, one can: (i) enable static analysis tools in scenarios where components are absent; (ii) improve precision of “zero setup” static analysis tools; (iii) apply stub generators, symbolic executors, and testing tools on code snippets; and (iv) provide engineers with an assortment of compilable benchmarks for performance and correctness validation. We evaluate our technique on code from a variety of C libraries, including GNU’s Coreutils and on snippets from popular projects such as CPython, FreeBSD, and Git.
DA  - 2020/11//
PY  - 2020
DO  - 10.1145/3421472
VL  - 42
IS  - 3
SN  - 0164-0925
UR  - https://doi.org/10.1145/3421472
KW  - C language
KW  - parsing
KW  - Partial programs
KW  - type inference
ER  - 

TY  - CONF
TI  - Strategies for Content Recommendation in the Brazilian Rapid Response to Syphilis Project
AU  - de Morais, Philippi Sedir Grilo
AU  - Silva, Rodrigo Dantas da
AU  - Filho, José Arilton Pereira
AU  - de Medeiros Valentim, Ricardo Alexsandro
AU  - Coutinho, Karilany Dantas
AU  - de Oliveira, Carlos Alberto Pereira
AU  - Roussanaly, Azim
AU  - Boyer, Anne
T3  - EATIS '20
AB  - Syphilis is a Sexually Transmitted Infection (STI) which the Brazilian Ministry of Health has acknowledged an epidemic since 2016. To face such a problem, it is essential to develop and implement educational actions enhanced by information and communication technologies to qualify, train and raise awareness nationally. Considering the increasing number of Open Educational Resources developed, existing open health repositories and other digital platforms that allow interaction in the Brazilian Unified Health System (SUS) as well as the vast number of Healthcare Information Systems, it is essential to the develop solutions to efficiently and costly recommend content accordingly to the interest of health professionals and the current needs and priorities of the SUS, such as the epidemic of syphilis. This paper presents a discussion on the information systems and the data available and strategies of recommendation systems based on this scenario, integrating health surveillance, formative needs, georeference of health teams and professionals and epidemiological data to recommend content to health professionals all over the country.
C1  - New York, NY, USA
C3  - Proceedings of the 10th Euro-American Conference on Telematics and Information Systems
DA  - 2021///
PY  - 2021
DO  - 10.1145/3401895.3402089
PB  - Association for Computing Machinery
SN  - 978-1-4503-7711-9
UR  - https://doi.org/10.1145/3401895.3402089
KW  - information and communication technologies
KW  - learning management systems
KW  - recommendation systems
KW  - unified health system
ER  - 

TY  - JOUR
TI  - Self-Care Technologies in HCI: Trends, Tensions, and Opportunities
AU  - Nunes, Francisco
AU  - Verdezoto, Nervo
AU  - Fitzpatrick, Geraldine
AU  - Kyng, Morten
AU  - Grönvall, Erik
AU  - Storni, Cristiano
T2  - ACM Trans. Comput.-Hum. Interact.
AB  - Many studies show that self-care technologies can support patients with chronic conditions and their carers in understanding the ill body and increasing control of their condition. However, many of these studies have largely privileged a medical perspective and thus overlooked how patients and carers integrate self-care into their daily lives and mediate their conditions through technology. In this review, we focus on how patients and carers use and experience self-care technology through a Human-Computer Interaction (HCI) lens. We analyse studies of self-care published in key HCI journals and conferences using the Grounded Theory Literature Review (GTLR) method and identify research trends and design tensions. We then draw out opportunities for advancing HCI research in self-care, namely, focusing further on patients' everyday life experience, considering existing collaborations in self-care, and increasing the influence on medical research and practice around self-care technology.
DA  - 2015/12//
PY  - 2015
DO  - 10.1145/2803173
VL  - 22
IS  - 6
SN  - 1073-0516
UR  - https://doi.org/10.1145/2803173
KW  - literature review
KW  - telehealth
KW  - self-management
KW  - Chronic conditions
KW  - chronic diseases
KW  - home care
KW  - home monitoring
KW  - pervasive health
KW  - self-care
KW  - self-care design
KW  - self-care technology
KW  - telecare
ER  - 

TY  - CONF
TI  - Non-Interactive Conference Key Distribution and Its Applications
AU  - Safavi-Naini, Reihaneh
AU  - Jiang, Shaoquan
T3  - ASIACCS '08
AB  - A non-interactive conference key distribution system (or, a NICKDS for short) allows conference members to calculate a shared key without interacting with each other. NICKDSs have been studied in unconditional and computational settings. In both cases security has been evaluated against an adversary who can corrupt participants. In this paper we consider an adaptive adversary who can both corrupt participants and also access the keys of conference of his choice. We re-visit security of a number of known NICKDSs in this new model and present characterizations and conditions that guarantee security of the system in the new model. We also give a generic construction for computationally secure (in the new model) NICKDSs, from unconditionally secure ones in corruption only model.To show the usefulness of the new security model, we consider two composition constructions. First, we compose a secure NICKDS with a secure MAC by using the key obtained from the NICKDS as the MAC key, and show that this results in a ring authentication that guarantees authenticity of the received message while the sender remains anonymous and this anonymity is unconditional. The security theorem for the composition guarantees security for unconditional and computational settings, both. We also consider composition of a NICKDS with a secure (CCA2 secure) encryption system and show this results in a broadcast encryption system (BES) that is CCA2 secure. This is the first CCA2 secure BES in symmetric key setting. We discuss future works and open problems.
C1  - New York, NY, USA
C3  - Proceedings of the 2008 ACM Symposium on Information, Computer and Communications Security
DA  - 2008///
PY  - 2008
DO  - 10.1145/1368310.1368349
SP  - 271
EP  - 282
PB  - Association for Computing Machinery
SN  - 978-1-59593-979-1
UR  - https://doi.org/10.1145/1368310.1368349
KW  - unconditional security
KW  - broadcast encryption
KW  - conference key distribution
KW  - ring authentication
ER  - 

TY  - CONF
TI  - The DASH Prototype: Implementation and Performance
AU  - Lenoski, Daniel
AU  - Laudon, James
AU  - Joe, Truman
AU  - Nakahira, David
AU  - Stevens, Luis
AU  - Gupta, Anoop
AU  - Hennessy, John
T3  - ISCA '92
AB  - The fundamental premise behind the DASH project is that it is feasible to build large-scale shared-memory multiprocessors with hardware cache coherence. While paper studies and software simulators are useful for understanding many high-level design trade-offs, prototypes are essential to ensure that no critical details are overlooked. A prototype provides convincing evidence of the feasibility of the design allows one to accurately estimate both the hardware and the complexity cost of various features, and provides a platform for studying real workloads. A 16-processor prototype of the DASH multiprocessor has been operational for the last six months. In this paper, the hardware overhead of directory-based cache coherence in the prototype is examined. We also discuss the performance of the system, and the speedups obtained by parallel applications running on the prototype. Using a sophisticated hardware performance monitor, we characterize the effectiveness of coherent caches and the relationship between an application's reference behavior and its speedup.
C1  - New York, NY, USA
C3  - Proceedings of the 19th Annual International Symposium on Computer Architecture
DA  - 1992///
PY  - 1992
DO  - 10.1145/139669.139706
SP  - 92
EP  - 103
PB  - Association for Computing Machinery
SN  - 0-89791-509-7
UR  - https://doi.org/10.1145/139669.139706
ER  - 

TY  - JOUR
TI  - The DASH Prototype: Implementation and Performance
AU  - Lenoski, Daniel
AU  - Laudon, James
AU  - Joe, Truman
AU  - Nakahira, David
AU  - Stevens, Luis
AU  - Gupta, Anoop
AU  - Hennessy, John
T2  - SIGARCH Comput. Archit. News
AB  - The fundamental premise behind the DASH project is that it is feasible to build large-scale shared-memory multiprocessors with hardware cache coherence. While paper studies and software simulators are useful for understanding many high-level design trade-offs, prototypes are essential to ensure that no critical details are overlooked. A prototype provides convincing evidence of the feasibility of the design allows one to accurately estimate both the hardware and the complexity cost of various features, and provides a platform for studying real workloads. A 16-processor prototype of the DASH multiprocessor has been operational for the last six months. In this paper, the hardware overhead of directory-based cache coherence in the prototype is examined. We also discuss the performance of the system, and the speedups obtained by parallel applications running on the prototype. Using a sophisticated hardware performance monitor, we characterize the effectiveness of coherent caches and the relationship between an application's reference behavior and its speedup.
DA  - 1992/04//
PY  - 1992
DO  - 10.1145/146628.139706
VL  - 20
IS  - 2
SP  - 92
EP  - 103
SN  - 0163-5964
UR  - https://doi.org/10.1145/146628.139706
ER  - 

TY  - CONF
TI  - DCDS Digital Simulating System
AU  - Potash, H.
AU  - Tyrrill, A.
AU  - Allen, D.
AU  - Joseph, S.
AU  - Estrin, G.
T3  - AFIPS '69 (Fall)
AB  - This article is concerned with the problems of digital simulation and describes methods used in the Digital Control Design System (DCDS) for the simulation of digital structures. The paper is divided into five parts:• A short introduction to DCDS, its structure and purposes.• A discussion of simulation techniques, entities and attributes.• The DCDS pseudo machine simulator.• The pseudo machine program.• A simple example of a DCDL program.
C1  - New York, NY, USA
C3  - Proceedings of the November 18-20, 1969, Fall Joint Computer Conference
DA  - 1969///
PY  - 1969
DO  - 10.1145/1478559.1478645
SP  - 707
EP  - 720
PB  - Association for Computing Machinery
SN  - 978-1-4503-7906-9
UR  - https://doi.org/10.1145/1478559.1478645
ER  - 

